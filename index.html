<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta property="og:type" content="website">
<meta property="og:title" content="EmberNLP">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EmberNLP">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/18/2017-09-18Convolutional Sequence to Sequence Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/18/2017-09-18Convolutional Sequence to Sequence Learning/" itemprop="url">Convolutional Sequence to Sequence Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-18T16:49:36+10:00">
                2017-09-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="introduction">1. Introduction</h1>
<p>The dominant approach of sequence to sequence learning to date encodes the input sequence with a series of bi-directional RNN and generates a variable length output with another set of decoder RNNs both of which interface via a <a href="https://embernlp.github.io/2017/09/15/2017-09-15Seq2seq/" target="_blank" rel="external">soft-attention mechanism</a>.</p>
<p>Compared to recurrent layers, convolutions create representations for <em>fixed size</em> contexts, however, the effective context size of the network can easily be made larger by stacking several layers on top of each other. This allows to precisely control the maximum length of dependencies to be modeled. Multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher levels.</p>
<p>This papr proposes an architecture for sequence to sequence modeling that is entirely convolutional. The model is equipped with <a href="https://embernlp.github.io/2017/09/14/2017-09-14Language%20Modeling%20with%20Gated%20Convolutional%20Networks/" target="_blank" rel="external">gated linear unit</a> and <a href="https://embernlp.github.io/2017/09/15/2017-09-15Highway&amp;Residual&amp;Training%20RNNs%20as%20Fast%20as%20CNNs/" target="_blank" rel="external">residual connections</a>. It also uses attention in every decoders layer and demonstrate that each attention layer only adds a negligible amount of overhead. And the combination of these choices enables us to tackle large scale problems.</p>
<h1 id="model-architecture">2. Model architecture</h1>
<p>The overall model architecture is shown below:</p>
<center>
<img src="/images/2017-09-18Convolutional%20Sequence%20to%20Sequence%20Learning/ModelArchitecture.png">
</center>
<h3 id="position-embedding">2.1 Position Embedding</h3>
<p>Input elements <span class="math inline">\(\mathbf{x} = (x_1,...x_m)\)</span> embedded in distributional space as <span class="math inline">\(\mathbf{w} = (w_1,...w_m)\)</span>, where <span class="math display">\[w_j \in \mathbb{R}^f\]</span> is a column in an embedding matrix <span class="math inline">\(\mathcal{D} \in \mathbb{R}^{V \times f}\)</span>.</p>
<p>The model is also equipped with a sense of order by embedding the absolute position of input elements <span class="math inline">\(\mathbf{p} = (p_1,...p_m)\)</span>, where <span class="math display">\[p_j \in \mathbb{R}^f\]</span> Just like a one-hot coding.</p>
Both are combined to obtain input element representations: <span class="math display">\[\mathbf{e} = (w_1+p_1,...,w_m+p_m)\]</span> And matrix <span class="math inline">\(\mathbf{e}\)</span> will result as below (column per word):
<span class="math display">\[\begin{equation}
\mathbf{e} = 
\left(\begin{array}{cc} 
w_{11} + 1 &amp; w_{21} &amp; \cdots &amp; w_{m1} \\ 
w_{12} &amp; w_{22} + 1 &amp; \cdots &amp; w_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{1f} &amp; w_{2f} &amp; \cdots &amp; w_{mf} + 1 \\
\end{array}\right)
\end{equation}\]</span>
<p>The model proceeds similarly for ouput elements that were <em>already generated by the decoder network</em> to yield output element representations that are being fed back into the decoder network, <span class="math inline">\(\mathbf{g} = (g_1,...,g_n)\)</span>.</p>
<p>Position embeddings are useful in model architecture since they give the model a sense of which portion of the sequence in the input or output it is currently dealing with.</p>
<h3 id="convolutional-block-structure">2.2 Convolutional Block Structure</h3>
<p>Both encoder and decoder networks share a simple block structure that computes intermediate states based on a fixed number of input elements. Each block contains <strong>one dimensional convolution</strong> followed by <strong>a non-linearity</strong>.</p>
<p>Denote the output of the <span class="math inline">\(l^{th}\)</span> block as <span class="math inline">\(\mathbf{h} = (h^l_1,...,h^l_n)\)</span> for the decoder network and <span class="math inline">\(\mathbf{z} = (z^l_1,...,z^l_m)\)</span> for the encoder network.</p>
<h4 id="gated-linear-convolution">2.2.1 Gated Linear Convolution</h4>
<p>For a decoder network with a single block and kernel width <span class="math inline">\(k\)</span>, each resulting state <span class="math inline">\(h^1_i\)</span> contains information over <span class="math inline">\(k\)</span> input elements. Stacking several blocks on top of each other increases the number of input elements represented in a state. For instance, stacking 6 blocks with <span class="math inline">\(k=5\)</span>, results in an input field of <span class="math inline">\(25 (4 \times 5 + 5)\)</span> elements.</p>
<p>Each convolution kernel is parameterized as <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{2d \times kd}\)</span>, <span class="math inline">\(b_w \in \mathbf{R}^{2d}\)</span> and takes as input vector <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{kd}\)</span> which is a concatenation (hstack) of <span class="math inline">\(k\)</span> input elements embedded in <span class="math inline">\(d\)</span> dimensions and maps them to a single output element <span class="math inline">\(\mathbf{Y} \in \mathbb{R}^{2d}\)</span>.</p>
<h4 id="non-linearities">2.2.2 Non-linearities</h4>
<p>Non-linearities allow the networks to exploit the full input field, or to focus on fewer elements if needed. This paper chooses gated linear units (<a href="https://embernlp.github.io/2017/09/14/2017-09-14Language%20Modeling%20with%20Gated%20Convolutional%20Networks/" target="_blank" rel="external">GLU</a>):</p>
<span class="math display">\[\begin{equation}\begin{split}
Y &amp;= [A \space B] \in \mathbb{R}^{2d}\\\\
v([A \space B]) &amp;= A \otimes \sigma(B)\\\\
v([A \space B]) &amp;\in \mathbb{R}^d
\end{split}\end{equation}\]</span>
<h4 id="residual-connection">2.2.3 Residual Connection</h4>
<p>To enable deep convolutional networks, residual connections are added from the input of each convolution to the output of the block:</p>
<span class="math display">\[\begin{equation}
h^l_i= v(\mathbf{W}^l[h^{l-1}_{i-k/2},...,h^{l-1}_{i+k/2}] + b^l_w) + h^{l-1}_i
\end{equation}\]</span>
<h4 id="block-output">2.2.4 Block Output</h4>
<p>For encoder networks, ensure that the output of the convolutional layers matches the input length by padding the input at each layer. However, for decoder networks, we need to take care that no future information is available to the decoder. To achieve this, pad the input by <span class="math inline">\(k-1\)</span> elements on both the left and right side by zero vectors, and then remove <span class="math inline">\(k\)</span> elements from the end of the convolution output.</p>
<h4 id="linear-mapping">2.2.5 Linear Mapping</h4>
<p>Linear mapping is applied to <span class="math inline">\(\mathbf{w}\)</span> when feeding embeddings to the encoder network; to the encoder output <span class="math inline">\(z^u_j\)</span>; to the finnal layer of the decoder just before the softmax <span class="math inline">\(\mathbf{h}^L\)</span>; and to all decoder layers <span class="math inline">\(\mathbf{h}^l\)</span> before computing attention scores.</p>
<h4 id="prediction-layer">2.2.6 Prediction Layer</h4>
<p>To compute a distribution over the T possible next target elements <span class="math inline">\(y_{i+1}\)</span> by transforming the top decoder output <span class="math inline">\(h^L_i\)</span> via a linear layer with weights <span class="math inline">\(W_0\)</span> and bias <span class="math inline">\(b_0\)</span>.</p>
<span class="math display">\[\begin{equation}
p(y_{i+1} | y_1,...,y_i, \mathbf{x}) = softmax(\mathbf{W}_0h^L_i + b_0) \in \mathbb{R}^T
\end{equation}\]</span>
<h3 id="multi-step-attention">2.3 Multi-step Attention</h3>
<p>The paper introduces a seperate attention mechanism for each decoder layer, shown below:</p>
<center>
<img src="/images/2017-09-18Convolutional%20Sequence%20to%20Sequence%20Learning/Attention.gif">
</center>
<p>To compute the attention, we combine the current docoder state <span class="math inline">\(h^l_i\)</span> with an embedding of the previous target element <span class="math inline">\(g_i\)</span>:</p>
<span class="math display">\[\begin{equation}
d^l_i = W^l_dh^l_i + b^l_i + g_i
\end{equation}\]</span>
<p>where <span class="math inline">\(h^l_i \in \mathbb{R}^d\)</span> and <span class="math inline">\(W^l_i \in \mathbb{R}^{f \times d}\)</span>， so <span class="math inline">\(d^l_i \in \mathbb{R}^f\)</span>.</p>
<p>For decoder layer <span class="math inline">\(l\)</span> the attention <span class="math inline">\(a^l_{ij}\)</span> of state <span class="math inline">\(i\)</span> and source element <span class="math inline">\(j\)</span> is computed as a dot_product between the deocder state summary <span class="math inline">\(d^l_i\)</span> and each output <span class="math inline">\(z^u_j\)</span> of the last encoder block <span class="math inline">\(u\)</span>:</p>
<span class="math display">\[\begin{equation}
a^l_{ij} = \frac{exp(d^l_i \cdot z^u_j)}{\Sigma^m_{t=1}exp(d^l_i \cdot z^u_t)}
\end{equation}\]</span>
<p>The conditional input <span class="math inline">\(c^l_i\)</span> to the current decoder layer is a weighted sum of the encoder outputs as well as the input element embeddings <span class="math inline">\(e_j\)</span>:</p>
<span class="math display">\[\begin{equation}
c^l_i = \Sigma^m_{j=1}a^l_{ij}(z^u_j+e_j)
\end{equation}\]</span>
<p>where <span class="math inline">\(m\)</span> is the length of encoder output (roughly equal to length of input to the model).</p>
<p>This is slightly different to recurrent approaches which compute both the attention and the weighted sum over <span class="math inline">\(z^u_j\)</span> only. Adding <span class="math inline">\(e_j\)</span> to be beneficial and it resembles key-value memory networks where the keys are the <span class="math inline">\(z^u_j\)</span> and the values are the <span class="math inline">\(z^u_j + e_j\)</span>. Encoder outputs <span class="math inline">\(z^u_j\)</span> represent <strong>potentially large input contexts</strong> and <span class="math inline">\(e_j\)</span> provides <strong>point information</strong> about a specific input element that is useful when making a prediction. Once <span class="math inline">\(c^l_i\)</span> has been computed, it is simply added to the output of the correspoding decoder layer <span class="math inline">\(h^l_i\)</span>.</p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="external">Convolutional Sequence to Sequence Learning</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/15/2017-09-15Attention Is All You Need/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/15/2017-09-15Attention Is All You Need/" itemprop="url">Attention Is All You Need</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-15T20:52:37+10:00">
                2017-09-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="introduction">1. Introduction</h1>
<p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. This paper proposes a new simple network architecture, the Transformer, based solely on attention mechnisms, dispensing with recurrence and convolutions entirely.</p>
<p>Most competitive neural sequence transduction models have an encoder-decoder structure which describes in my previous <a href="https://embernlp.github.io/2017/09/15/2017-09-15Seq2seq/" target="_blank" rel="external">Seq2seq blog</a>. Here, the encoder maps an input sequence of symbol representations <span class="math inline">\((x_1,...,x_n)\)</span> to a sequence of continuous representations <span class="math inline">\(\mathbf{z} = (z_1,...,z_n)\)</span>. Given <span class="math inline">\(\mathbf{z}\)</span>, the decoder then generates an output sequence <span class="math inline">\((y_1,...,y_m)\)</span> of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.</p>
<p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.</p>
<center>
<img src="/images/2017-09-15Attention%20Is%20All%20You%20Need/ModelGIF.gif">
</center>
<p><em><strong>Self-attention</strong></em>, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</p>
<p>The entire model structure is shown below:</p>
<center>
<img src="/images/2017-09-15Attention%20Is%20All%20You%20Need/ModelStructure.png">
</center>
<p>I’ll follow the paper to describle each block in the Transform model.</p>
<h1 id="encoder-and-decoder-stacks">2. Encoder and Decoder Stacks</h1>
<h3 id="encoder">2.1 Encoder</h3>
<p>The encoder is composed of a stack of <span class="math inline">\(N=6\)</span> identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. It employs a <a href="https://embernlp.github.io/2017/09/15/2017-09-15Highway&amp;Residual&amp;Training%20RNNs%20as%20Fast%20as%20CNNs/" target="_blank" rel="external">residual connection</a> around each of the two sub-layers, followed by <a href="https://embernlp.github.io/2017/08/31/2017-08-31Batch%20Normalization/" target="_blank" rel="external">layer normalization</a>. That is, the output of each sub-layer is <span class="math inline">\(LayerNorm(x+Sublayer(x))\)</span>. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <span class="math inline">\(d_{model} = 512\)</span>.</p>
<h3 id="decoder">2.2 Decoder</h3>
<p>The decoder is also composed of a stack of <span class="math inline">\(N=6\)</span> identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, it also employs residual connections around each of the sub-layers, followed by layer normalization. Self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position <span class="math inline">\(i\)</span> can depend only on the known outputs at positions less than <span class="math inline">\(i\)</span>.</p>
<h1 id="attention">3. Attention</h1>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<h3 id="scaled-dot-product-attention">3.1 Scaled Dot-Product Attention</h3>
<p>The input consists of queries and keys of dimension <span class="math inline">\(d_k\)</span> , and values of dimension <span class="math inline">\(d_v\)</span> . And compute the dot products of the query with all keys, divide each by <span class="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax function to obtain the weights on the values.</p>
<center>
<img src="/images/2017-09-15Attention%20Is%20All%20You%20Need/ScaledDotProductAttention.png">
</center>
<p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix <span class="math inline">\(Q\)</span>. The keys and values are also packed together into matrices <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span> . We compute the matrix of outputs as:</p>
<span class="math display">\[\begin{equation}
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}\]</span>
<p><em>According to the graph above, the matrix <span class="math inline">\(Q,K,V\)</span> are all the same. Each row represents the embedding or represention(includes postion) of a word. The meaning of <span class="math inline">\(QK^T\)</span> is to compute pair-wise dot-product of word embeddings. After Softmax transformation for each row, we get a mtraix whose rows represent the similarity between the word embedding of this row and all the other word embeddings. Then we mulplied by <span class="math inline">\(V\)</span>, results in a matrix whose row is new word presentation calculated by weighted sum of previous word embeddings.</em></p>
<p>The author suspects that for large values of <span class="math inline">\(d_k\)</span>, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, they scale the dot products by <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>.</p>
<p><em>The dimension of inputs are <span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span>, <span class="math inline">\(K \in \mathbb{R}^{n \times d_k}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{n \times d_v}\)</span>; the dimension of the output is <span class="math inline">\(\mathbb{R}^{n \times d_v}\)</span>. <span class="math inline">\(n\)</span> is the number of words.</em></p>
<h3 id="multi-head-attention">3.2 Multi-Head Attention</h3>
<p>Instead of performing a single attention function with <span class="math inline">\(d_{model}\)</span>-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values <span class="math inline">\(h\)</span> times with different, learned linear projections to <span class="math inline">\(d_k\)</span>, <span class="math inline">\(d_k\)</span> and <span class="math inline">\(d_v\)</span> dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding <span class="math inline">\(d_v\)</span>-dimensional output values. These are concatenated and once again projected, resulting in the final values</p>
<center>
<img src="/images/2017-09-15Attention%20Is%20All%20You%20Need/MultiHeadAttention.png">
</center>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
<span class="math display">\[\begin{equation}\begin{split}
MultiHead(Q,K,V) &amp;= Concat(head_1,...,head_h)W^o\\\\
where \space head_i &amp;= Attention(QW^Q_i, KW^K_i,VW^V_i)
\end{split}\end{equation}\]</span>
<p>Where the projections are parameter matrices <span class="math inline">\(W^Q_i \in \mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W^K_i \in \mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W^V_i \in \mathbb{R}^{d_{model} \times d_v}\)</span> and <span class="math inline">\(W^O_i \in \mathbb{R}^{hd_{v} \times d_{model}}\)</span>.</p>
<p>In this paper, it employs <span class="math inline">\(h=8\)</span> parallel attention layers, or heads. For each of these we use <span class="math inline">\(d_k = d_v = d_{model} / h =64\)</span>. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p>
<p>The dimensions of inputs are <span class="math inline">\(Q \in \mathbb{R}^{n \times d_{model}}\)</span>, <span class="math inline">\(K \in \mathbb{R}^{n \times d_{model}}\)</span>, <span class="math inline">\(V \in \mathbb{R}^{n \times d_{model}}\)</span>; the dimension of output is <span class="math inline">\(\mathbb{R}^{n \times d_{model}}\)</span></p>
<h3 id="position-wise-feed-forward-networks">3.3 Position-wise Feed-Forward Networks</h3>
<p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. <em>Here, position means the representation of each word.</em> This consists of two linear transformations with a ReLU activation in between.</p>
<span class="math display">\[\begin{equation}
FFN(x) = max(0,xW_1 + b_1)W_2 + b_2
\end{equation}\]</span>
<p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size <span class="math inline">\(1\)</span>. The dimensionality of input and output is <span class="math inline">\(d_{model} = 512\)</span>, and the inner-layer has dimensionality <span class="math inline">\(d_{ff} =2048\)</span>.</p>
<p><em>The dimension is changing from <span class="math inline">\(\mathbb{R}^{n \times d_{model}}\)</span> to <span class="math inline">\(\mathbb{R}^{n \times d_{ff}}\)</span> then the output will be transformed back to <span class="math inline">\(\mathbb{R}^{n \times d_{model}}\)</span>.</em></p>
<h3 id="embedding-and-softmax">3.4 Embedding and Softmax</h3>
<p>Similarly to other sequence transduction models, it uses learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. It also uses the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In the model, weight matrix is shared between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, weights are multiplied by <span class="math inline">\(\sqrt{d_{model}}\)</span>.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/15/2017-09-15Highway&Residual&Training RNNs as Fast as CNNs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/15/2017-09-15Highway&Residual&Training RNNs as Fast as CNNs/" itemprop="url">Highway Network & Residual Network & Training RNNs as Fast as CNNs</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-15T15:15:33+10:00">
                2017-09-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="highway-network">Highway Network</h1>
<p>Highway networks allow unimpeded information flow across several layers on information highways.</p>
<p>A plain feedforward neural network typically consists of <span class="math inline">\(L\)</span> layers where the <span class="math inline">\(l^{th}\)</span> layer <span class="math inline">\((l \in \{ 1,2,...,L \})\)</span> applies a non-linear transform <span class="math inline">\(H\)</span> (parameterized by <span class="math inline">\(\mathbf{W_{H,l}}\)</span>) on its input <span class="math inline">\(\mathbf{x_l}\)</span> to produce its output <span class="math inline">\(\mathbf{y_l}\)</span>. Thus, <span class="math inline">\(\mathbf{x_1}\)</span> is the input to the network and <span class="math inline">\(\mathbf{y_L}\)</span> is the network’s output. Omitting the layer index and biases for clarity,</p>
<span class="math display">\[\begin{equation}\begin{split}
\mathbf{y} = H (\mathbf{x}, \mathbf{W_H})
\end{split}\end{equation}\]</span>
<p><span class="math inline">\(H\)</span> is usually an affine transform followed by a non-linear activation function, but in general it may take other forms.</p>
<p>For a highway network, the author defined two non-linear transforms <span class="math inline">\(T (\mathbf{x}, \mathbf{W_T})\)</span> and <span class="math inline">\(C (\mathbf{x}, \mathbf{W_C})\)</span> such that</p>
<span class="math display">\[\begin{equation}\begin{split}
\mathbf{y} = H (\mathbf{x}, \mathbf{W_H}) \cdot T (\mathbf{x}, \mathbf{W_T}) + \mathbf{x} \cdot C (\mathbf{x}, \mathbf{W_C})
\end{split}\end{equation}\]</span>
<p><span class="math inline">\(T\)</span> is the <em><strong>transform gate</strong></em> and <span class="math inline">\(C\)</span> is the <em><strong>carry gate</strong></em>, since they express how much of the output is produced by transforming the input and carrying it, respectively. For simplicity, in this paper, <span class="math inline">\(C = 1 − T\)</span> , giving</p>
<span class="math display">\[\begin{equation}\begin{split}
\mathbf{y} = H (\mathbf{x}, \mathbf{W_H}) \cdot T (\mathbf{x}, \mathbf{W_T}) + \mathbf{x} \cdot (1 - T (\mathbf{x}, \mathbf{W_T}))
\end{split}\end{equation}\]</span>
<p>We should notice that the dimensionality of <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\mathbf{y}\)</span>, <span class="math inline">\(H (\mathbf{x}, \mathbf{W_H})\)</span> and <span class="math inline">\(T (\mathbf{x}, \mathbf{W_T})\)</span> must be the same for above Equation (3) to be valid. Also note that this re-parametrization of the layer transformation is much more flexible than Equation (1). In particular, observe that</p>
<span class="math display">\[\begin{equation}
\mathbf{y} =
\left\{
\begin{aligned}
&amp;\mathbf{x}, &amp;if \space T (\mathbf{x}, \mathbf{W_T}) = \mathbf{0} \\
&amp;H (\mathbf{x}, \mathbf{W_H}), &amp;if \space T (\mathbf{x}, \mathbf{W_T}) = \mathbf{1}\\
\end{aligned}
\right.
\end{equation}\]</span>
<p>Thus, depending on the output of the transform gates, a highway layer can smoothly vary its behavior between that of a plain layer and that of a layer which simply passes its inputs through. Just as a plain layer consists of multiple computing units such that the <span class="math inline">\(i^{th}\)</span> unit computes <span class="math inline">\(y_i = H_i (\mathbf{x})\)</span>, a highway network consists of multiple blocks such that the <span class="math inline">\(i^{th}\)</span> block computes a block state <span class="math inline">\(H_i (\mathbf{x})\)</span> and transform gate output <span class="math inline">\(T_i (\mathbf{x})\)</span>. Finally, it produces the block output <span class="math inline">\(y_i =H_i (\mathbf{x}) \ast T_i (\mathbf{x}) + x_i \ast (1−T_i (\mathbf{x}))\)</span>, which is connected to the next layer.</p>
<p><strong>In other words, Highway network treats every layer from layerwise to blockwise.</strong></p>
<h1 id="residual-network">Residual Network</h1>
<p>In this paper, the author addressed the degradation problem by introducing a deep residual learning framework. Instead of hoping each few stacked layers directly fit a desired underlying mapping, explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as <span class="math inline">\(H(\mathbf{x})\)</span>, we let the stacked nonlinear layers fit another mapping of <span class="math inline">\(F(\mathbf{x}) := H(\mathbf{x}) − \mathbf{x}\)</span>. The original mapping is recast into <span class="math inline">\(F(\mathbf{x})+\mathbf{x}\)</span>. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.</p>
<p>Compared to Highway Network, Residual Network is just a special case when <span class="math inline">\(T (\mathbf{x}, \mathbf{W_T}) = \mathbf{0.5}\)</span>.</p>
<h1 id="training-rnns-as-fast-as-cnns">Training RNNs as Fast as CNNs</h1>
<p>Recurrent neural networks scale poorly due to the intrinsic difficulty in parallelizing their state computations. For instance, the forward pass computation of <span class="math inline">\(h_t\)</span> is blocked until the entire computation of <span class="math inline">\(h_{t−1}\)</span> finishes, which is a major bottleneck for parallel computing.</p>
<p>This paper introduces a Simple Recurrent Unit (SRU) which operates significantly faster than traditional recurrent implementations. The recurrent unit simplifies state computation and hence exposes the same parallelism as CNNs, attention and feed-forward nets. Specifically, while the update of internal state <span class="math inline">\(c_t\)</span> still makes use of the previous state <span class="math inline">\(c_{t−1}\)</span>, the dependence on <span class="math inline">\(h_{t−1}\)</span> in a recurrence step has been dropped. As a result, all matrix multiplications and element-wise operations in the recurrent unit can be easily parallelized across different dimensions and steps.</p>
<p>Most top-performing recurrent neural networks such as LSTMs and GRUs make use of neural gates to control the information flow and alleviate the gradient vanishing (or explosion) problem. Consider a typical implementation,</p>
<span class="math display">\[\begin{equation}\begin{split}
\mathbf{c}_t &amp;= \mathbf{f_t} \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \widetilde{\mathbf{x}}_t\\\\
&amp;= \mathbf{f_t} \odot \mathbf{c}_{t-1} + (1 - \mathbf{f}_t) \odot \widetilde{\mathbf{x}}_t
\end{split}\end{equation}\]</span>
<p>where <span class="math inline">\(\mathbf{f}_t\)</span> and <span class="math inline">\(\mathbf{i}_t\)</span> are sigmoid gates referred as the <em><strong>forget gate</strong></em> and <em><strong>input gate</strong></em>. <span class="math inline">\(\widetilde{\mathbf{x}}_t\)</span> is the transformed input at step <span class="math inline">\(t\)</span>. <span class="math inline">\(\mathbf{i}_t = 1 − \mathbf{f}_t\)</span> here for simplicity. The computation of <span class="math inline">\(\widetilde{\mathbf{x}}_t\)</span> also varies in different RNN instances. The paper uses the simplest version that performs a linear transformation over the input vector <span class="math inline">\(\widetilde{\mathbf{x}}_t = \mathbf{Wx}_t\)</span>. Finally, the internal state <span class="math inline">\(\mathbf{c}_t\)</span> is passed to an activation function <span class="math inline">\(g(·)\)</span> to produce the output state <span class="math inline">\(\mathbf{h}_t = g(\mathbf{c}_t)\)</span>.</p>
<p>The paper includes two additional features in implementation. First, it adds skip connections between recurrent layers since they are shown quite effective for training deep networks. Specifically, we use highway connections and the output state <span class="math inline">\(\mathbf{h}&#39;_t\)</span> is computed as,</p>
<span class="math display">\[\begin{equation}\begin{split}
\mathbf{h}&#39;_t &amp;= \mathbf{r}_t \odot \mathbf{h}_t + (1 - \mathbf{r}_t) \odot \mathbf{x}_t\\\\
&amp;= \mathbf{r}_t \odot g(\mathbf{c}_t) + (1 - \mathbf{r}_t) \odot \mathbf{x}_t
\end{split}\end{equation}\]</span>
<p>where <span class="math inline">\(\mathbf{r}_t\)</span> is the output of a reset gate.</p>
<p>The associated equations of SRU are given below:</p>
<span class="math display">\[\begin{equation}\begin{split}
\widetilde{\mathbf{x}}_t &amp;= \mathbf{Wx}_t\\\\
\mathbf{f}_t &amp;= \sigma(\mathbf{W}_f\mathbf{x}_t + \mathbf{b}_f)\\\\
\mathbf{r}_t &amp;= \sigma(\mathbf{W}_r\mathbf{x}_t + \mathbf{b}_r)\\\\
\mathbf{c}_t &amp;= \mathbf{f}_t \odot \mathbf{c}_{t-1} + (1 - \mathbf{f}_t) \odot \widetilde{\mathbf{x}_t}\\\\
\mathbf{h}_t &amp;= \mathbf{r}_t \odot g(\mathbf{c}_t) + (1 - \mathbf{r}_t) \odot \mathbf{x}_t
\end{split}\end{equation}\]</span>
<p>We can see that the first three equations can be computed in parallel. And last two equations can be computed quite easily and fast since all operations are element-wise.</p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="external">Highway Network</a></li>
<li><a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="external">Deep Residual Learning for Image Recognition</a></li>
<li><a href="https://arxiv.org/pdf/1709.02755.pdf" target="_blank" rel="external">Training RNNs as Fast as CNNs</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/15/2017-09-15Seq2seq/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/15/2017-09-15Seq2seq/" itemprop="url">Sequence To Sequence Models</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-15T10:21:38+10:00">
                2017-09-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="contents">Contents</h1>
<ul>
<li>Sequence to Sequence Learning with Neural Networks</li>
<li>Neural Machine Translation by Jointly Learning to Align and Translate</li>
<li>The Cost of Attention</li>
</ul>
<h3 id="sequence-to-sequence-learning-with-neural-networks">Sequence to Sequence Learning with Neural Networks</h3>
<center>
<img src="/images/2017-09-15Seq2seq/GeneralSeq2seq.png">
</center>
<p>The Recurrent Neural Network (RNN) is a natural generalization of feedforward neural networks to sequences. Given a sequence of inputs <span class="math inline">\((x_1,...,x_T)\)</span>, a standard RNN computes a sequence of outputs <span class="math inline">\((y_1,...,y_T)\)</span> by iterating the following equation:</p>
<span class="math display">\[\begin{equation}\begin{split}
h_t &amp;= sigm(W^{hx}x_t + W^{hh}h_{t-1})\\\\
y_t &amp;= W^{yh}h_t
\end{split}\end{equation}\]</span>
<p>The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships.</p>
<p>A simple strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one RNN, and then to map the vector to the target sequence with another RNN. While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies. However, the Long Short-Term Memory (LSTM) is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting.</p>
<p>The goal of the LSTM is to estimate the conditional probability <span class="math inline">\(p(y_1,...,y_{T&#39;} | x_1,...,x_T)\)</span> where <span class="math inline">\((x_1,...,x_T)\)</span> is an input sequence and <span class="math inline">\((y_1,...,y_{T&#39;})\)</span> is its corresponding output sequence whose length <span class="math inline">\(T&#39;\)</span> may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-dimensional representation <span class="math inline">\(v\)</span> of the input sequence <span class="math inline">\((x_1,...,x_T)\)</span> given by the last hidden state of the LSTM, and then computing the probability of <span class="math inline">\(y_1,...,y_{T&#39;}\)</span> with a standard LSTM-LM formulation whose initial hidden state is set to the representation <span class="math inline">\(v\)</span> of <span class="math inline">\(x_1,...,x_T\)</span>:</p>
<span class="math display">\[\begin{equation}\begin{split}
p(y_1,...,y_{T&#39;} | x_1,...,x_T) = \prod\limits^{T&#39;}_{t=1}p(y_t|v,y_1,...,y_{t-1})
\end{split}\end{equation}\]</span>
<p>In this equation, each <span class="math inline">\(p(y_t|v,y_1,...,y_{t-1})\)</span> distribution is represented with a softmax over all the words in the vocabulary. Note that each sentence ends with a special end-of-sentence symbol “<span class="math inline">\(&lt;EOS&gt;\)</span>”, which enables the model to define a distribution over sequences of all possible lengths. The overall scheme is outlined in figure 1, where the shown LSTM computes the representation of “A”, “B”, “C”, “<span class="math inline">\(&lt;EOS&gt;\)</span>” and then uses this representation to compute the probability of “W”, “X”, “Y”, “Z”, “<eos>”.</eos></p>
<p>Three things to note:</p>
<ol style="list-style-type: decimal">
<li>Two LSTM are used, one for the encoder and one for the decoder.</li>
<li>Deep LSTMs significantly outperformed shallow LSTMs.</li>
<li>It extremely valuable to reverse the order of the words of the input sentence.</li>
<li>Different sentences have different lengths. Most sentences are short, but some sentences are long, so a minibatch of 128 randomly chosen training sentences will have many short sentences and few long sentences, and as a result, much of the computation in the minibatch is wasted. The strategy is that make sure all sentences in a minibatch are roughly of the same length.</li>
</ol>
<h3 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation by Jointly Learning to Align and Translate</h3>
<center>
<img src="/images/2017-09-15Seq2seq/SoftAttentionSeq2seq.png">
</center>
<p>Each conditional probability is defined as:</p>
<span class="math display">\[\begin{equation}\begin{split}
p(y_i|y_1,...,y_{i-1},\mathbf{x}) = g(y_{i-1},s_i,c_i)
\end{split}\end{equation}\]</span>
where <span class="math inline">\(s_i\)</span> is an RNN hidden state for time <span class="math inline">\(i\)</span>, computed by:
<span class="math display">\[\begin{equation}\begin{split}
s_i = f(s_{i-1},y_{i-1},c_i)
\end{split}\end{equation}\]</span>
<p>It should be noted that unlike the above general encoder-decoder approach, here the probability is conditioned on a distinct context vector <span class="math inline">\(c_i\)</span> for each target <span class="math inline">\(y_i\)</span>.</p>
<p>The context vector <span class="math inline">\(c_i\)</span> depends on a sequence of annotations <span class="math inline">\((h_1,...,h_{T_x})\)</span> to which an encoder maps the input sentence. Each annotation <span class="math inline">\(h_i\)</span> contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence. The context vector <span class="math inline">\(c_i\)</span> is, then, computed as a weighted sum of these annotations <span class="math inline">\(h_i\)</span>:</p>
<span class="math display">\[\begin{equation}\begin{split}
c_i = \prod\limits^{T_x}_{j=1}\alpha_{ij}h_j
\end{split}\end{equation}\]</span>
<p>The weight <span class="math inline">\(\alpha_{ij}\)</span> of each annotation <span class="math inline">\(h_j\)</span> is computed by:</p>
<span class="math display">\[\begin{equation}\begin{split}
\alpha_{ij} = \frac{exp(e_{ij})}{\Sigma^{T_x}_{k=1} exp(e_{ik})}
\end{split}\end{equation}\]</span>
<p>where</p>
<span class="math display">\[\begin{equation}\begin{split}
e_{ij} = a(s_{i-1}, h_j)
\end{split}\end{equation}\]</span>
<p>is an alignment model which scores how well the inputs around position <span class="math inline">\(j\)</span> and the output at position <span class="math inline">\(i\)</span> match. The score is based on the RNN hidden state <span class="math inline">\(s_{i−1}\)</span> and the j-th annotation <span class="math inline">\(h_j\)</span> of the input sentence.</p>
<h3 id="the-cost-of-attention">The Cost of Attention</h3>
<p>If we look a bit more look closely at the equation for attention we can see that attention comes at a cost. We need to calculate an attention value for each combination of input and output word. If you have a 50-word input sequence and generate a 50-word output sequence that would be 2500 attention values. That’s not too bad, but if you do character-level computations and deal with sequences consisting of hundreds of tokens the above attention mechanisms can become prohibitively expensive.</p>
<p>Actually, that’s quite counterintuitive. Human attention is something that’s supposed to save computational resources. By focusing on one thing, we can neglect many other things. But that’s not really what we’re doing in the above model. We’re essentially looking at everything in detail before deciding what to focus on. Intuitively that’s equivalent outputting a translated word, and then going back through all of your internal memory of the text in order to decide which word to produce next. That seems like a waste, and not at all what humans are doing. In fact, it’s more akin to memory access, not attention, which in my opinion is somewhat of a misnomer (more on that below). Still, that hasn’t stopped attention mechanisms from becoming quite popular and performing well on many tasks.</p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="external">Attention and memory in deep learning and nlp</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/14/2017-09-14Language Modeling with Gated Convolutional Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/14/2017-09-14Language Modeling with Gated Convolutional Networks/" itemprop="url">Language Modeling with Gated Convolutional Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-14T21:31:19+10:00">
                2017-09-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This paper introduces a new neural language model that replaces the recurrent connections typically used in recurrent networks with gated temporal convolutions. Neural language models (Bengio et al., 2003) produce a representation <span class="math inline">\(\mathbf{H} = [\mathbf{h}_0,...,\mathbf{h}_N]\)</span> of the context for each word <span class="math inline">\(w_0,...,w_N\)</span> to predict the next word <span class="math inline">\(P(w_i|\mathbf{h}_i)\)</span>. Recurrent neural networks <span class="math inline">\(f\)</span> computes <span class="math inline">\(\mathbf{H}\)</span> through a recurrent function <span class="math inline">\(\mathbf{h}_i =f(\mathbf{h}_{i−1},w_{i−1})\)</span> which is an inherently sequential process that cannot be parallelized over <span class="math inline">\(i\)</span>.</p>
<p>The proposed approach convolves the inputs with a function <span class="math inline">\(f\)</span> to obtain <span class="math inline">\(\mathbf{H} = f \ast w\)</span> and therefore has no temporal dependencies, so it is easier to parallelize over the individual words of a sentence.This process will compute each context as a function of a number of preceding words.</p>
<center>
<img src="/images/2017-09-14Language%20Modeling%20with%20Gated%20Convolutional%20Networks/ComputationGraph.png">
</center>
<p>Figure above illustrates the model architecture. Words are represented by a vector embedding stored in a lookup table <span class="math inline">\(\mathbf{D}^{|V| \times e}\)</span> where <span class="math inline">\(|V|\)</span> is the number of words in the vocabulary and <span class="math inline">\(e\)</span> is the embedding size. The input to the model is a sequence of words <span class="math inline">\(w_0,...,w_N\)</span> which are represented by word embeddings <span class="math inline">\(\mathbf{E} = [\mathbf{D}_{w_0},...,\mathbf{D}_{w_N} ]\)</span>. We compute the hidden layers <span class="math inline">\(h_0,...,h_L\)</span> as</p>
<p><span class="math display">\[h_l(\mathbf{X}) = (\mathbf{X} \ast \mathbf{W} + \mathbf{b}) \otimes \sigma(\mathbf{X} \ast \mathbf{V} + \mathbf{c})\]</span></p>
<p>Where <span class="math inline">\(m\)</span>, <span class="math inline">\(n\)</span> are respectively the number of input and output feature maps and <span class="math inline">\(k\)</span> is the patch size (kernel size), <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{N \times m}\)</span> is the input of layer <span class="math inline">\(h_l\)</span> (either word embeddings or the outputs of previous layers), <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{k \times m \times n}\)</span>, <span class="math inline">\(\mathbf{b} \in \mathbb{R}^n\)</span>, <span class="math inline">\(\mathbf{V} \in \mathbb{R}^{k \times m \times n}\)</span>, <span class="math inline">\(\mathbf{c} \in \mathbb{R}^n\)</span> are learned parameters, <span class="math inline">\(\sigma\)</span> is the sigmoid function and <span class="math inline">\(\otimes\)</span> is the element-wise product between matrices.</p>
<p>If the input is word embeddings, then <span class="math inline">\(m=e\)</span>. For one filter (<span class="math inline">\(k \times m\)</span>), the output will be a vector (<span class="math inline">\(n \times 1\)</span>); and for <span class="math inline">\(n\)</span> filters (<span class="math inline">\(k \times m \times n\)</span>), the output will be <span class="math inline">\(n\)</span> vectors which can be concatenated to a matrix (<span class="math inline">\(n \times m\)</span>).</p>
<p>When convolving inputs, take care that <span class="math inline">\(\mathbf{h}_i\)</span> does not contain information from future words. To address this, shift the convolutional inputs to prevent the kernels from seeing future context (Oord et al., 2016a). Specifically, zero-pad the beginning of the sequence with <span class="math inline">\(k − 1\)</span> elements, assuming the first input element is the beginning of sequence marker which we do not predict and <span class="math inline">\(k\)</span> is the width of the kernel.</p>
<p>The papre also wraps the convolution and the gated linear unit in a pre-activation residual block that adds the input of the block to the output (He et al., 2015a). The blocks have a bottleneck structure for computational efficiency and each block has up to 5 layers.</p>
<h1 id="reference">Reference</h1>
<p><a href="https://arxiv.org/pdf/1612.08083.pdf" target="_blank" rel="external">Language Modeling with Gated Convolutional Networks</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/14/2017-09-14Supervised Learning of Universal Sentence Representations from Natural Language Inference Data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/14/2017-09-14Supervised Learning of Universal Sentence Representations from Natural Language Inference Data/" itemprop="url">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-14T15:52:30+10:00">
                2017-09-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="contents">Contents</h1>
<ul>
<li>Introduction</li>
<li>The Natural Language Inference Task</li>
<li>Encoder Models
<ul>
<li>LSTM and GRU</li>
<li>BiLSTM With Mean/Max Pooling</li>
<li>Self-attentive Network</li>
<li>Hierarchical ConvNet</li>
</ul></li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>This paper studies the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks.</p>
<p>Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks. The experiments of this paper show that an encoder based on a bi-directional LSTM architecture with max pooling, trained the Stanford Natural Language Inference dataset, yields state-of-the-art sentence embeddings compared to all existing alternative unsupervised approaches like SkipThought or FastSent, while being much faster to train.</p>
<p>The difficulty for this task is that neural networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these biases.</p>
<h1 id="the-natural-language-inference-task">The Natural Language Inference Task</h1>
<p>The SNLI dataset consists of 570k human-generated English sentence pairs, manually labeled with one of three categories: entailment, contradiction and neutral. It captures natural language inference, also known in previous incarnations as Recognizing Textual Entailment (RTE), and constitutes one of the largest high-quality labeled resources explicitly constructed in order to require understanding sentence semantics.</p>
<center>
<img src="/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/GenericNLITrainingScheme.png">
</center>
<h1 id="encoder-models">Encoder Models</h1>
<h3 id="lstm-and-gru">LSTM and GRU</h3>
<p>Encoders apply recurrent neural networks using either LSTM or GRU modules, as in sequence to sequence encoders. For a sequence of T words <span class="math inline">\((w_1,...,w_T)\)</span>, the network computes a set of <span class="math inline">\(T\)</span> hidden representations <span class="math inline">\(h_1,h_2,...,h_T\)</span>, with <span class="math inline">\(h_t = \overrightarrow{LSTM}(w_1,...,w_T)\)</span> (or GRU units instead). A sentence is represented by the last hidden vector, <span class="math inline">\(h_T\)</span>.</p>
<h3 id="bilstm-with-meanmax-pooling">BiLSTM With Mean/Max Pooling</h3>
<center>
<img src="/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/BiLSTMMaxPoolingNetwork.png">
</center>
<span class="math display">\[\begin{equation}\begin{split}
\overrightarrow{h_t} &amp;= \overrightarrow{LSTM}_t(w_1,...,w_T)\\\\
\overleftarrow{h_t} &amp;= \overleftarrow{LSTM}_t(w_1,...,w_T)\\\\
h_t &amp;= [\overrightarrow{h_t}, \overleftarrow{h_t}]
\end{split}\end{equation}\]</span>
<h3 id="self-attentive-network">Self-attentive Network</h3>
<center>
<img src="/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/InnerAttentionNetworkArchitecture.png">
</center>
<span class="math display">\[\begin{equation}\begin{split}
\overline{h}_i &amp;= tanh(Wh_i + b_w)\\\\
\alpha_i &amp;= \frac{e^{\overline{h}^T_i u_w}}{\Sigma_i e^{\overline{h}^T_i u_w}}\\\\
u &amp;= \Sigma_i \alpha_i h_i
\end{split}\end{equation}\]</span>
<p>Where {<span class="math inline">\(h_1,...,h_T\)</span>} are the output vectors of a BiLSTM. These are fed to an affine transformation (<span class="math inline">\(W, b_w\)</span>) whcih outputs a set of keys (<span class="math inline">\(\overline{h}_1,...,\overline{h}_T\)</span>). The {<span class="math inline">\(\alpha_i\)</span>} represent the score of similarity between the keys and a learned context query vector <span class="math inline">\(u_w\)</span>. These weights are used to produce the final representation <span class="math inline">\(u\)</span>, which is a weighted linear combination of the hidden vectors.</p>
<h3 id="hierarchical-convnet">Hierarchical ConvNet</h3>
<center>
<img src="/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/HierarchicalConvNetArchitecture.png">
</center>
<p>One of the currently best performing models on classification tasks is a convolutional architecture. It concatenates different representations of the sentences at different level of abstractions. The paper introduces a faster version consisting of 4 convolutional layers. At every layer, a representation <span class="math inline">\(u_i\)</span> is computed by a max-pooling operation over the feature maps. The final representation <span class="math inline">\(u = [u_1,u_2,u_3,u_4]\)</span> concatenates representations at different levels of the input sentence. The model thus captures hierarchical abstractions of an input sentence in a fixed-size representation.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/13/2017-09-13Show attend and tell/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/13/2017-09-13Show attend and tell/" itemprop="url">Show Attend and Tell</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-13T10:01:52+10:00">
                2017-09-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Let’s introduce an example to explain attention mechanism. The task we want to achieve is image captioning: we want to generate a caption for a given image.</p>
<p>A <em>classic</em> image captioning system would encode the image, using a pre-trained Convolutional Neural Network that would produce a hidden state h. Then, it would decode this hidden state by using a Recurrent Neural Network (RNN), and generate recursively each word of the caption. See image below:</p>
<center>
<img src="/images/2017-09-13Show%20attend%20and%20tell/ClassicImageCaptioning.png">
</center>
<p>The problem with this method is that, when the model is trying to generate the next word of the caption, this word is usually describing only a part of the image. Using the whole representation of the image <span class="math inline">\(h\)</span> to condition the generation of each word cannot efficiently produce different words for different parts of the image. This is exactly where an attention mechanism is helpful.</p>
<p>With an attention mechanism, the image is first divided into <span class="math inline">\(L\)</span> parts, and we compute with a Convolutional Neural Network (CNN) representations of each part <span class="math inline">\(a = \{ a_1, ..., a_L \}\)</span>. When the RNN is generating a new word, the attention mechanism is focusing on the relevant part of the image, so the decoder only uses specific parts of the image.</p>
<p>On the figure below (upper row), we can see for each word of the caption what part of the image (in white) is used to generate it.</p>
<center>
<img src="/images/2017-09-13Show%20attend%20and%20tell/AttentionCaptioning.png">
</center>
<h2 id="what-is-an-attention-model">What is an attention model</h2>
<p>An attention model is a method that takes <span class="math inline">\(n\)</span> arguments <span class="math inline">\(y_1, ..., y_n\)</span> (in the precedent examples, the <span class="math inline">\(y_i\)</span> would be the <span class="math inline">\(a_i\)</span>), and a context <span class="math inline">\(c\)</span>. It returns a vector <span class="math inline">\(z\)</span> which is supposed to be the summary of the <span class="math inline">\(y_i\)</span>, focusing on information linked to the context <span class="math inline">\(c\)</span>. More formally, it returns a weighted arithmetic mean of the <span class="math inline">\(y_i\)</span>, and the weights are chosen according the relevance of each <span class="math inline">\(y_i\)</span> given the context <span class="math inline">\(c\)</span>.</p>
<p>In the example presented before, the context is the beginning of the generated sentence, the <span class="math inline">\(y_i\)</span> are the representations of the parts of the image (<span class="math inline">\(a_i\)</span>), and the output is a representation of the filtered image, with a filter putting the focus of the interesting part for the word currently generated.</p>
<p>One interesting feature of attention model is that the weight of the arithmetic means are accessible and can be plotted. This is exactly the figures we were showing before, a pixel is whiter if the weight of this image is high.</p>
<h2 id="show-attend-and-tell">Show Attend and Tell</h2>
<p>I’ll explain the details of paper <a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show Attend and Tell</a>.</p>
<p>The model takes a single raw image and generates a caption <span class="math inline">\(\mathbf{y}\)</span> encoded as a sequence of <span class="math inline">\(1-of-K\)</span> encoded words. <span class="math display">\[y = \{ \mathbf{y}_1,\mathbf{y}_2,...,\mathbf{y}_C \}, \space \mathbf{y}_i \in \mathbb{R}^K\]</span></p>
<h3 id="encoder-convolutional-features">Encoder: Convolutional Features</h3>
<p>The encoder uses a convolutional neural network to extract a set of feature vectors which refer to as annotation vectors. The extractor produces <span class="math inline">\(L\)</span> vectors, each of which is a D-dimensional representation corresponding to a part of the image. <span class="math display">\[a = \{ \mathbf{a}_1,\mathbf{a}_2,...,\mathbf{a}_L\}, \space \mathbf{a}_i \in \mathbb{R}^D\]</span> In order to obtain a correspondence between the feature vectors and portions of the 2-D image, the features are extracted from a lower convolutional layer. This allows the decoder to selectively focus on certain parts of an image by selecting a subset of all the feature vectors.</p>
<p>For a concrete example, we resize the input image to <span class="math inline">\(224 \times 224\)</span>. Feature vectors <span class="math inline">\(a\)</span> use <span class="math inline">\(14 \times 14 \times 512\)</span> dimensional feature maps of layer <span class="math inline">\(conv5\_3\)</span> of VGG net. So the number of regions is <span class="math inline">\(L = 14 \times 14 = 196\)</span>, and the dimension of each feature vector is <span class="math inline">\(D = 512\)</span>.</p>
<h3 id="decoder-lstm-network">Decoder: LSTM network</h3>
The decoder is a LSTM network that produces a caption bt generating one word at ecery time step conditioned on a context vector, the previous hidden state and the previously generated words.
<center>
<img src="/images/2017-09-13Show%20attend%20and%20tell/DecoderLSTM.png">
</center>
From the figure above, we can see:
<span class="math display">\[\begin{equation}\begin{split}
f_t &amp;= \sigma(W_f[Ey_{t-1}, h_{t-1}, z_t]) \\\\
i_t &amp;= \sigma(W_i[Ey_{t-1}, h_{t-1}, z_t]) \\\\
o_t &amp;= \sigma(W_o[Ey_{t-1}, h_{t-1}, z_t]) \\\\
g_t &amp;= tanh(W_g[Ey_{t-1}, h_{t-1}, z_t]) \\\\
c_t &amp;= f_t \odot c_{t-1} + i_t \odot g_t\\\\
h_t &amp;= o_t \odot tanh(c_t)
\end{split}\end{equation}\]</span>
<p>Here, <span class="math inline">\(i_t,f_t,c_t,o_t,h_t\)</span> are the input, forget, memory, output and hidden state of the LSTM, respectively.</p>
<ul>
<li><span class="math inline">\(y_{t-1}\)</span>: the previously generated word, <span class="math inline">\(y_i \in \mathbb{R}^K\)</span>, <span class="math inline">\(K\)</span> is the size of the vocabulary.</li>
<li><span class="math inline">\(E\)</span>: the Embedding lookup table which are learned parameters initialized randomly, <span class="math inline">\(E \in \mathbb{R}^{m \times K}\)</span>, <span class="math inline">\(m\)</span> denotes the Embedding dimensionality.</li>
<li><span class="math inline">\(z_t\)</span>: the context vector, it’s a function of annotation vectors <span class="math inline">\(a\)</span>, capturing the visual information associated with a particular input location, <span class="math inline">\(z_t \in \mathbb{R}^D\)</span></li>
</ul>
<p>It’s time to illustrate the attention part, that is where <span class="math inline">\(z_t\)</span> comes from.</p>
<p>In simple terms, the context vectors <span class="math inline">\(\mathbf{z}_t\)</span> is a dynamic representation of the relevant part of the image input at time <span class="math inline">\(t\)</span>. Mechanism <span class="math inline">\(\phi\)</span> is defined to compute <span class="math inline">\(\mathbf{z}_t\)</span> from annotatioin vectors <span class="math inline">\(\mathbf{a}_i, i=1,2,...,L\)</span> corresponding to the features extracted at different image locations. For each location <span class="math inline">\(i\)</span>, the mechanism generates a positive weight location <span class="math inline">\(\alpha_i\)</span> which can be interpreted either as the probability that location <span class="math inline">\(i\)</span> is the right place to focus for producing the next word (the ‘hard’ but stochastic attention mechanism), or as the relative importance to give to location <span class="math inline">\(i\)</span> in blending the <span class="math inline">\(a_i\)</span>’s together. The weight <span class="math inline">\(\alpha_i\)</span> of each annotation vectors <span class="math inline">\(a_i\)</span> is computed using an attention model <span class="math inline">\(f_{att}\)</span> which is a multilayer perceptron conditioned on the previous hidden state <span class="math inline">\(h_{t-1}\)</span> <span class="math display">\[e_{ti} = f_{att} ( \mathbf{a}_i, \mathbf{h}_{t-1} )\]</span> <span class="math display">\[\alpha_{ti} = \frac{exp(e_{ti})}{\Sigma^L_{k=1}exp} (e_{tk} )\]</span> Once the weigths (which sums to 1) are computed, the context vector <span class="math inline">\(\mathbf{z}_t\)</span> is computed by: <span class="math display">\[\mathbf{z}_t = \phi ( \{ \mathbf{a}_i \}, \{ \alpha_i \})\]</span> For <em><strong>soft attention</strong></em>: <span class="math display">\[\mathbf{z}_t = \phi ( \{ \mathbf{a}_i \}, \{ \alpha_i \}) = \Sigma^L_{i=1} \alpha_i \mathbf{a}_i\]</span></p>
<p>The initial memory state and hidden state of the LSTM are predicted by an average of the annotation vectors fed through two separate MLPs: <span class="math display">\[\mathbf{c}_0 = f_{init,c} (\frac{1}{L} \Sigma^L_i \mathbf{a}_1)\]</span> <span class="math display">\[\mathbf{h}_0 = f_{init,h} (\frac{1}{L} \Sigma^L_i \mathbf{a}_1)\]</span></p>
<h3 id="output-layer">Output Layer</h3>
<p>Output is a deep layer to compute the output word probability given the LSTM state, the context voector and the previous word: <span class="math display">\[p(y_t|a,y_{t-1}) \propto exp(L_0(Ey_{t-1} + L_hh_t + L_zz_t))\]</span></p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></li>
<li><a href="https://blog.heuritech.com/2016/01/20/attention-mechanism/" target="_blank" rel="external">Attention Mechanism</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/06/2017-09-06Siamese Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/06/2017-09-06Siamese Network/" itemprop="url">Siamese Network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-06T16:56:51+10:00">
                2017-09-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>The main idea of Siamese Network is to map the inputs to a new space by a function which can be learned by neural network. Then it is able to use similarity measurement like (Euclidean Metric or Cosine Similarity) to compare the extent of similarity. During training, the goal is to minimize the value of cost functon of the pairs of samples from same category, in the meantime, maximize the value of cost function of the pairs from different categories. For a given neural network (CNN, RNN), the goal is to optimize its parameters <span class="math inline">\(\mathbf{W}\)</span> to make:</p>
<p><span class="math inline">\(E_w = \left \| G_w(X_1) - G_w(X_2) \right \|\)</span>, small if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are in the same category</p>
<p><span class="math inline">\(E_w = \left \| G_w(X_1) - G_w(X_2) \right \|\)</span>, large if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are in the different categories</p>
<p>The only assumption about W is differentiable.</p>
<p>And the cost function is defined as below: <span class="math display">\[\mathcal{L}(W) = \Sigma^n_{i=1} (1-Y)L_G(E_w(x_1,x_2)^i) + YL_I(E_w(X_1,X_2)^i)\]</span></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/05/2017-09-05PixelCNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/05/2017-09-05PixelCNN/" itemprop="url">PixelCNN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-05T14:49:49+10:00">
                2017-09-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="content">Content</h1>
<ul>
<li>PixelCNN</li>
<li>Gated Convolutional Layer</li>
<li>BlindSpot And Solution</li>
<li>Conditional PixelCNN</li>
<li>fast PixelCNN ++</li>
<li>PixelRNN</li>
</ul>
<h2 id="pixelcnn">PixelCNN</h2>
<p>PixelCNNs (and PixelRNNs) model the joint distribution of pixels over an image <span class="math inline">\(x\)</span> as the following product of conditional distributions, where <span class="math inline">\(x_i\)</span> is a single pixel: <span class="math display">\[p(x) = \prod\limits^{n^2}_{i=1}p(x_i|x_1,...,x_{i-1})\]</span> The ordering of the pixel dependencies is in raster scan order: row by row and pixel by pixel within every row. Every pixel therefore depends on all the pixels above and to the left of it, and not on any of other pixels.</p>
<p>In PixelCNN every conditional distribution is modelled by a convolutional neural network. To make sure the CNN can only use information about pixels above and to the left of the current pixel, the convolution are <em><strong>masked</strong></em> as the figure below.</p>
<center>
<img src="/images/2017-09-05PixelCNN/PixelCNN.png">
</center>
<p>For image with only one channel (like mnist dataset): <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">filter_mid_h = shape[0]//2</div><div class="line">filter_mid_w = shape[1]//2</div><div class="line">mask_filter =np.ones(shape, dtype= np.float32)</div><div class="line">mask_filter[filter_mid_h,filter_mid_w+1:, :, :] = 0.</div><div class="line">mask_filter[filter_mid_h+1:, :, :, :] = 0.</div><div class="line"><span class="comment"># W *= mask_filter</span></div></pre></td></tr></table></figure></p>
<p>For a <span class="math inline">\(7 * 7\)</span> filter, the mask tensor should look like this: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.],</div><div class="line">       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],</div><div class="line">       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],</div><div class="line">       [ 1.,  1.,  1.,  0.,  0.,  0.,  0.],</div><div class="line">       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</div><div class="line">       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</div><div class="line">       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]])</div></pre></td></tr></table></figure></p>
<p>For colourful images with three channels (R,G,B), in the first layer, each of the RGB channels is connected to previous channels and to the context, but is not connected to itself. In subsequent layers, the channels are also connected to themselves (Shown below).</p>
<center>
<img src="/images/2017-09-05PixelCNN/RGB.png">
</center>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def bmask(i_in, i_out):</div><div class="line">	cin_idx = np.expand_dims(np.arange(Cin) % 3 == i_in, 1)</div><div class="line">    cout_idx = np.expand_dims(np.arange(Cout) % 3 == i_out, 0)</div><div class="line">    a1, a2 = np.broadcast_arrays(cin_idx, cout_idx)</div><div class="line">    <span class="built_in">return</span> a1 * a2</div><div class="line"></div><div class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(3):</div><div class="line">    mask_filter[filter_mid_h,filter_mid_w][bmask(j,j)] = 0. <span class="keyword">if</span> mask ==<span class="string">"A"</span> <span class="keyword">else</span> 1.</div><div class="line"></div><div class="line">mask_filter[filter_mid_h,filter_mid_w][bmask(1, 0)] = 0.</div><div class="line">mask_filter[filter_mid_h,filter_mid_w][bmask(2, 0)] = 0.</div><div class="line">mask_filter[filter_mid_h,filter_mid_w][bmask(2, 1)] = 0.</div></pre></td></tr></table></figure>
<p>For example, <span class="math inline">\(i_{in} = 3\)</span>, <span class="math inline">\(i_{out} = 6\)</span>, the last two dimensions should look like this: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Mask A</span></div><div class="line">array([[ 0.,  1.,  1.,  0.,  1.,  1.],</div><div class="line">       [ 0.,  0.,  1.,  0.,  0.,  1.],</div><div class="line">       [ 0.,  0.,  0.,  0.,  0.,  0.]])</div><div class="line"><span class="comment"># Mask B</span></div><div class="line">array([[ 1.,  1.,  1.,  1.,  1.,  1.],</div><div class="line">       [ 0.,  1.,  1.,  0.,  1.,  1.],</div><div class="line">       [ 0.,  0.,  1.,  0.,  0.,  1.]])</div></pre></td></tr></table></figure></p>
<center>
<img src="/images/2017-09-05PixelCNN/PixelCNNPred.png">
</center>
<p>As shown above, the 256 possible values for each colour channel are then modelled using a softmax.</p>
<p>PixelCNN typically consists of a stack of masked convolutional layers that takes an <span class="math inline">\((N,N,3)\)</span> image as input and produces <span class="math inline">\((N,N,3,256)\)</span> predictions as output. The use of convolutions allows the predictions for all the pixels to be made in parallel during training.</p>
<p>During sampling the predictions are sequential: every time a pixel is predicted, it is fed back into the network to predict the next pixel. This sequentiality is essential to generating high quality images, as it allows every pixel to depend in a highly non-linear and multimodal way on the previous pixels.</p>
<h2 id="gated-convolutional-layer">Gated Convolutional Layer</h2>
<p>For a gated convolutional layer, relu activation block with more complex combination of sigmoid (as a forget gate) and tanh (as real activation). It suggests that: <span class="math display">\[\mathbf{y} = tanh(W_{k,f} \ast \mathbf{x}) \odot \sigma(W_{k,g} \ast \mathbf{x})\]</span></p>
<p>Here is the code to illustrate the formula: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">W_f = get_weights(W_shape, <span class="string">"v_W"</span>, mask=mask)</div><div class="line">W_g = get_weights(W_shape, <span class="string">"h_W"</span>, mask=mask)</div><div class="line"></div><div class="line">b_f = tf.get_variable(<span class="string">"v_b"</span> , b_shape, tf.float32, tf.zeros_initializer)</div><div class="line">b_g = tf.get_variable(<span class="string">"h_b"</span> , b_shape, tf.float32, tf.zeros_initializer)</div><div class="line"></div><div class="line">conv_f = tf.nn.conv2d(input_layer, W_f, strides=[1,1,1,1], padding=<span class="string">'SAME'</span>)</div><div class="line">conv_g = tf.nn.conv2d(input_layer, W_g, strides=[1,1,1,1], padding=<span class="string">'SAME'</span>)</div><div class="line">output_layer = tf.mul(tf.tanh(conv_f + b_f), tf.sigmoid(conv_g+ b_g))</div></pre></td></tr></table></figure></p>
<h2 id="blindspot-and-solution">BlindSpot And Solution</h2>
<p>Note that a significant portion of the input image is ignored by the masked convolutional architecture. This ‘blind spot’ can cover as much as a quarter of the potential receptive field (e.g., when using 3x3 filters), meaning that none of the content to the right of the current pixel would be taken into account.</p>
<center>
<img src="/images/2017-09-05PixelCNN/BlindSpot.png">
</center>
<p>To gain some intuition about blind spot, following code may help: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># create 5x5 input</span></div><div class="line"><span class="comment"># chainer requires input to have shape [BATCH, CHANNELS, HEIGHT, WIDTH]</span></div><div class="line">input = np.arange(25).reshape([1,1,5,5]).astype(<span class="string">'f'</span>)</div><div class="line"><span class="comment"># array([[[[    0.,     1.,     2.,     3.,     4.],</span></div><div class="line"><span class="comment">#          [    5.,     6.,     7.,     8.,     9.],</span></div><div class="line"><span class="comment">#          [   10.,    11.,    12.,    13.,    14.],</span></div><div class="line"><span class="comment">#          [   15.,    16.,    17.,    18.,    19.],</span></div><div class="line"><span class="comment">#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)</span></div><div class="line"></div><div class="line"><span class="comment"># create kernel of ones so it just sums all values within</span></div><div class="line"><span class="comment"># use one for simplicity: easy to check</span></div><div class="line">kernel = np.ones([3, 3])</div><div class="line"><span class="comment"># turn to proper type 'A' mask</span></div><div class="line">kernel[2:, :] = 0.0</div><div class="line">kernel[1, 1:] = 0.0</div><div class="line"><span class="comment"># array([[ 1.,  1.,  1.],</span></div><div class="line"><span class="comment">#        [ 1.,  0.,  0.],</span></div><div class="line"><span class="comment">#        [ 0.,  0.,  0.]])</span></div><div class="line"></div><div class="line"><span class="comment"># create two convolution layers with total receptive field size 5x5</span></div><div class="line"><span class="comment"># so out input is exact fit</span></div><div class="line">import chainer.links as L</div><div class="line"></div><div class="line">l1 = L.Convolution2D(1, 1, ksize=3, initialW=kernel)</div><div class="line">l2 = L.Convolution2D(1, 1, ksize=3, initialW=kernel)</div><div class="line"></div><div class="line"><span class="comment"># here is the trick: pixel at [1, 4] position will be inside blind spot</span></div><div class="line"><span class="comment"># if we perform convolution its value won't be included in final sum</span></div><div class="line"><span class="comment"># so let's increase its value so it would be easy to check</span></div><div class="line">input[:, :, 1, 4] = 1000</div><div class="line"><span class="comment"># array([[[[    0.,     1.,     2.,     3.,     4.],</span></div><div class="line"><span class="comment">#          [    5.,     6.,     7.,     8.,  1000.],</span></div><div class="line"><span class="comment">#          [   10.,    11.,    12.,    13.,    14.],</span></div><div class="line"><span class="comment">#          [   15.,    16.,    17.,    18.,    19.],</span></div><div class="line"><span class="comment">#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)</span></div><div class="line"></div><div class="line">output = l2(l1(input)).data</div><div class="line"><span class="comment"># array([[[[ 64.]]]], dtype=float32)</span></div><div class="line"><span class="comment"># Viola! Sum is lesser that 1000 which means pixel at [1, 4] wasn't seen!</span></div><div class="line"></div><div class="line"><span class="comment"># Otherwise, let's return it value back</span></div><div class="line">input[:, :, 1, 4] = 9</div><div class="line"><span class="comment"># array([[[[    0.,     1.,     2.,     3.,     4.],</span></div><div class="line"><span class="comment">#          [    5.,     6.,     7.,     8.,     9.],</span></div><div class="line"><span class="comment">#          [   10.,    11.,    12.,    13.,    14.],</span></div><div class="line"><span class="comment">#          [   15.,    16.,    17.,    18.,    19.],</span></div><div class="line"><span class="comment">#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)</span></div><div class="line"></div><div class="line"><span class="comment"># perform computation again..</span></div><div class="line">output = l2(l1(input)).data</div><div class="line"><span class="comment"># array([[[[ 64.]]]], dtype=float32)</span></div><div class="line"><span class="comment"># Another evidence: no matter what value we assign to it final sum doesn't change</span></div><div class="line"><span class="comment"># That proves it's within blind spot and we can't access information at it.</span></div></pre></td></tr></table></figure></p>
<p>To remove the blind spot, authors introduce neat idea: they split convolution into two different operations: two separate stacks - vertical and horizontal.</p>
<center>
<img src="/images/2017-09-05PixelCNN/PixelCNNStack.png">
</center>
<p>Here we have horizontal stack (in purple): convolution operation that conditions on only current row, so it has access to left pixels. Vertical stack (blue) has access to all top pixels. Implementation details would follow. Note that horizontal and vertical stacks are sort of independent: vertical stack should not access any information horizontal stack has: otherwise it will have access to pixels it shouldn’t see. But vertical stack can be connected to vertical as it predicts pixel following those in vertical stack.</p>
<h2 id="conditional-pixelcnn">Conditional PixelCNN</h2>
<p>Given a high-level image description represented as a latent vector <span class="math inline">\(\mathbf{h}\)</span>, we seek to model the conditional distribution <em><span class="math inline">\(p(\mathbf{x}|\mathbf{h})\)</span></em> of images suiting this description. Formally the conditional PixelCNN models the following distribution: <span class="math display">\[p(\mathbf{x}|\mathbf{h}) = \prod\limits^{n^2}_{i=1}p(x_i|x_1,x_2,...,x_{i-1},\mathbf{h})\]</span></p>
<p>To model the conditional distribution by adding terms that depend on <span class="math inline">\(\mathbf{h}\)</span> to the activations before the nonlinearities, which now becomes: <span class="math display">\[\mathbf{y} = tanh(W_{k,f} \ast \mathbf{x} + V^T_{k,f} \mathbf{h}) \odot \sigma(W_{k,g} \ast \mathbf{x} + V^T_{k,g} \mathbf{h})\]</span></p>
<p>where k is the layer number. If <span class="math inline">\(\mathbf{h}\)</span> is a one-hot encoding that specifies a class this is equivalent to adding a class dependent bias at every layer. Notice that the conditioning does not depend on the location of the pixel in the image; this is appropriate as long as <span class="math inline">\(\mathbf{h}\)</span> only contains information about what should be in the image and not where.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">def conditional_gated_conv(self):</div><div class="line">    W_f = get_weights(self.W_shape, <span class="string">"v_W"</span>, mask=self.mask)</div><div class="line">    W_g = get_weights(self.W_shape, <span class="string">"h_W"</span>, mask=self.mask)</div><div class="line">    <span class="keyword">if</span> self.conditional is not None:</div><div class="line">        h_shape = int(self.conditional.get_shape()[1])</div><div class="line">        V_f = get_weights([h_shape, self.W_shape[3]], <span class="string">"v_V"</span>)</div><div class="line">        b_f = tf.matmul(self.conditional, V_f)</div><div class="line">        V_g = get_weights([h_shape, self.W_shape[3]], <span class="string">"h_V"</span>)</div><div class="line">        b_g = tf.matmul(self.conditional, V_g)</div><div class="line"></div><div class="line">        b_f_shape = tf.shape(b_f)</div><div class="line">        b_f = tf.reshape(b_f, (b_f_shape[0], 1, 1, b_f_shape[1]))</div><div class="line">	    b_g_shape = tf.shape(b_g)</div><div class="line">        b_g = tf.reshape(b_g, (b_g_shape[0], 1, 1, b_g_shape[1]))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        b_f = get_bias(self.b_shape, <span class="string">"v_b"</span>)</div><div class="line">        b_g = get_bias(self.b_shape, <span class="string">"h_b"</span>)</div><div class="line"></div><div class="line">    conv_f = conv_op(self.fan_in, W_f)</div><div class="line">    conv_g = conv_op(self.fan_in, W_g)</div><div class="line">       </div><div class="line">    self.fan_out = tf.multiply(tf.tanh(conv_f + b_f), tf.sigmoid(conv_g + b_g))</div></pre></td></tr></table></figure>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/1601.06759.pdf" target="_blank" rel="external">Pixel Recurrent Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/1606.05328.pdf" target="_blank" rel="external">Conditional Image Generation with PixelCNN Decoders</a></li>
<li><a href="http://sergeiturukin.com/2017/02/24/gated-pixelcnn.html" target="_blank" rel="external">sergeiturukin’s Post</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/04/2017-09-04The Intuition Behind LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/04/2017-09-04The Intuition Behind LSTM/" itemprop="url">Intuition Behind LSTM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-04T10:55:06+10:00">
                2017-09-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="content">Content</h1>
<ul>
<li>A mathematically Sufficient Condition For Vanishing Sensitivity</li>
<li>A minimum Weight Initialization to Avoid Vanishing Gradients</li>
<li>Written memories: the intuition behind LSTMs</li>
<li>Different Between Tanh and Sigmoid Function</li>
</ul>
<h2 id="a-mathematically-sufficient-condition-for-vanishing-sensitivity">A mathematically Sufficient Condition For Vanishing Sensitivity</h2>
<p>This is a mathematical proof of a sufficient condition for vanishing sensitivity in vanilla RNNs. The proof here also takes advantage of the mean value theorem to go one step further than Pascanu et al. and reach a slightly stronger result, effectively showing vanishing causation rather than vanishing sensitivity. Note that mathematical analyses of vanishing and exploding gradients date back to the early 1990s, in Bengio et al. (1994) and Hochreiter (1991) (original in German, relevant portions summarized in Hochreiter and Schmidhuber (1997)).</p>
To begin, from the definition of a vanilla RNN cell, we have: <span class="math display">\[s_{t+1} = \phi (z_t), where \space z_t = Ws_t + Ux_{t+1} + b\]</span> Recall the <em><strong>Mean value theorem</strong></em>: If a function <span class="math inline">\(f\)</span> is continuous on the closed interval <span class="math inline">\([a,b]\)</span>, and differentiable on the open interval <span class="math inline">\((a,b)\)</span> , then there exists a point <span class="math inline">\(c\)</span> in <span class="math inline">\((a,b)\)</span> such that: <span class="math display">\[f&#39;(c) = \frac{f(b)-f(a)}{b-a}\]</span> Applying the mean value theorem, we get that there exists <span class="math inline">\(c \in [z_t, z_t + \Delta z_t]\)</span> such that:
<span class="math display">\[\begin{equation}\begin{split}
\Delta S_{t+1} &amp;= [\phi&#39;(c)] \Delta z_t\\\\
&amp;= [\phi&#39;(c)] \Delta (Ws_t)\\\\
&amp;= [\phi&#39;(c)] W\Delta(s_t)
\end{split}\end{equation}\]</span>
<p>Now let <span class="math inline">\(\left \| A \right \|\)</span> represent the matrix 2-norm, <span class="math inline">\(\left | v \right |\)</span>the Euclidean vector norm, and define: <span class="math display">\[\gamma = sup_{c \in \{z_t, z_t + \Delta z_t\}} \left \| [\phi&#39; (c)] \right \|\]</span> Note that for the logistic sigmoid, <span class="math inline">\(\gamma \leq \frac{1}{4}\)</span>, and for tanh, <span class="math inline">\(\gamma \leq 1\)</span>.</p>
Taking the vector norm of each side, we obtain, where the first inequality comes from the definition of the 2-norm (applied twice), and second from the definition of supremum:
<span class="math display">\[\begin{equation}\begin{split}
\left | \Delta s_{t+1} \right | &amp;= \left | [\phi&#39;(c)] W \Delta s_t \right |\\\\
&amp;\leq \left \| [\phi&#39;(c)] \right \| \left \| W \right \| \left | \Delta s_t \right |\\\\
&amp;\leq \gamma \left \| W \right \| \left | \Delta s_t \right |\\\\
&amp;= \left \| \gamma W \right \| \left | \Delta s_t \right |
\end{split}\end{equation}\]</span>
<p>By expanding this formula over <span class="math inline">\(k\)</span> time steps we get: <span class="math display">\[\left | \Delta s_{t+k} \right | \leq \left \| \gamma W \right \|^k \left | \Delta s_t \right |\]</span> So that: <span class="math display">\[\frac{\left | \Delta s_{t+k} \right |}{\left | \Delta s_t \right |} = \left \| \gamma W \right \|^k\]</span></p>
<p>Therefore, if <span class="math inline">\(\left \| \gamma W \right \| &lt; 1\)</span> , we have that <span class="math inline">\(\frac{\left | \Delta s_{t+k} \right |}{\left | \Delta s_t \right |}\)</span> decreases exponentially in time.</p>
<h2 id="a-minimum-weight-initialization-to-avoid-vanishing-gradients">A minimum Weight Initialization to Avoid Vanishing Gradients</h2>
<p>It is beneficial to find a weight initialization that will not immediately suffer from this problem. Extending the above analysis to find the initialization of <span class="math inline">\(W\)</span> that gets us as close to equality as possible leads to a nice result. First, let us assume that <span class="math inline">\(\phi = tanh\)</span> and take <span class="math inline">\(\gamma = 1\)</span>. Our goal is to find an initialization of W for which:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\left \| \gamma W \right \| = 1\)</span></li>
<li>We get as close to equality as possible in above equation.</li>
</ol>
<p>From point 1, since we took <span class="math inline">\(\gamma\)</span> to be 1, we have <span class="math inline">\(\left \| W \right \| = 1\)</span>. From point 2, we get that we should try to set all singular values of <span class="math inline">\(W\)</span> to 1, not just the largest. Then, if all singular values of <span class="math inline">\(W\)</span> equal 1, that means that the norm of each column of <span class="math inline">\(W\)</span> is 1 (since each column is <span class="math inline">\(We_i\)</span> for some elementary basis vector <span class="math inline">\(e_i\)</span> and we have <span class="math inline">\(\left |We_i \right | = \left | e_i \right | = 1\)</span>). That means that for column <span class="math inline">\(j\)</span> we have: <span class="math display">\[\Sigma_i w_{ij}^2 = 1\]</span></p>
<p>There are <span class="math inline">\(n\)</span> entries in column <span class="math inline">\(j\)</span>, and we are choosing each from the same random distribution, so let us find a distribution for a random weight <span class="math inline">\(w\)</span> for which: <span class="math display">\[n\mathbb{E}(w^2) = 1\]</span></p>
<p>Now let’s suppose we want to initialize <span class="math inline">\(w\)</span> uniformly in the interval <span class="math inline">\([-R,R]\)</span>. Then the mean of <span class="math inline">\(w\)</span> is 0, so that, by definition, <span class="math inline">\(\mathbb{E}(w^2)\)</span> is its variance, <span class="math inline">\(\mathbb{V}(w)\)</span>. The variance of a uniform distribution over the interval <span class="math inline">\([a,b]\)</span> is given by <span class="math inline">\(\frac{(b-a)^2}{12}\)</span>, from which we get <span class="math inline">\(\mathbb{V}(w) = \frac{R^2}{3}\)</span>. Substituting this into our equation we get: <span class="math display">\[n\frac{R^2}{3} = 1\]</span></p>
<p>So that: <span class="math display">\[R = \frac{\sqrt{3}}{\sqrt{n}}\]</span></p>
<p>This is a nice result because it is the Xavier-Glorot initialization for a square weight matrix, yet was motivated by a different idea. The Xavier-Glorot initialization, introduced by Glorot and Bengio (2010), has proven to be an effective weight initialization prescription in practice. More generally, the Xavier-Glorot prescription applies to m-by-n weight matrices used in a layer that has an activation function whose derivative is near one at the origin (like tanh), and says that we should initialize our weights according to a uniform distribution of the interval: <span class="math display">\[[-\frac{\sqrt{6}}{\sqrt{m+n}},+\frac{\sqrt{6}}{\sqrt{m+n}}]\]</span></p>
<p>We saw above that good weight initializations are crucial, but this only impacts the start of training.</p>
<h2 id="written-memories-the-intuition-behind-lstms">Written memories: the intuition behind LSTMs</h2>
<p>Very much like the messages passed by children playing a game of broken telephone, information is morphed by RNN cells and the original message is lost. A small change in the original message may not have made any difference in the final message, or it may have resulted in something completely different.</p>
<p>How can we protect the integrity of messages? This is the fundamental principle of LSTMs: to ensure the integrity of our messages in the real world, we write them down. Writing is <em><strong>a delta to the current state</strong></em>: it is an act of creation (pen on paper) or destruction (carving in stone); the subject itself does not morph when you write on it and the error gradient on the backward-pass is constant.</p>
<p>This is precisely what was proposed by the landmark paper of Hocreiter and Schmidhuber (1997), which introduced the LSTM. They asked: “how can we achieve constant error flow through a single unit with a single connection to itself [i.e., a single piece of isolated information]?”</p>
<p>The answer, quite simply, is to avoid information morphing: changes to the state of an LSTM are explicitly written in, by an explicit addition or subtraction, so that each element of the state stays constant without outside interference: “the unit’s activation has to remain constant … this will be ensured by using the identity function”.</p>
<span class="math display">\[\begin{equation}\begin{split}
f_t &amp;= \sigma (W_f \cdot [h_{t-1}, x_t] + b_f)\\\\
i_t &amp;= \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\\\
\tilde{C}_t &amp;= tanh(W_c \cdot [h_{t-1}, x_t] +b_c)\\\\
C_t &amp;= f_t \ast C_{t-1} + i_t \ast \tilde{C}_t \\\\
o_t &amp;= \sigma (W_o \cdot [h_{t-1}, x_t] + b_o)\\\\
h_t &amp;= o_t \ast tanh(C_t)
\end{split}\end{equation}\]</span>
<p>Suppose we have calculated the value of <span class="math inline">\(\frac{\partial L}{C_{t+k}}\)</span>, then: <span class="math display">\[\frac{\partial L}{C_t} = \frac{\partial L}{C_{t+k}} \odot f_{t+k-1} \odot f_{t+k-2} ... \odot f_{t}\]</span> Except for the very first term, the vanishing problem still exist.</p>
<h2 id="different-between-tanh-and-sigmoid-function">Different Between Tanh and Sigmoid Function</h2>
<ul>
<li>Sigmoid function have domain of all real numbers, ranging from 0 to 1, campared with Tanh function whose domain ranging from -1 to 1. The problem of Sigmoid is that the activation value is always positive, which give rise to the phenomenon that the derivative of weights are always all positive or all negative.</li>
<li>When a sigmoidal activation function must be used, the hyperbolic tangent activation function typically performs better than the logistic sigmoid. It <em><strong>resembles the identity function more closely</strong></em>, in the sense that <span class="math inline">\(tanh(0) = 0\)</span> while <span class="math inline">\(\sigma(0) = \frac{1}{2}\)</span>. Because tanh is similar to the identity function near 0, training a deep neural network <span class="math inline">\(\hat{y} = W^T tanh(U^T tanh(V^TX))\)</span> resembles training a linear model <span class="math inline">\(\hat{y} = W^TU^TV^TX\)</span> so long as the activations of the network can be kept small. This makes training the tanh network easier.</li>
</ul>
<h1 id="reference">Reference</h1>
<p><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#written-memories-the-intuition-behind-lstms" target="_blank" rel="external">Written memories the intuition behind lstms</a> <a href="http://www.deeplearningbook.org/contents/mlp.html" target="_blank" rel="external">Deep Learning Chap6</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  








  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>

<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta property="og:type" content="website">
<meta property="og:title" content="EmberNLP">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EmberNLP">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/31/Optimization Methods in Deep Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/31/Optimization Methods in Deep Learning/" itemprop="url">Optimization Methods in Deep Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-31T15:56:48+10:00">
                2017-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="why-gradient-descend-works">Why Gradient Descend Works?</h1>
Gradient Descend is the fundamental optimization method to learn parameters. To illustrate why it works, let’s take the simple example $f(x) = (x - 1)^2 $ to gain some intuition.
<center>
<img src="/images/Optimization/GD_intuition.png">
</center>
<ul>
<li>For point <span class="math inline">\(x=3\)</span>, <span class="math inline">\(f&#39;(x)=4&gt;0\)</span></li>
<li>For point <span class="math inline">\(x=-1\)</span>, <span class="math inline">\(f&#39;(x)=-4&lt;0\)</span></li>
</ul>
<p>If we want to decreace the value of <span class="math inline">\(f(x)\)</span>:<br>
- For point <span class="math inline">\(x=3\)</span>, we need to move a small step towards the opposite direction of <span class="math inline">\(f&#39;(x=3)\)</span><br>
- For point <span class="math inline">\(x=-1\)</span>, we need to move a samll step towards the opposite direction of <span class="math inline">\(f&#39;(x=-1)\)</span></p>
<p>So, if we want to descreace the value of a given function, we just move a small step towards the opposite direction of the function’s gradient.</p>
<h1 id="the-difference-among-gd-sgd-batch-sgd">The difference among GD, SGD, Batch-SGD</h1>
For <em><strong>Gradient Descend</strong></em>, take gradient steps with respect to all training examples on each step:
<center>
<img src="/images/Optimization/SGD_GD.png">
</center>
For <em><strong>Stochastic Gradient Descend</strong></em>, take gradient steps with respect to just one random training example on each step:
<center>
<img src="/images/Optimization/SGD_BGD.png">
</center>
<p>For <em><strong>Batch Gradient Descend</strong></em>, take gradient steps with respect to m training examples on each step.</p>
<h1 id="momentum">Momentum</h1>
<p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will <em><strong>oscillate</strong></em> toward convergence. Using momentum can reduce these oscillations.</p>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the <em><strong>direction</strong></em> of the previous gradients in the variable <span class="math inline">\(v\)</span>. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of <span class="math inline">\(v\)</span> as the <em><strong>velocity</strong></em> of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.</p>
<center>
<img src="/images/Optimization/Momentum.png">
</center>
<p>The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence <span class="math inline">\(v\)</span> and then take a step in the direction of <span class="math inline">\(v\)</span>.</p>
<p>First, let’s gain some intuition from <em><strong>Exponentially Weighted Average</strong></em>.<br>
Let <span class="math inline">\(v\)</span> denote the variable for later use, and <span class="math inline">\(\theta\)</span> is the current observation <span class="math display">\[v_0 = 0\]</span> <span class="math display">\[v_1 = \beta v_0 + (1-\beta)\theta_1\]</span> <span class="math display">\[v_2 = \beta v_1 + (1-\beta)\theta_2\]</span> <span class="math display">\[.\]</span> <span class="math display">\[.\]</span> <span class="math display">\[.\]</span> <span class="math display">\[v_t = \beta v_{t-1} + (1-\beta)\theta_t\]</span></p>
The intuition behind the above equation is that:
<span class="math display">\[\begin{equation}\begin{split}
v_t &amp;= \beta v_{t-1} + (1-\beta)\theta_t\\\\
&amp;= (1 - \beta)\theta_t + \beta(\beta v_{t-2} + (1-\beta)\theta_{t-1})\\\\
&amp;=(1-\beta)\theta_t + \beta(1-\beta)\theta_{t-1} + \beta^2v_{t-2}\\\\
&amp;=(1-\beta)\theta_t + \beta(1-\beta)\theta_{t-1} + \beta^2(1-\beta)\theta_{t-2} + ...
\end{split}\end{equation}\]</span>
<p>It’s just the exponentially weighted average of all the observations.</p>
<p>Inspired by the above equations, we define the gradient descent with momentum as follows: <span class="math display">\[v_{d_w} = \beta v_{d_w} + (1 - \beta)d_w\]</span> <span class="math display">\[v_{d_b} = \beta v_{d_b} + (1 - \beta)d_b\]</span> <span class="math display">\[w := w - \alpha v_{d_w}\]</span> <span class="math display">\[b := w - \alpha v_{d_b}\]</span></p>
<p>How do you choose <span class="math inline">\(\beta\)</span>?<br>
- The larger the momentum <span class="math inline">\(\beta\)</span> is, the smoother the update because the more we take the past gradients into account. But if <span class="math inline">\(\beta\)</span> is too big, it could also smooth out the updates too much.<br>
- Common values for <span class="math inline">\(\beta\)</span> range from 0.8 to 0.999. If you don’t feel inclined to tune this, β=0.9 is often a reasonable default.<br>
- Tuning the optimal <span class="math inline">\(\beta\)</span> for your model might need trying several values to see what works best in term of reducing the value of the cost function.</p>
<p>Example code: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using Momentum</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></div><div class="line"><span class="string">                    v['dW' + str(l)] = ...</span></div><div class="line"><span class="string">                    v['db' + str(l)] = ...</span></div><div class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line"></div><div class="line">    L = len(parameters) // 2 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    </div><div class="line">    <span class="comment"># Momentum update for each parameter</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        </div><div class="line">        <span class="comment"># compute velocities</span></div><div class="line">        v[<span class="string">"dW"</span> + str(l+1)] = beta * v[<span class="string">"dW"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        v[<span class="string">"db"</span> + str(l+1)] = beta * v[<span class="string">"db"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"db"</span> + str(l+1)]</div><div class="line">        <span class="comment"># update parameters</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * v[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * v[<span class="string">"db"</span> + str(l+1)]</div><div class="line">        </div><div class="line">    <span class="built_in">return</span> parameters, v</div></pre></td></tr></table></figure></p>
<h1 id="rmsprop">RMSprop</h1>
<p>RMSprop, which stands for root mean square prop, is also inspired by the <em>Exponentially Weighted Average</em>. The algorithm is:<br>
<span class="math display">\[s_{d_w} = \beta s_{d_w} + (1 - \beta)d_w^2\]</span> <span class="math display">\[s_{d_b} = \beta s_{d_b} + (1 - \beta)d_b^2\]</span> <span class="math display">\[w := w - \alpha \frac{d_w}{\sqrt{s_{d_w}}}\]</span> <span class="math display">\[b := b - \alpha \frac{d_b}{\sqrt{s_{d_b}}}\]</span></p>
<p>The intuition behind is also to damp out the huge oscillations. Let’s say <span class="math inline">\(d_w\)</span> is larger than <span class="math inline">\(d_b\)</span>, then <span class="math inline">\(s_{d_w}\)</span> is also larger than <span class="math inline">\(s_{d_b}\)</span>. When we update parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>, <span class="math inline">\(d_w\)</span> is divided by a relatively larger number than <span class="math inline">\(d_b\)</span> whcih helps to damp out oscillations.</p>
<p>The common choice of <span class="math inline">\(\beta\)</span> is 0.999.</p>
<p>Example code: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_RMSprop(parameters, grads, s, beta, learning_rate):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using RMSprop</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    s -- python dictionary containing the current squared gradient:</span></div><div class="line"><span class="string">                    s['dW' + str(l)] = ...</span></div><div class="line"><span class="string">                    s['db' + str(l)] = ...</span></div><div class="line"><span class="string">    beta -- the RMSprop hyperparameter, scalar</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    s -- python dictionary containing your updated squared gradient</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line"></div><div class="line">    L = len(parameters) // 2 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    </div><div class="line">    <span class="comment"># RMSprop update for each parameter</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        </div><div class="line">        <span class="comment"># compute current squared gradient</span></div><div class="line">        s[<span class="string">"dW"</span> + str(l+1)] = beta * s[<span class="string">"dW"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"dW"</span> + str(l+1)] ** 2</div><div class="line">        s[<span class="string">"db"</span> + str(l+1)] = beta * s[<span class="string">"db"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"db"</span> + str(l+1)] ** 2</div><div class="line">        <span class="comment"># update parameters</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * s[<span class="string">"dW"</span> + str(l+1)] / np.sqrt(s[<span class="string">"dW"</span> + str(l+1)])</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * s[<span class="string">"db"</span> + str(l+1)] / np.sqrt(s[<span class="string">"db"</span> + str(l+1)])</div><div class="line">        </div><div class="line">    <span class="built_in">return</span> parameters, s</div></pre></td></tr></table></figure></p>
<h1 id="adam">Adam</h1>
<p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. The update rule is, for <span class="math inline">\(l = 1,...L\)</span>: <span class="math display">\[v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial Cost}{\partial W^{[l]}}\]</span> <span class="math display">\[v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t}\]</span> <span class="math display">\[s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial Cost}{\partial W^{[l]}})^2\]</span> <span class="math display">\[s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_2)^t}\]</span> <span class="math display">\[W^{[L]} = W^{[L]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}} + \epsilon}}\]</span></p>
<p>where:<br>
- t counts the number of steps taken of Adam<br>
- L is the number of layers<br>
- <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters that control the two exponentially weighted averages.<br>
- <span class="math inline">\(\alpha\)</span> is the learning rate<br>
- <span class="math inline">\(\epsilon\)</span> is a very small number to avoid dividing by zero.</p>
<p>The common choice of hyperparameters s <span class="math inline">\(\beta_1 = 0.9. \beta_2 = 0.999, \epsilon = 1e-8\)</span></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,</div><div class="line">                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using Adam</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></div><div class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></div><div class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></div><div class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></div><div class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></div><div class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line">    </div><div class="line">    L = len(parameters) // 2                 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    v_corrected = &#123;&#125;                         <span class="comment"># Initializing first moment estimate, python dictionary</span></div><div class="line">    s_corrected = &#123;&#125;                         <span class="comment"># Initializing second moment estimate, python dictionary</span></div><div class="line">    </div><div class="line">    <span class="comment"># Perform Adam update on all parameters</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        <span class="comment"># Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".</span></div><div class="line">        v[<span class="string">"dW"</span> + str(l+1)] = beta1 * v[<span class="string">"dW"</span> + str(l+1)] + (1 - beta1) * grads[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        v[<span class="string">"db"</span> + str(l+1)] = beta1 * v[<span class="string">"db"</span> + str(l+1)] + (1 - beta1) * grads[<span class="string">"db"</span> + str(l+1)]</div><div class="line"></div><div class="line">        <span class="comment"># Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".</span></div><div class="line">        v_corrected[<span class="string">"dW"</span> + str(l+1)] = v[<span class="string">"dW"</span> + str(l+1)] / (1 - beta1 ** t)</div><div class="line">        v_corrected[<span class="string">"db"</span> + str(l+1)] = v[<span class="string">"db"</span> + str(l+1)] / (1 - beta1 ** t)</div><div class="line"></div><div class="line">        <span class="comment"># Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".</span></div><div class="line">        s[<span class="string">"dW"</span> + str(l+1)] = beta2 * s[<span class="string">"dW"</span> + str(l+1)] + (1 - beta2) * grads[<span class="string">"dW"</span> + str(l+1)] ** 2</div><div class="line">        s[<span class="string">"db"</span> + str(l+1)] = beta2 * s[<span class="string">"db"</span> + str(l+1)] + (1 - beta2) * grads[<span class="string">"db"</span> + str(l+1)] ** 2</div><div class="line"></div><div class="line">        <span class="comment"># Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".</span></div><div class="line">        s_corrected[<span class="string">"dW"</span> + str(l+1)] = s[<span class="string">"dW"</span> + str(l+1)] / (1 - beta2 ** t)</div><div class="line">        s_corrected[<span class="string">"db"</span> + str(l+1)] = s[<span class="string">"db"</span> + str(l+1)] / (1 - beta2 ** t)</div><div class="line"></div><div class="line">        <span class="comment"># Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * v_corrected[<span class="string">"dW"</span> + str(l+1)] / np.sqrt(s_corrected[<span class="string">"dW"</span> + str(l+1)] + epsilon)</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * v_corrected[<span class="string">"db"</span> + str(l+1)] / np.sqrt(s_corrected[<span class="string">"db"</span> + str(l+1)] + epsilon)</div><div class="line"></div><div class="line">    <span class="built_in">return</span> parameters, v, s</div></pre></td></tr></table></figure>
<h1 id="reference">Reference</h1>
<p>Andrew Ng’s deep learning course</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/31/Vanishing and Exploding Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/31/Vanishing and Exploding Gradients/" itemprop="url">Vanishing and Exploding Gradients</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-31T15:08:39+10:00">
                2017-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Intuitively, extra hidden layers ought to make the network able to learn more complex classification functions, and thus do a better job classifying. Certainly, things shouldn’t get worse, since the extra layers can, in the worst case, simply do nothing. But that’s not what’s going on.</p>
<p>Here is an example to illustrate why the vanishing gradient problem occurs. Let’s explicitly write out the entire expression for the gradient: <span class="math display">\[\frac{\partial Cost}{\partial w1} = \frac{\partial Cost}{\partial a_4}f&#39;(z_4)w_4f&#39;(z_3)w_3f&#39;(z_2)w_2f&#39;(z_1)x_{input}\]</span></p>
<p>Excepting the very first term and very last term, this expression is a product of terms of the form: <span class="math display">\[f&#39;(z_j)w_j\]</span></p>
<p>Recall that the standard approach to initialize the weights in the network is to choose the weights using a Gaussian with mean 0 and small standard deviation. So the weights will usually satisfy <span class="math inline">\(|w_j| &lt; 1\)</span>.</p>
<p>If the non-linear function is the widely used relu function, then the derivative of <span class="math inline">\(f&#39;(z_j)\)</span> can only take the values of 1 or 0. So the terms <span class="math inline">\(f&#39;(z_j)w_j\)</span> will usually much less than 1. And when we take a product of many such terms, the product will tend to exponentially decrease: the more terms, the smaller the product will be. This is starting to smell like a possible explanation for the vanishing gradient problem. In particular, we might wonder whether the weights <span class="math inline">\(w_j\)</span> could grow during training. If they do, it’s possible the terms <span class="math inline">\(f&#39;(z_j)w_j\)</span> in the product will no longer satisfy <span class="math inline">\(|f&#39;(z_j)w_j| &lt; 1\)</span>. Indeed, if the terms get large enough - greater than 1 - then we will no longer have a vanishing gradient problem. Instead, the gradient will actually grow exponentially as we move backward through the layers. Instead of a vanishing gradient problem, we’ll have an exploding gradient problem.</p>
<p>It is the fact that the gradient in early layers is the product of terms from all the later layers. When there are many layers, that’s an intrinsically unstable situation. The only way all layers can learn at close to the same speed is if all those products of terms come close to balancing out. Without some mechanism or underlying reason for that balancing to occur, it’s highly unlikely to happen simply by chance. In short, the real problem here is that neural networks suffer from an unstable gradient problem. As a result, if we use standard gradient-based learning techniques, different layers in the network will tend to learn at wildly different speeds.</p>
<p>To reduce the effect of vanishing or exploding gradient problem, espeically for relu function, we can initilize the weights by: <span class="math display">\[w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}})\]</span> The intuition behind is that: If we need the value of <span class="math inline">\(z = w_1x_1 + ... + w_nx_n + b\)</span> to be near 1, we should set <span class="math inline">\(Var(w) = \frac{1}{n}\)</span>, because the larger is n, the smaller the term <span class="math inline">\(w_jx_j\)</span> should be. In practice, using <span class="math inline">\(\frac{2}{n^{[l-1]}}\)</span> instead works better.</p>
<h1 id="reference">Reference</h1>
<p>1.<a href="http://neuralnetworksanddeeplearning.com/chap5.html" target="_blank" rel="external">Neural Network and Deep Leanring Chpater 5</a><br>
2.Andrew Ng’s Deep Learning Course</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/31/Notes on Neural Network Methods in Natural Language Processing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/31/Notes on Neural Network Methods in Natural Language Processing/" itemprop="url">Notes on Neural Network Methods in Natural Language Processing Chapter1-5</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-31T11:41:28+10:00">
                2017-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Book-notes/" itemprop="url" rel="index">
                    <span itemprop="name">Book notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="chapter-1">Chapter 1</h1>
<p>1.1 The challenge for computational approaches, including machine leanring:</p>
<ul>
<li><strong>Discrete</strong>
<ul>
<li>The basic elements of weitten language are characters.</li>
<li>Characters form words that in turn denote objects, concepts,events, actions and ideas.</li>
<li>e.g. There is no simple operation that will allow us to move from the word “red” to the word “pink” without using a large lookup table or a dictonary.</li>
</ul></li>
<li><strong>Compositional</strong>
<ul>
<li>Letters form words, words form phrases and sentences.</li>
<li>In oder to interpret a text, we need to work beyond the level of letters and words, and look at long sequences of words such as sentences, or even complete documents.</li>
</ul></li>
<li><strong>Sparse</strong>
<ul>
<li>The combination of the above properties leads to <em>data sparseness</em>.</li>
<li>The way in which words can be combined to form meanings is practically infinite.</li>
</ul></li>
</ul>
<p>1.2 Deep learning approaches work by learning to not only predict but also to <em><strong>correctly represent</strong></em> the data, such that it is suitable for prediction.</p>
<h1 id="chapter-2">Chapter 2</h1>
<p>2.2 Deep learning models often learn a cascade of representations of the input that build on top of each other, in order to best model the problem at hand, and these representations are often not interpretable–we do not know which properties of the input they capture.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/29/fastText/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/29/fastText/" itemprop="url">Some understanding about how fastText works</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-29T16:31:06+10:00">
                2017-08-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-notes/" itemprop="url" rel="index">
                    <span itemprop="name">paper notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="table-of-contents">Table of contents</h1>
<ul>
<li>Introduction</li>
<li>Word Embedding
<ul>
<li>The Skip-gram Model</li>
<li>CBOW Model</li>
<li>fastText Model</li>
</ul></li>
<li>Hierarchical Softmax
<ul>
<li>Huffman Tree</li>
<li>Huffman Coding</li>
<li>Derivatives of Hierarchical Softmax</li>
</ul></li>
<li>Conclusion</li>
</ul>
<h1 id="introduction">1. Introduction</h1>
<p>FastText is a library created by the Facebook Research Team for efficient learning of <strong>word representations</strong> and <strong>sentence classification</strong>. There are two main refernece papers:</p>
<ul>
<li>P. Bojanowski<em>, E. Grave</em>, A. Joulin, T. Mikolov, <a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a></li>
<li>A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="external">Bag of Tricks for Efficient Text Classification</a></li>
</ul>
<h1 id="word-embedding">2. Word Embedding</h1>
<p>Representing words in vectors (espicially word2vec) is widely used in Natural Language Processing applications because it’s fast and easy to use. However, there are several drawbacks of word2vec.</p>
<ol style="list-style-type: decimal">
<li>You can not build sentence representations easily. People take like average vector of a sentence and use it, but actually it not works well.</li>
<li>The other thing is that it doesn’t exploit morphology. Which means words with same radicals do not share parameters. For example: <strong>disastrous</strong> is different from <strong>disaster</strong>.</li>
</ol>
<h2 id="the-skip-gram-model">2.1 The Skip-gram Model</h2>
<p>Given a word in the middle, we can sucessively predict all the words in its context.</p>
<center>
<img src="/images/fastText/skipgram.png">
</center>
<p>For instance: <em>The mighty <strong>knight</strong> Lancelot fought bravely</em>, we use <strong>knight</strong> to predict <em>the</em>, <em>mighty</em>, <em>lancelot</em>, <em>fought</em> and <em>bravely</em>.</p>
<p>Our goal is to model probability of a <strong>context word</strong> given a word:</p>
<ul>
<li>feature for word w: <span class="math inline">\(\space x_{w}\)</span></li>
<li>classifier for context word c: <span class="math inline">\(\space v_{c}\)</span></li>
</ul>
<p><span class="math display">\[p(w|c) = \frac{exp(x^T_wv_c)}{\Sigma_{k=1}^Kexp(x_w^Tv_k)}\]</span></p>
<p>And Word Vectors <span class="math inline">\(x_w\in\mathbb{R}^d\)</span>, are used for some downstream tasks.</p>
<p>To train this model, we need to minimize a <em><strong>negative log likelihood</strong></em>:</p>
<p><span class="math display">\[min_{x,v} \space-\Sigma^T_{t=1}\Sigma_{c{\in}C_t}log\frac{e^{x^T_{w_t}v_c}}{\Sigma^K_{k=1}e^{x^T_{w_t}v_k}}\]</span></p>
<p>However, the denominator of the softmax is very computationally intensive. There are two main approximations.</p>
<ul>
<li>Replace the multiclass loss by a set of binary logistic losses</li>
<li><em><strong>Negative sampling</strong></em> (you have the positive word in the context and then just sample some random ones in the dictionary)</li>
</ul>
<p><span class="math display">\[log(1+e^{-x^T_{w_t}v_c}) + \Sigma_{n{\in}\mathcal{N}_c}log(1+e^{x^T_{w_t}v_n})\]</span></p>
<ul>
<li><em><strong>Hierarchical softmax</strong></em>: represent every word as a set of codes <span class="math inline">\(y_{ck}\)</span>. These codes are obtained by computing the Huffman tree. In the end, very frequent words are going to have short codes and very unfrequent words are going to have longer codes, so it’s very fast to evaluate and just sum over the codes for every example. I’ll explain this method in detail in the next section.</li>
</ul>
<p><span class="math display">\[\Sigma_{k{\in}\mathcal{K}_c}log(1 + e^{y_{ck}x^T_{w_t}v_k})\]</span></p>
<h2 id="cbow-model">2.2 CBOW Model</h2>
<p>Given a context and sum the vectors to predict the word in the middle.</p>
<center>
<img src="/images/fastText/CBOW.png">
</center>
<p>For instance: <em>The mighty <strong>knight</strong> Lancelot fought bravely</em>, we use <em>the</em>, <em>mighty</em>, <em>lancelot</em>, <em>fought</em> and <em>bravely</em> to predict <em>knight</em>.</p>
<p>Our goal is to model the probability of a word given a context:</p>
<ul>
<li>feature for context <span class="math inline">\(\mathcal{C}: h_{\mathcal{C}}\)</span></li>
<li>classifier for word <span class="math inline">\(w: v_w\)</span></li>
</ul>
<p><span class="math display">\[p(w|\mathcal{C}) = \frac{e^{h^T_{\mathcal{C}}v_w}}{\Sigma^K_{k=1}e^{h^T_{\mathcal{C}}v_k}}\]</span></p>
<p>And <span class="math inline">\(h_{\mathcal{C}}\)</span> is just the sum of the words in the context and that’s why it is called the <em><strong>continuous bag of words</strong></em>:</p>
<p><span class="math display">\[h_{\mathcal{C}} = \Sigma_{c{\in}\mathcal{C}}x_c\]</span></p>
<h2 id="fasttext-model">2.3 fastText Model</h2>
<p>The goal is to model the probability of a <em>label</em> given a <em>paragraph</em> so you have:</p>
<ul>
<li>feature for paragraph <span class="math inline">\(\mathcal{P}: h_{\mathcal{p}}\)</span></li>
<li>classifier for label <span class="math inline">\(l: v_l\)</span></li>
</ul>
<p><span class="math display">\[p(l|\mathcal{P}) = \frac{e^{h^T_{\mathcal{P}}v_l}}{\Sigma^K_{k=1}e^{h^T_{\mathcal{P}}v_k}}\]</span></p>
<ul>
<li>Paragraph feather:</li>
</ul>
<p><span class="math display">\[h_{\mathcal{p}} = \Sigma_{w{\in}\mathcal{p}}x_w\]</span></p>
<h4 id="exploiting-sub-word-information">Exploiting sub-word information</h4>
<ul>
<li>Represent words as the sum of its <em><strong>character n-grams</strong></em>.
<ul>
<li>Add special positional characters: ^knight$</li>
<li>All ending n-grams have special meaning</li>
</ul></li>
<li>Grammatical variations have roughtly the same mixture of n-grams</li>
<li>As in skip-gram: model probability of a <em><em>context word</em></em> given a word
<ul>
<li>classifer for word <span class="math inline">\(c: v_c\)</span></li>
<li>feature for word <span class="math inline">\(w: h_w\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[p(c|w) = \frac{e^{h^T_w}v_c}{\Sigma^K_{k=1}e^{h^T_wv_k}}\]</span></p>
<ul>
<li>Feature of a word is computed using n-grams, it takes all the character n-grams and the word itself, using hashing to store them. This makes it possible to build vectors for unseen words.</li>
</ul>
<p><span class="math display">\[h_w = \Sigma_{g{\in}w}x_g\]</span></p>
<h1 id="hierarchical-softmax">3. Hierarchical Softmax</h1>
<h2 id="hoffman-tree">3.1 Hoffman Tree</h2>
<p><em><strong>Algorithm of constucting a Huffman Tree</strong></em></p>
<ol style="list-style-type: decimal">
<li>Create a leaf node for each symbol and add it to the priority queue.</li>
<li>While there is more than one node in the queue:
<ul>
<li>Remove the two nodes of highest priority (lowest probability) from the queue</li>
<li>Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes’ probabilities.</li>
<li>Add the new node to the queue.</li>
</ul></li>
<li>The remaining node is the root node and the tree is complete.</li>
</ol>
<p>The following graph is an example:</p>
<center>
<img src="/images/fastText/HuffmanTree.png">
</center>
<p>To use Hoffman tree in our language models, symbols here can represents words (for both Skip-gram model and CBOW model) or paragraph labels (fastText Model). And also, from the algorithm, we can easily know that symbols with higher frequency are closer to the root of the Huffman tree.</p>
<h2 id="hoffman-coding">3.2 Hoffman Coding</h2>
To code a Huffman tree, just simply code all left nodes as 1 and right nodes as 0. Then we can represent each leaf based on the path from root. Also, nodes closer to the root will have shorter path.
<center>
<img src="/images/fastText/HuffmanCoding.png">
</center>
<p>For exampl, the Huffman coding of node (3) is <em>0111</em>.</p>
<h2 id="derivatives-of-hierarchical-softmax">3.3 Derivatives of Hierarchical Softmax</h2>
Let us use the following graph as an example to illustrate how to calculate the derivatives of hierarchical softmax.
<center>
<img src="/images/fastText/Derivative_of_Softmax.png">
</center>
<p>First, let’s define some notations here:</p>
<ul>
<li><span class="math inline">\(x_w\)</span> denote the input feature to the hierarchical softmax;</li>
<li><span class="math inline">\(p^w\)</span>: path from root to leaf <span class="math inline">\(w\)</span>;</li>
<li><span class="math inline">\(l^w\)</span>: number of nodes in the path <span class="math inline">\(p^w\)</span>;</li>
<li><span class="math inline">\(p^w_1,p^w_2,...,p^w_{l^w}\)</span>: denote the nodes in the path <span class="math inline">\(p^w\)</span>;</li>
<li><span class="math inline">\(d^w_2,d^w_3,...,d^w_{l^w}\)</span>: the Huffman code of word w, it consists <span class="math inline">\(l^w-1\)</span> digits (0 or 1);</li>
<li><span class="math inline">\(\theta^w_1,\theta^w_2,...,\theta^w_{l^w-1}\)</span>: the weights of nodes in the path <span class="math inline">\(p^w\)</span> except for the leaf;</li>
<li>And just follow Google’s usage, 1 stands for negative samples and 0 stands for positive samples.</li>
</ul>
<p>Now, take the node “Sport” as an example. It takes four binary classification to travel from root to node “Sport”:</p>
<ul>
<li>First: <span class="math inline">\(\space p(d^w_2|x_w,\theta^w_1) = \sigma(x^T_w\theta^w_1)\)</span></li>
<li>Second: <span class="math inline">\(\space p(d^w_3|x_w,\theta^w_2) = 1 - \sigma(x^T_w\theta^w_2)\)</span></li>
<li>Third: <span class="math inline">\(\space p(d^w_4|x_w,\theta^w_3) = 1 - \sigma(x^T_w\theta^w_3)\)</span></li>
<li>Fourth: <span class="math inline">\(\space p(d^w_5|x_w,\theta^w_4) = 1 - \sigma(x^T_w\theta^w_4)\)</span></li>
</ul>
<p>And we have: <span class="math display">\[p(Sport|input) = \prod^5_{j=2}p(d^w_j|x_w,\theta^w_1)\]</span></p>
In general, we have: <span class="math display">\[p(w|input) = \prod^{l^w}_{j=2}p(d^w_j|x_w,\theta^w_{j-1})\]</span> <span class="math display">\[p(d^w_j|x_w,\theta^w_{j-1}) = [\sigma(x^T_w\theta^w_{j-1})]^{1-d^w_j}[1 - \sigma(x^T_w\theta^w_{j-1})]^{d^w_j}\]</span> The log-likelihood function is:
<span class="math display">\[\begin{equation}\begin{split}
\mathcal{L} &amp;= log \prod^{l^w}_{j=2}{[\sigma(x^T_w\theta^w_{j-1})]^{1-d^w_j}[1 - \sigma(x^T_w\theta^w_{j-1})]^{d^w_j}}\\\\
&amp;= \Sigma^{l^w}_{j=2}{(1-d^w_j)log[\sigma(x^T_w\theta^w_{j-1})]+d^w_jlog[1-\sigma(x^T_w\theta^w_{j-1})]}
\end{split}\end{equation}\]</span>
The calculation of derivatives are as follows:
<span class="math display">\[\begin{equation}\begin{split}
\frac{\partial \mathcal{L}(w,j)}{\partial \theta^w_{j-1}} &amp;= \frac{\partial}{\partial\theta^w_{j-1}}\{(1-d^w_j)log[\sigma(x^T_w\theta^w_{j-1})] + d^w_jlog[1-\sigma(x^T_w\theta^w_{j-1})]\}\\\\
&amp;= (1-d^w_j)[1-\sigma(x^T_w\theta^w_{j-1})]x_w-d^w_j\sigma(x^T_w\theta^w_{j-1})x_w\\\\
&amp;=\{(1-d^w_j)[1-\sigma(x^T_w\theta^w_{j-1})]-d^w_j\sigma(x^T_w\theta^w_{j-1})\}x_w\\\\
&amp;= [1-d^w_j-\sigma(x^T_w\theta^w_{j-1})]x_w
\end{split}\end{equation}\]</span>
<p>And similarly, for <span class="math inline">\(x_w\)</span>: <span class="math display">\[\frac{\partial\mathcal{L}(w,j)}{\partial x_w} = [1-d^w_j-\sigma(x^T_w\theta^w_{j-1})]\theta^w_{j-1}\]</span></p>
<h1 id="conclusion">Conclusion</h1>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/" target="_blank" rel="external">Text Classification &amp; Word Representations using FastText (An NLP library by Facebook)</a></li>
<li><a href="http://www.cnblogs.com/peghoty/p/3857839.html" class="uri" target="_blank" rel="external">http://www.cnblogs.com/peghoty/p/3857839.html</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/27/Start Your Blog Using Hexo and Githubpage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/27/Start Your Blog Using Hexo and Githubpage/" itemprop="url">Start Your Blog Using Hexo and Githubpage</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-27T23:24:53+10:00">
                2017-08-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This post will decrible how to setup your Blog using GithubPage and Hexo briefly. # Installation ### Requirement 1. <a href="https://nodejs.org/en/" target="_blank" rel="external">Node.js</a> 2. <a href="https://git-scm.com" target="_blank" rel="external">Git</a></p>
<p>If your computer already has these, just install Hexo with npm: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install -g hexo-cli</div></pre></td></tr></table></figure></p>
<h1 id="setup">Setup</h1>
<p>Once Hexo is installed, run the following commands to initialise Hexo in the target <folder>. <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo init &lt;folder&gt;</div><div class="line">$ <span class="built_in">cd</span> &lt;folder&gt;</div><div class="line">$ npm install</div></pre></td></tr></table></figure></folder></p>
<h1 id="deployment">Deployment</h1>
<p>Before your first deployment, you will have to modify some settings in _config.yml. A valid deployment setting must have a type field. For example: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  <span class="built_in">type</span>: git</div><div class="line">  repo: http://github.com/&lt;username&gt;/&lt;username&gt;.github.io.git</div><div class="line">  branch: master</div></pre></td></tr></table></figure></p>
<p>Before your first deployment, install <a href="https://github.com/hexojs/hexo-deployer-git" target="_blank" rel="external">hexo-deployer-git</a>: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-deployer-git --save</div></pre></td></tr></table></figure></p>
<p>Then need to set your git information as follow: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git config --global user.name <span class="string">"yourname"</span></div><div class="line">git config --global user.email <span class="string">"youremail"</span></div></pre></td></tr></table></figure></p>
<h1 id="write">Write</h1>
<p>Then you can write your posts and save them in the folder _post under source.</p>
<h1 id="publish">Publish</h1>
<p>Just type in the following command to publish your posts: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</div></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  








  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>

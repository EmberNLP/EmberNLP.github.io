<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Word Embedding," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Table of contents  Introduction Word Embedding  The Skip-gram Model CBOW Model fastText Model  Hierarchical Softmax  Huffman Tree Huffman Coding Derivatives of Hierarchical Softmax  Conclusion  1. Int">
<meta name="keywords" content="Word Embedding">
<meta property="og:type" content="article">
<meta property="og:title" content="Some understanding about how fastText works">
<meta property="og:url" content="http://yoursite.com/2017/08/29/2017-08-29fastText/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:description" content="Table of contents  Introduction Word Embedding  The Skip-gram Model CBOW Model fastText Model  Hierarchical Softmax  Huffman Tree Huffman Coding Derivatives of Hierarchical Softmax  Conclusion  1. Int">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/2017-08-29fastText/skipgram.png">
<meta property="og:image" content="http://yoursite.com/images/2017-08-29fastText/CBOW.png">
<meta property="og:image" content="http://yoursite.com/images/2017-08-29fastText/HuffmanTree.png">
<meta property="og:image" content="http://yoursite.com/images/2017-08-29fastText/HuffmanCoding.png">
<meta property="og:image" content="http://yoursite.com/images/2017-08-29fastText/Derivative_of_Softmax.png">
<meta property="og:updated_time" content="2017-09-14T06:27:04.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Some understanding about how fastText works">
<meta name="twitter:description" content="Table of contents  Introduction Word Embedding  The Skip-gram Model CBOW Model fastText Model  Hierarchical Softmax  Huffman Tree Huffman Coding Derivatives of Hierarchical Softmax  Conclusion  1. Int">
<meta name="twitter:image" content="http://yoursite.com/images/2017-08-29fastText/skipgram.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/08/29/2017-08-29fastText/"/>





  <title>Some understanding about how fastText works | EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/29/2017-08-29fastText/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Some understanding about how fastText works</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-29T16:31:06+10:00">
                2017-08-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="table-of-contents">Table of contents</h1>
<ul>
<li>Introduction</li>
<li>Word Embedding
<ul>
<li>The Skip-gram Model</li>
<li>CBOW Model</li>
<li>fastText Model</li>
</ul></li>
<li>Hierarchical Softmax
<ul>
<li>Huffman Tree</li>
<li>Huffman Coding</li>
<li>Derivatives of Hierarchical Softmax</li>
</ul></li>
<li>Conclusion</li>
</ul>
<h1 id="introduction">1. Introduction</h1>
<p>FastText is a library created by the Facebook Research Team for efficient learning of <strong>word representations</strong> and <strong>sentence classification</strong>. There are two main refernece papers:</p>
<ul>
<li>P. Bojanowski<em>, E. Grave</em>, A. Joulin, T. Mikolov, <a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a></li>
<li>A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="external">Bag of Tricks for Efficient Text Classification</a></li>
</ul>
<h1 id="word-embedding">2. Word Embedding</h1>
<p>Representing words in vectors (espicially word2vec) is widely used in Natural Language Processing applications because it’s fast and easy to use. However, there are several drawbacks of word2vec.</p>
<ol style="list-style-type: decimal">
<li>You can not build sentence representations easily. People take like average vector of a sentence and use it, but actually it not works well.</li>
<li>The other thing is that it doesn’t exploit morphology. Which means words with same radicals do not share parameters. For example: <strong>disastrous</strong> is different from <strong>disaster</strong>.</li>
</ol>
<h2 id="the-skip-gram-model">2.1 The Skip-gram Model</h2>
<p>Given a word in the middle, we can sucessively predict all the words in its context.</p>
<center>
<img src="/images/2017-08-29fastText/skipgram.png">
</center>
<p>For instance: <em>The mighty <strong>knight</strong> Lancelot fought bravely</em>, we use <strong>knight</strong> to predict <em>the</em>, <em>mighty</em>, <em>lancelot</em>, <em>fought</em> and <em>bravely</em>.</p>
<p>Our goal is to model probability of a <strong>context word</strong> given a word:</p>
<ul>
<li>feature for word w: <span class="math inline">\(\space x_{w}\)</span></li>
<li>classifier for context word c: <span class="math inline">\(\space v_{c}\)</span></li>
</ul>
<p><span class="math display">\[p(w|c) = \frac{exp(x^T_wv_c)}{\Sigma_{k=1}^Kexp(x_w^Tv_k)}\]</span></p>
<p>And Word Vectors <span class="math inline">\(x_w\in\mathbb{R}^d\)</span>, are used for some downstream tasks.</p>
<p>To train this model, we need to minimize a <em><strong>negative log likelihood</strong></em>:</p>
<p><span class="math display">\[min_{x,v} \space-\Sigma^T_{t=1}\Sigma_{c{\in}C_t}log\frac{e^{x^T_{w_t}v_c}}{\Sigma^K_{k=1}e^{x^T_{w_t}v_k}}\]</span></p>
<p>However, the denominator of the softmax is very computationally intensive. There are two main approximations.</p>
<ul>
<li>Replace the multiclass loss by a set of binary logistic losses</li>
<li><em><strong>Negative sampling</strong></em> (you have the positive word in the context and then just sample some random ones in the dictionary)</li>
</ul>
<p><span class="math display">\[log(1+e^{-x^T_{w_t}v_c}) + \Sigma_{n{\in}\mathcal{N}_c}log(1+e^{x^T_{w_t}v_n})\]</span></p>
<ul>
<li><em><strong>Hierarchical softmax</strong></em>: represent every word as a set of codes <span class="math inline">\(y_{ck}\)</span>. These codes are obtained by computing the Huffman tree. In the end, very frequent words are going to have short codes and very unfrequent words are going to have longer codes, so it’s very fast to evaluate and just sum over the codes for every example. I’ll explain this method in detail in the next section.</li>
</ul>
<p><span class="math display">\[\Sigma_{k{\in}\mathcal{K}_c}log(1 + e^{y_{ck}x^T_{w_t}v_k})\]</span></p>
<h2 id="cbow-model">2.2 CBOW Model</h2>
<p>Given a context and sum the vectors to predict the word in the middle.</p>
<center>
<img src="/images/2017-08-29fastText/CBOW.png">
</center>
<p>For instance: <em>The mighty <strong>knight</strong> Lancelot fought bravely</em>, we use <em>the</em>, <em>mighty</em>, <em>lancelot</em>, <em>fought</em> and <em>bravely</em> to predict <em>knight</em>.</p>
<p>Our goal is to model the probability of a word given a context:</p>
<ul>
<li>feature for context <span class="math inline">\(\mathcal{C}: h_{\mathcal{C}}\)</span></li>
<li>classifier for word <span class="math inline">\(w: v_w\)</span></li>
</ul>
<p><span class="math display">\[p(w|\mathcal{C}) = \frac{e^{h^T_{\mathcal{C}}v_w}}{\Sigma^K_{k=1}e^{h^T_{\mathcal{C}}v_k}}\]</span></p>
<p>And <span class="math inline">\(h_{\mathcal{C}}\)</span> is just the sum of the words in the context and that’s why it is called the <em><strong>continuous bag of words</strong></em>:</p>
<p><span class="math display">\[h_{\mathcal{C}} = \Sigma_{c{\in}\mathcal{C}}x_c\]</span></p>
<h2 id="fasttext-model">2.3 fastText Model</h2>
<p>The goal is to model the probability of a <em>label</em> given a <em>paragraph</em> so you have:</p>
<ul>
<li>feature for paragraph <span class="math inline">\(\mathcal{P}: h_{\mathcal{p}}\)</span></li>
<li>classifier for label <span class="math inline">\(l: v_l\)</span></li>
</ul>
<p><span class="math display">\[p(l|\mathcal{P}) = \frac{e^{h^T_{\mathcal{P}}v_l}}{\Sigma^K_{k=1}e^{h^T_{\mathcal{P}}v_k}}\]</span></p>
<ul>
<li>Paragraph feather:</li>
</ul>
<p><span class="math display">\[h_{\mathcal{p}} = \Sigma_{w{\in}\mathcal{p}}x_w\]</span></p>
<h4 id="exploiting-sub-word-information">Exploiting sub-word information</h4>
<ul>
<li>Represent words as the sum of its <em><strong>character n-grams</strong></em>.
<ul>
<li>Add special positional characters: ^knight$</li>
<li>All ending n-grams have special meaning</li>
</ul></li>
<li>Grammatical variations have roughtly the same mixture of n-grams</li>
<li>As in skip-gram: model probability of a <em><em>context word</em></em> given a word
<ul>
<li>classifer for word <span class="math inline">\(c: v_c\)</span></li>
<li>feature for word <span class="math inline">\(w: h_w\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[p(c|w) = \frac{e^{h^T_w}v_c}{\Sigma^K_{k=1}e^{h^T_wv_k}}\]</span></p>
<ul>
<li>Feature of a word is computed using n-grams, it takes all the character n-grams and the word itself, using hashing to store them. This makes it possible to build vectors for unseen words.</li>
</ul>
<p><span class="math display">\[h_w = \Sigma_{g{\in}w}x_g\]</span></p>
<h1 id="hierarchical-softmax">3. Hierarchical Softmax</h1>
<h2 id="hoffman-tree">3.1 Hoffman Tree</h2>
<p><em><strong>Algorithm of constucting a Huffman Tree</strong></em></p>
<ol style="list-style-type: decimal">
<li>Create a leaf node for each symbol and add it to the priority queue.</li>
<li>While there is more than one node in the queue:
<ul>
<li>Remove the two nodes of highest priority (lowest probability) from the queue</li>
<li>Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes’ probabilities.</li>
<li>Add the new node to the queue.</li>
</ul></li>
<li>The remaining node is the root node and the tree is complete.</li>
</ol>
<p>The following graph is an example:</p>
<center>
<img src="/images/2017-08-29fastText/HuffmanTree.png">
</center>
<p>To use Hoffman tree in our language models, symbols here can represents words (for both Skip-gram model and CBOW model) or paragraph labels (fastText Model). And also, from the algorithm, we can easily know that symbols with higher frequency are closer to the root of the Huffman tree.</p>
<h2 id="hoffman-coding">3.2 Hoffman Coding</h2>
To code a Huffman tree, just simply code all left nodes as 1 and right nodes as 0. Then we can represent each leaf based on the path from root. Also, nodes closer to the root will have shorter path.
<center>
<img src="/images/2017-08-29fastText/HuffmanCoding.png">
</center>
<p>For exampl, the Huffman coding of node (3) is <em>0111</em>.</p>
<h2 id="derivatives-of-hierarchical-softmax">3.3 Derivatives of Hierarchical Softmax</h2>
Let us use the following graph as an example to illustrate how to calculate the derivatives of hierarchical softmax.
<center>
<img src="/images/2017-08-29fastText/Derivative_of_Softmax.png">
</center>
<p>First, let’s define some notations here:</p>
<ul>
<li><span class="math inline">\(x_w\)</span> denote the input feature to the hierarchical softmax;</li>
<li><span class="math inline">\(p^w\)</span>: path from root to leaf <span class="math inline">\(w\)</span>;</li>
<li><span class="math inline">\(l^w\)</span>: number of nodes in the path <span class="math inline">\(p^w\)</span>;</li>
<li><span class="math inline">\(p^w_1,p^w_2,...,p^w_{l^w}\)</span>: denote the nodes in the path <span class="math inline">\(p^w\)</span>;</li>
<li><span class="math inline">\(d^w_2,d^w_3,...,d^w_{l^w}\)</span>: the Huffman code of word w, it consists <span class="math inline">\(l^w-1\)</span> digits (0 or 1);</li>
<li><span class="math inline">\(\theta^w_1,\theta^w_2,...,\theta^w_{l^w-1}\)</span>: the weights of nodes in the path <span class="math inline">\(p^w\)</span> except for the leaf;</li>
<li>And just follow Google’s usage, 1 stands for negative samples and 0 stands for positive samples.</li>
</ul>
<p>Now, take the node “Sport” as an example. It takes four binary classification to travel from root to node “Sport”:</p>
<ul>
<li>First: <span class="math inline">\(\space p(d^w_2|x_w,\theta^w_1) = \sigma(x^T_w\theta^w_1)\)</span></li>
<li>Second: <span class="math inline">\(\space p(d^w_3|x_w,\theta^w_2) = 1 - \sigma(x^T_w\theta^w_2)\)</span></li>
<li>Third: <span class="math inline">\(\space p(d^w_4|x_w,\theta^w_3) = 1 - \sigma(x^T_w\theta^w_3)\)</span></li>
<li>Fourth: <span class="math inline">\(\space p(d^w_5|x_w,\theta^w_4) = 1 - \sigma(x^T_w\theta^w_4)\)</span></li>
</ul>
<p>And we have: <span class="math display">\[p(Sport|input) = \prod^5_{j=2}p(d^w_j|x_w,\theta^w_1)\]</span></p>
In general, we have: <span class="math display">\[p(w|input) = \prod^{l^w}_{j=2}p(d^w_j|x_w,\theta^w_{j-1})\]</span> <span class="math display">\[p(d^w_j|x_w,\theta^w_{j-1}) = [\sigma(x^T_w\theta^w_{j-1})]^{1-d^w_j}[1 - \sigma(x^T_w\theta^w_{j-1})]^{d^w_j}\]</span> The log-likelihood function is:
<span class="math display">\[\begin{equation}\begin{split}
\mathcal{L} &amp;= log \prod^{l^w}_{j=2}{[\sigma(x^T_w\theta^w_{j-1})]^{1-d^w_j}[1 - \sigma(x^T_w\theta^w_{j-1})]^{d^w_j}}\\\\
&amp;= \Sigma^{l^w}_{j=2}{(1-d^w_j)log[\sigma(x^T_w\theta^w_{j-1})]+d^w_jlog[1-\sigma(x^T_w\theta^w_{j-1})]}
\end{split}\end{equation}\]</span>
The calculation of derivatives are as follows:
<span class="math display">\[\begin{equation}\begin{split}
\frac{\partial \mathcal{L}(w,j)}{\partial \theta^w_{j-1}} &amp;= \frac{\partial}{\partial\theta^w_{j-1}}\{(1-d^w_j)log[\sigma(x^T_w\theta^w_{j-1})] + d^w_jlog[1-\sigma(x^T_w\theta^w_{j-1})]\}\\\\
&amp;= (1-d^w_j)[1-\sigma(x^T_w\theta^w_{j-1})]x_w-d^w_j\sigma(x^T_w\theta^w_{j-1})x_w\\\\
&amp;=\{(1-d^w_j)[1-\sigma(x^T_w\theta^w_{j-1})]-d^w_j\sigma(x^T_w\theta^w_{j-1})\}x_w\\\\
&amp;= [1-d^w_j-\sigma(x^T_w\theta^w_{j-1})]x_w
\end{split}\end{equation}\]</span>
<p>And similarly, for <span class="math inline">\(x_w\)</span>: <span class="math display">\[\frac{\partial\mathcal{L}(w,j)}{\partial x_w} = [1-d^w_j-\sigma(x^T_w\theta^w_{j-1})]\theta^w_{j-1}\]</span></p>
<h1 id="conclusion">Conclusion</h1>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/" target="_blank" rel="external">Text Classification &amp; Word Representations using FastText (An NLP library by Facebook)</a></li>
<li><a href="http://www.cnblogs.com/peghoty/p/3857839.html" class="uri" target="_blank" rel="external">http://www.cnblogs.com/peghoty/p/3857839.html</a></li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Word-Embedding/" rel="tag"># Word Embedding</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/27/2017-08-26Start Your Blog Using Hexo and Githubpage/" rel="next" title="Start Your Blog Using Hexo and Githubpage">
                <i class="fa fa-chevron-left"></i> Start Your Blog Using Hexo and Githubpage
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/31/2017-08-31Vanishing and Exploding Gradients/" rel="prev" title="Vanishing and Exploding Gradients">
                Vanishing and Exploding Gradients <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#table-of-contents"><span class="nav-number">1.</span> <span class="nav-text">Table of contents</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#word-embedding"><span class="nav-number">3.</span> <span class="nav-text">2. Word Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-skip-gram-model"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 The Skip-gram Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cbow-model"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 CBOW Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fasttext-model"><span class="nav-number">3.3.</span> <span class="nav-text">2.3 fastText Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#exploiting-sub-word-information"><span class="nav-number">3.3.0.1.</span> <span class="nav-text">Exploiting sub-word information</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#hierarchical-softmax"><span class="nav-number">4.</span> <span class="nav-text">3. Hierarchical Softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#hoffman-tree"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Hoffman Tree</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hoffman-coding"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Hoffman Coding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#derivatives-of-hierarchical-softmax"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 Derivatives of Hierarchical Softmax</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#conclusion"><span class="nav-number">5.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  








  








  





  

  

  

  
  


  

  

</body>
</html>

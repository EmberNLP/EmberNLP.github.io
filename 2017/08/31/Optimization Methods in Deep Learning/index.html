<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DL Basics,Optimization," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Why Gradient Descend Works? Gradient Descend is the fundamental optimization method to learn parameters. To illustrate why it works, let’s take the simple example $f(x) = (x - 1)^2 $ to gain some intu">
<meta name="keywords" content="DL Basics,Optimization">
<meta property="og:type" content="article">
<meta property="og:title" content="Optimization Methods in Deep Learning">
<meta property="og:url" content="http://yoursite.com/2017/08/31/Optimization Methods in Deep Learning/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:description" content="Why Gradient Descend Works? Gradient Descend is the fundamental optimization method to learn parameters. To illustrate why it works, let’s take the simple example $f(x) = (x - 1)^2 $ to gain some intu">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/Optimization/GD_intuition.png">
<meta property="og:image" content="http://yoursite.com/images/Optimization/SGD_GD.png">
<meta property="og:image" content="http://yoursite.com/images/Optimization/SGD_BGD.png">
<meta property="og:image" content="http://yoursite.com/images/Optimization/Momentum.png">
<meta property="og:updated_time" content="2017-09-01T01:13:10.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Optimization Methods in Deep Learning">
<meta name="twitter:description" content="Why Gradient Descend Works? Gradient Descend is the fundamental optimization method to learn parameters. To illustrate why it works, let’s take the simple example $f(x) = (x - 1)^2 $ to gain some intu">
<meta name="twitter:image" content="http://yoursite.com/images/Optimization/GD_intuition.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/08/31/Optimization Methods in Deep Learning/"/>





  <title>Optimization Methods in Deep Learning | EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/31/Optimization Methods in Deep Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Optimization Methods in Deep Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-31T15:56:48+10:00">
                2017-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="why-gradient-descend-works">Why Gradient Descend Works?</h1>
Gradient Descend is the fundamental optimization method to learn parameters. To illustrate why it works, let’s take the simple example $f(x) = (x - 1)^2 $ to gain some intuition.
<center>
<img src="/images/Optimization/GD_intuition.png">
</center>
<ul>
<li>For point <span class="math inline">\(x=3\)</span>, <span class="math inline">\(f&#39;(x)=4&gt;0\)</span></li>
<li>For point <span class="math inline">\(x=-1\)</span>, <span class="math inline">\(f&#39;(x)=-4&lt;0\)</span></li>
</ul>
<p>If we want to decreace the value of <span class="math inline">\(f(x)\)</span>:<br>
- For point <span class="math inline">\(x=3\)</span>, we need to move a small step towards the opposite direction of <span class="math inline">\(f&#39;(x=3)\)</span><br>
- For point <span class="math inline">\(x=-1\)</span>, we need to move a samll step towards the opposite direction of <span class="math inline">\(f&#39;(x=-1)\)</span></p>
<p>So, if we want to descreace the value of a given function, we just move a small step towards the opposite direction of the function’s gradient.</p>
<h1 id="the-difference-among-gd-sgd-batch-sgd">The difference among GD, SGD, Batch-SGD</h1>
For <em><strong>Gradient Descend</strong></em>, take gradient steps with respect to all training examples on each step:
<center>
<img src="/images/Optimization/SGD_GD.png">
</center>
For <em><strong>Stochastic Gradient Descend</strong></em>, take gradient steps with respect to just one random training example on each step:
<center>
<img src="/images/Optimization/SGD_BGD.png">
</center>
<p>For <em><strong>Batch Gradient Descend</strong></em>, take gradient steps with respect to m training examples on each step.</p>
<h1 id="momentum">Momentum</h1>
<p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will <em><strong>oscillate</strong></em> toward convergence. Using momentum can reduce these oscillations.</p>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the <em><strong>direction</strong></em> of the previous gradients in the variable <span class="math inline">\(v\)</span>. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of <span class="math inline">\(v\)</span> as the <em><strong>velocity</strong></em> of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.</p>
<center>
<img src="/images/Optimization/Momentum.png">
</center>
<p>The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence <span class="math inline">\(v\)</span> and then take a step in the direction of <span class="math inline">\(v\)</span>.</p>
<p>First, let’s gain some intuition from <em><strong>Exponentially Weighted Average</strong></em>.<br>
Let <span class="math inline">\(v\)</span> denote the variable for later use, and <span class="math inline">\(\theta\)</span> is the current observation <span class="math display">\[v_0 = 0\]</span> <span class="math display">\[v_1 = \beta v_0 + (1-\beta)\theta_1\]</span> <span class="math display">\[v_2 = \beta v_1 + (1-\beta)\theta_2\]</span> <span class="math display">\[.\]</span> <span class="math display">\[.\]</span> <span class="math display">\[.\]</span></p>
<p><span class="math display">\[v_t = \beta v_{t-1} + (1-\beta)\theta_t\]</span></p>
The intuition behind the above equation is that:
<span class="math display">\[\begin{equation}\begin{split}
v_t &amp;= \beta v_{t-1} + (1-\beta)\theta_t\\\\
&amp;= (1 - \beta)\theta_t + \beta(\beta v_{t-2} + (1-\beta)\theta_{t-1})\\\\
&amp;=(1-\beta)\theta_t + \beta(1-\beta)\theta_{t-1} + \beta^2v_{t-2}\\\\
&amp;=(1-\beta)\theta_t + \beta(1-\beta)\theta_{t-1} + \beta^2(1-\beta)\theta_{t-2} + ...
\end{split}\end{equation}\]</span>
<p>It’s just the exponentially weighted average of all the observations.</p>
<p>Inspired by the above equations, we define the gradient descent with momentum as follows: <span class="math display">\[v_{d_w} = \beta v_{d_w} + (1 - \beta)d_w\]</span> <span class="math display">\[v_{d_b} = \beta v_{d_b} + (1 - \beta)d_b\]</span> <span class="math display">\[w := w - \alpha v_{d_w}\]</span> <span class="math display">\[b := w - \alpha v_{d_b}\]</span></p>
<p>How do you choose <span class="math inline">\(\beta\)</span>?<br>
- The larger the momentum <span class="math inline">\(\beta\)</span> is, the smoother the update because the more we take the past gradients into account. But if <span class="math inline">\(\beta\)</span> is too big, it could also smooth out the updates too much.<br>
- Common values for <span class="math inline">\(\beta\)</span> range from 0.8 to 0.999. If you don’t feel inclined to tune this, β=0.9 is often a reasonable default.<br>
- Tuning the optimal <span class="math inline">\(\beta\)</span> for your model might need trying several values to see what works best in term of reducing the value of the cost function.</p>
<p>Example code: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using Momentum</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></div><div class="line"><span class="string">                    v['dW' + str(l)] = ...</span></div><div class="line"><span class="string">                    v['db' + str(l)] = ...</span></div><div class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line"></div><div class="line">    L = len(parameters) // 2 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    </div><div class="line">    <span class="comment"># Momentum update for each parameter</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        </div><div class="line">        <span class="comment"># compute velocities</span></div><div class="line">        v[<span class="string">"dW"</span> + str(l+1)] = beta * v[<span class="string">"dW"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        v[<span class="string">"db"</span> + str(l+1)] = beta * v[<span class="string">"db"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"db"</span> + str(l+1)]</div><div class="line">        <span class="comment"># update parameters</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * v[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * v[<span class="string">"db"</span> + str(l+1)]</div><div class="line">        </div><div class="line">    <span class="built_in">return</span> parameters, v</div></pre></td></tr></table></figure></p>
<h1 id="rmsprop">RMSprop</h1>
<p>RMSprop, which stands for root mean square prop, is also inspired by the <em>Exponentially Weighted Average</em>. The algorithm is:<br>
<span class="math display">\[s_{d_w} = \beta s_{d_w} + (1 - \beta)d_w^2\]</span> <span class="math display">\[s_{d_b} = \beta s_{d_b} + (1 - \beta)d_b^2\]</span> <span class="math display">\[w := w - \alpha \frac{d_w}{\sqrt{s_{d_w}}}\]</span> <span class="math display">\[b := b - \alpha \frac{d_b}{\sqrt{s_{d_b}}}\]</span></p>
<p>The intuition behind is also to damp out the huge oscillations. Let’s say <span class="math inline">\(d_w\)</span> is larger than <span class="math inline">\(d_b\)</span>, then <span class="math inline">\(s_{d_w}\)</span> is also larger than <span class="math inline">\(s_{d_b}\)</span>. When we update parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>, <span class="math inline">\(d_w\)</span> is divided by a relatively larger number than <span class="math inline">\(d_b\)</span> whcih helps to damp out oscillations.</p>
<p>The common choice of <span class="math inline">\(\beta\)</span> is 0.999.</p>
<p>Example code: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_RMSprop(parameters, grads, s, beta, learning_rate):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using RMSprop</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    s -- python dictionary containing the current squared gradient:</span></div><div class="line"><span class="string">                    s['dW' + str(l)] = ...</span></div><div class="line"><span class="string">                    s['db' + str(l)] = ...</span></div><div class="line"><span class="string">    beta -- the RMSprop hyperparameter, scalar</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    s -- python dictionary containing your updated squared gradient</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line"></div><div class="line">    L = len(parameters) // 2 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    </div><div class="line">    <span class="comment"># RMSprop update for each parameter</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        </div><div class="line">        <span class="comment"># compute current squared gradient</span></div><div class="line">        s[<span class="string">"dW"</span> + str(l+1)] = beta * s[<span class="string">"dW"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"dW"</span> + str(l+1)] ** 2</div><div class="line">        s[<span class="string">"db"</span> + str(l+1)] = beta * s[<span class="string">"db"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"db"</span> + str(l+1)] ** 2</div><div class="line">        <span class="comment"># update parameters</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * s[<span class="string">"dW"</span> + str(l+1)] / np.sqrt(s[<span class="string">"dW"</span> + str(l+1)])</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * s[<span class="string">"db"</span> + str(l+1)] / np.sqrt(s[<span class="string">"db"</span> + str(l+1)])</div><div class="line">        </div><div class="line">    <span class="built_in">return</span> parameters, s</div></pre></td></tr></table></figure></p>
<h1 id="adam">Adam</h1>
<p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. The update rule is, for <span class="math inline">\(l = 1,...L\)</span>: <span class="math display">\[v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial Cost}{\partial W^{[l]}}\]</span> <span class="math display">\[v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t}\]</span> <span class="math display">\[s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial Cost}{\partial W^{[l]}})^2\]</span> <span class="math display">\[s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_2)^t}\]</span> <span class="math display">\[W^{[L]} = W^{[L]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}} + \epsilon}}\]</span></p>
<p>where:<br>
- t counts the number of steps taken of Adam<br>
- L is the number of layers<br>
- <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters that control the two exponentially weighted averages.<br>
- <span class="math inline">\(\alpha\)</span> is the learning rate<br>
- <span class="math inline">\(\epsilon\)</span> is a very small number to avoid dividing by zero.</p>
<p>The common choice of hyperparameters s <span class="math inline">\(\beta_1 = 0.9. \beta_2 = 0.999, \epsilon = 1e-8\)</span></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,</div><div class="line">                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using Adam</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></div><div class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></div><div class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></div><div class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></div><div class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></div><div class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line">    </div><div class="line">    L = len(parameters) // 2                 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    v_corrected = &#123;&#125;                         <span class="comment"># Initializing first moment estimate, python dictionary</span></div><div class="line">    s_corrected = &#123;&#125;                         <span class="comment"># Initializing second moment estimate, python dictionary</span></div><div class="line">    </div><div class="line">    <span class="comment"># Perform Adam update on all parameters</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        <span class="comment"># Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".</span></div><div class="line">        v[<span class="string">"dW"</span> + str(l+1)] = beta1 * v[<span class="string">"dW"</span> + str(l+1)] + (1 - beta1) * grads[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        v[<span class="string">"db"</span> + str(l+1)] = beta1 * v[<span class="string">"db"</span> + str(l+1)] + (1 - beta1) * grads[<span class="string">"db"</span> + str(l+1)]</div><div class="line"></div><div class="line">        <span class="comment"># Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".</span></div><div class="line">        v_corrected[<span class="string">"dW"</span> + str(l+1)] = v[<span class="string">"dW"</span> + str(l+1)] / (1 - beta1 ** t)</div><div class="line">        v_corrected[<span class="string">"db"</span> + str(l+1)] = v[<span class="string">"db"</span> + str(l+1)] / (1 - beta1 ** t)</div><div class="line"></div><div class="line">        <span class="comment"># Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".</span></div><div class="line">        s[<span class="string">"dW"</span> + str(l+1)] = beta2 * s[<span class="string">"dW"</span> + str(l+1)] + (1 - beta2) * grads[<span class="string">"dW"</span> + str(l+1)] ** 2</div><div class="line">        s[<span class="string">"db"</span> + str(l+1)] = beta2 * s[<span class="string">"db"</span> + str(l+1)] + (1 - beta2) * grads[<span class="string">"db"</span> + str(l+1)] ** 2</div><div class="line"></div><div class="line">        <span class="comment"># Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".</span></div><div class="line">        s_corrected[<span class="string">"dW"</span> + str(l+1)] = s[<span class="string">"dW"</span> + str(l+1)] / (1 - beta2 ** t)</div><div class="line">        s_corrected[<span class="string">"db"</span> + str(l+1)] = s[<span class="string">"db"</span> + str(l+1)] / (1 - beta2 ** t)</div><div class="line"></div><div class="line">        <span class="comment"># Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * v_corrected[<span class="string">"dW"</span> + str(l+1)] / np.sqrt(s_corrected[<span class="string">"dW"</span> + str(l+1)] + epsilon)</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * v_corrected[<span class="string">"db"</span> + str(l+1)] / np.sqrt(s_corrected[<span class="string">"db"</span> + str(l+1)] + epsilon)</div><div class="line"></div><div class="line">    <span class="built_in">return</span> parameters, v, s</div></pre></td></tr></table></figure>
<h1 id="reference">Reference</h1>
<p>Andrew Ng’s deep learning course</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL-Basics/" rel="tag"># DL Basics</a>
          
            <a href="/tags/Optimization/" rel="tag"># Optimization</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/31/Vanishing and Exploding Gradients/" rel="next" title="Vanishing and Exploding Gradients">
                <i class="fa fa-chevron-left"></i> Vanishing and Exploding Gradients
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/31/Batch Normalization/" rel="prev" title="Batch Normalization">
                Batch Normalization <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#why-gradient-descend-works"><span class="nav-number">1.</span> <span class="nav-text">Why Gradient Descend Works?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#the-difference-among-gd-sgd-batch-sgd"><span class="nav-number">2.</span> <span class="nav-text">The difference among GD, SGD, Batch-SGD</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#momentum"><span class="nav-number">3.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rmsprop"><span class="nav-number">4.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#adam"><span class="nav-number">5.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  








  








  





  

  

  

  
  


  

  

</body>
</html>

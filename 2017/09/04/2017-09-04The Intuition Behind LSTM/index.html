<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DL Basics,RNN," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Content  A mathematically Sufficient Condition For Vanishing Sensitivity A minimum Weight Initialization to Avoid Vanishing Gradients Written memories: the intuition behind LSTMs Different Between Tan">
<meta name="keywords" content="DL Basics,RNN">
<meta property="og:type" content="article">
<meta property="og:title" content="Intuition Behind LSTM">
<meta property="og:url" content="http://yoursite.com/2017/09/04/2017-09-04The Intuition Behind LSTM/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:description" content="Content  A mathematically Sufficient Condition For Vanishing Sensitivity A minimum Weight Initialization to Avoid Vanishing Gradients Written memories: the intuition behind LSTMs Different Between Tan">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2017-09-08T09:30:56.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Intuition Behind LSTM">
<meta name="twitter:description" content="Content  A mathematically Sufficient Condition For Vanishing Sensitivity A minimum Weight Initialization to Avoid Vanishing Gradients Written memories: the intuition behind LSTMs Different Between Tan">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/04/2017-09-04The Intuition Behind LSTM/"/>





  <title>Intuition Behind LSTM | EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/04/2017-09-04The Intuition Behind LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Intuition Behind LSTM</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-04T10:55:06+10:00">
                2017-09-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="content">Content</h1>
<ul>
<li>A mathematically Sufficient Condition For Vanishing Sensitivity</li>
<li>A minimum Weight Initialization to Avoid Vanishing Gradients</li>
<li>Written memories: the intuition behind LSTMs</li>
<li>Different Between Tanh and Sigmoid Function</li>
</ul>
<h2 id="a-mathematically-sufficient-condition-for-vanishing-sensitivity">A mathematically Sufficient Condition For Vanishing Sensitivity</h2>
<p>This is a mathematical proof of a sufficient condition for vanishing sensitivity in vanilla RNNs. The proof here also takes advantage of the mean value theorem to go one step further than Pascanu et al. and reach a slightly stronger result, effectively showing vanishing causation rather than vanishing sensitivity. Note that mathematical analyses of vanishing and exploding gradients date back to the early 1990s, in Bengio et al. (1994) and Hochreiter (1991) (original in German, relevant portions summarized in Hochreiter and Schmidhuber (1997)).</p>
To begin, from the definition of a vanilla RNN cell, we have: <span class="math display">\[s_{t+1} = \phi (z_t), where \space z_t = Ws_t + Ux_{t+1} + b\]</span> Recall the <em><strong>Mean value theorem</strong></em>: If a function <span class="math inline">\(f\)</span> is continuous on the closed interval <span class="math inline">\([a,b]\)</span>, and differentiable on the open interval <span class="math inline">\((a,b)\)</span> , then there exists a point <span class="math inline">\(c\)</span> in <span class="math inline">\((a,b)\)</span> such that: <span class="math display">\[f&#39;(c) = \frac{f(b)-f(a)}{b-a}\]</span> Applying the mean value theorem, we get that there exists <span class="math inline">\(c \in [z_t, z_t + \Delta z_t]\)</span> such that:
<span class="math display">\[\begin{equation}\begin{split}
\Delta S_{t+1} &amp;= [\phi&#39;(c)] \Delta z_t\\\\
&amp;= [\phi&#39;(c)] \Delta (Ws_t)\\\\
&amp;= [\phi&#39;(c)] W\Delta(s_t)
\end{split}\end{equation}\]</span>
<p>Now let <span class="math inline">\(\left \| A \right \|\)</span> represent the matrix 2-norm, <span class="math inline">\(\left | v \right |\)</span>the Euclidean vector norm, and define: <span class="math display">\[\gamma = sup_{c \in \{z_t, z_t + \Delta z_t\}} \left \| [\phi&#39; (c)] \right \|\]</span> Note that for the logistic sigmoid, <span class="math inline">\(\gamma \leq \frac{1}{4}\)</span>, and for tanh, <span class="math inline">\(\gamma \leq 1\)</span>.</p>
Taking the vector norm of each side, we obtain, where the first inequality comes from the definition of the 2-norm (applied twice), and second from the definition of supremum:
<span class="math display">\[\begin{equation}\begin{split}
\left | \Delta s_{t+1} \right | &amp;= \left | [\phi&#39;(c)] W \Delta s_t \right |\\\\
&amp;\leq \left \| [\phi&#39;(c)] \right \| \left \| W \right \| \left | \Delta s_t \right |\\\\
&amp;\leq \gamma \left \| W \right \| \left | \Delta s_t \right |\\\\
&amp;= \left \| \gamma W \right \| \left | \Delta s_t \right |
\end{split}\end{equation}\]</span>
<p>By expanding this formula over <span class="math inline">\(k\)</span> time steps we get: <span class="math display">\[\left | \Delta s_{t+k} \right | \leq \left \| \gamma W \right \|^k \left | \Delta s_t \right |\]</span> So that: <span class="math display">\[\frac{\left | \Delta s_{t+k} \right |}{\left | \Delta s_t \right |} = \left \| \gamma W \right \|^k\]</span></p>
<p>Therefore, if <span class="math inline">\(\left \| \gamma W \right \| &lt; 1\)</span> , we have that <span class="math inline">\(\frac{\left | \Delta s_{t+k} \right |}{\left | \Delta s_t \right |}\)</span> decreases exponentially in time.</p>
<h2 id="a-minimum-weight-initialization-to-avoid-vanishing-gradients">A minimum Weight Initialization to Avoid Vanishing Gradients</h2>
<p>It is beneficial to find a weight initialization that will not immediately suffer from this problem. Extending the above analysis to find the initialization of <span class="math inline">\(W\)</span> that gets us as close to equality as possible leads to a nice result. First, let us assume that <span class="math inline">\(\phi = tanh\)</span> and take <span class="math inline">\(\gamma = 1\)</span>. Our goal is to find an initialization of W for which:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\left \| \gamma W \right \| = 1\)</span></li>
<li>We get as close to equality as possible in above equation.</li>
</ol>
<p>From point 1, since we took <span class="math inline">\(\gamma\)</span> to be 1, we have <span class="math inline">\(\left \| W \right \| = 1\)</span>. From point 2, we get that we should try to set all singular values of <span class="math inline">\(W\)</span> to 1, not just the largest. Then, if all singular values of <span class="math inline">\(W\)</span> equal 1, that means that the norm of each column of <span class="math inline">\(W\)</span> is 1 (since each column is <span class="math inline">\(We_i\)</span> for some elementary basis vector <span class="math inline">\(e_i\)</span> and we have <span class="math inline">\(\left |We_i \right | = \left | e_i \right | = 1\)</span>). That means that for column <span class="math inline">\(j\)</span> we have: <span class="math display">\[\Sigma_i w_{ij}^2 = 1\]</span></p>
<p>There are <span class="math inline">\(n\)</span> entries in column <span class="math inline">\(j\)</span>, and we are choosing each from the same random distribution, so let us find a distribution for a random weight <span class="math inline">\(w\)</span> for which: <span class="math display">\[n\mathbb{E}(w^2) = 1\]</span></p>
<p>Now let’s suppose we want to initialize <span class="math inline">\(w\)</span> uniformly in the interval <span class="math inline">\([-R,R]\)</span>. Then the mean of <span class="math inline">\(w\)</span> is 0, so that, by definition, <span class="math inline">\(\mathbb{E}(w^2)\)</span> is its variance, <span class="math inline">\(\mathbb{V}(w)\)</span>. The variance of a uniform distribution over the interval <span class="math inline">\([a,b]\)</span> is given by <span class="math inline">\(\frac{(b-a)^2}{12}\)</span>, from which we get <span class="math inline">\(\mathbb{V}(w) = \frac{R^2}{3}\)</span>. Substituting this into our equation we get: <span class="math display">\[n\frac{R^2}{3} = 1\]</span></p>
<p>So that: <span class="math display">\[R = \frac{\sqrt{3}}{\sqrt{n}}\]</span></p>
<p>This is a nice result because it is the Xavier-Glorot initialization for a square weight matrix, yet was motivated by a different idea. The Xavier-Glorot initialization, introduced by Glorot and Bengio (2010), has proven to be an effective weight initialization prescription in practice. More generally, the Xavier-Glorot prescription applies to m-by-n weight matrices used in a layer that has an activation function whose derivative is near one at the origin (like tanh), and says that we should initialize our weights according to a uniform distribution of the interval: <span class="math display">\[[-\frac{\sqrt{6}}{\sqrt{m+n}},+\frac{\sqrt{6}}{\sqrt{m+n}}]\]</span></p>
<p>We saw above that good weight initializations are crucial, but this only impacts the start of training.</p>
<h2 id="written-memories-the-intuition-behind-lstms">Written memories: the intuition behind LSTMs</h2>
<p>Very much like the messages passed by children playing a game of broken telephone, information is morphed by RNN cells and the original message is lost. A small change in the original message may not have made any difference in the final message, or it may have resulted in something completely different.</p>
<p>How can we protect the integrity of messages? This is the fundamental principle of LSTMs: to ensure the integrity of our messages in the real world, we write them down. Writing is <em><strong>a delta to the current state</strong></em>: it is an act of creation (pen on paper) or destruction (carving in stone); the subject itself does not morph when you write on it and the error gradient on the backward-pass is constant.</p>
<p>This is precisely what was proposed by the landmark paper of Hocreiter and Schmidhuber (1997), which introduced the LSTM. They asked: “how can we achieve constant error flow through a single unit with a single connection to itself [i.e., a single piece of isolated information]?”</p>
<p>The answer, quite simply, is to avoid information morphing: changes to the state of an LSTM are explicitly written in, by an explicit addition or subtraction, so that each element of the state stays constant without outside interference: “the unit’s activation has to remain constant … this will be ensured by using the identity function”.</p>
<span class="math display">\[\begin{equation}\begin{split}
f_t &amp;= \sigma (W_f \cdot [h_{t-1}, x_t] + b_f)\\\\
i_t &amp;= \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\\\
\tilde{C}_t &amp;= tanh(W_c \cdot [h_{t-1}, x_t] +b_c)\\\\
C_t &amp;= f_t \ast C_{t-1} + i_t \ast \tilde{C}_t \\\\
o_t &amp;= \sigma (W_o \cdot [h_{t-1}, x_t] + b_o)\\\\
h_t &amp;= o_t \ast tanh(C_t)
\end{split}\end{equation}\]</span>
<p>Suppose we have calculated the value of <span class="math inline">\(\frac{\partial L}{C_{t+k}}\)</span>, then: <span class="math display">\[\frac{\partial L}{C_t} = \frac{\partial L}{C_{t+k}} \odot f_{t+k-1} \odot f_{t+k-2} ... \odot f_{t}\]</span> Except for the very first term, the vanishing problem still exist.</p>
<h2 id="different-between-tanh-and-sigmoid-function">Different Between Tanh and Sigmoid Function</h2>
<ul>
<li>Sigmoid function have domain of all real numbers, ranging from 0 to 1, campared with Tanh function whose domain ranging from -1 to 1. The problem of Sigmoid is that the activation value is always positive, which give rise to the phenomenon that the derivative of weights are always all positive or all negative.</li>
<li>When a sigmoidal activation function must be used, the hyperbolic tangent activation function typically performs better than the logistic sigmoid. It <em><strong>resembles the identity function more closely</strong></em>, in the sense that <span class="math inline">\(tanh(0) = 0\)</span> while <span class="math inline">\(\sigma(0) = \frac{1}{2}\)</span>. Because tanh is similar to the identity function near 0, training a deep neural network <span class="math inline">\(\hat{y} = W^T tanh(U^T tanh(V^TX))\)</span> resembles training a linear model <span class="math inline">\(\hat{y} = W^TU^TV^TX\)</span> so long as the activations of the network can be kept small. This makes training the tanh network easier.</li>
</ul>
<h1 id="reference">Reference</h1>
<p><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#written-memories-the-intuition-behind-lstms" target="_blank" rel="external">Written memories the intuition behind lstms</a> <a href="http://www.deeplearningbook.org/contents/mlp.html" target="_blank" rel="external">Deep Learning Chap6</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL-Basics/" rel="tag"># DL Basics</a>
          
            <a href="/tags/RNN/" rel="tag"># RNN</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/02/2017-09-02RNN for NLP/" rel="next" title="RNN for NLP">
                <i class="fa fa-chevron-left"></i> RNN for NLP
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/05/2017-09-05PixelCNN/" rel="prev" title="PixelCNN">
                PixelCNN <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#content"><span class="nav-number">1.</span> <span class="nav-text">Content</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#a-mathematically-sufficient-condition-for-vanishing-sensitivity"><span class="nav-number">1.1.</span> <span class="nav-text">A mathematically Sufficient Condition For Vanishing Sensitivity</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a-minimum-weight-initialization-to-avoid-vanishing-gradients"><span class="nav-number">1.2.</span> <span class="nav-text">A minimum Weight Initialization to Avoid Vanishing Gradients</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#written-memories-the-intuition-behind-lstms"><span class="nav-number">1.3.</span> <span class="nav-text">Written memories: the intuition behind LSTMs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#different-between-tanh-and-sigmoid-function"><span class="nav-number">1.4.</span> <span class="nav-text">Different Between Tanh and Sigmoid Function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">2.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  








  








  





  

  

  

  
  


  

  

</body>
</html>

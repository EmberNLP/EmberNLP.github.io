<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Sentence Embedding," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Contents  Introduction The Natural Language Inference Task Encoder Models  LSTM and GRU BiLSTM With Mean/Max Pooling Self-attentive Network Hierarchical ConvNet   Introduction This paper studies the t">
<meta name="keywords" content="Sentence Embedding">
<meta property="og:type" content="article">
<meta property="og:title" content="Supervised Learning of Universal Sentence Representations from Natural Language Inference Data">
<meta property="og:url" content="http://yoursite.com/2017/09/14/2017-09-14Supervised Learning of Universal Sentence Representations from Natural Language Inference Data/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:description" content="Contents  Introduction The Natural Language Inference Task Encoder Models  LSTM and GRU BiLSTM With Mean/Max Pooling Self-attentive Network Hierarchical ConvNet   Introduction This paper studies the t">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/GenericNLITrainingScheme.png">
<meta property="og:image" content="http://yoursite.com/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/BiLSTMMaxPoolingNetwork.png">
<meta property="og:image" content="http://yoursite.com/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/InnerAttentionNetworkArchitecture.png">
<meta property="og:image" content="http://yoursite.com/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/HierarchicalConvNetArchitecture.png">
<meta property="og:updated_time" content="2017-09-14T07:01:25.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Supervised Learning of Universal Sentence Representations from Natural Language Inference Data">
<meta name="twitter:description" content="Contents  Introduction The Natural Language Inference Task Encoder Models  LSTM and GRU BiLSTM With Mean/Max Pooling Self-attentive Network Hierarchical ConvNet   Introduction This paper studies the t">
<meta name="twitter:image" content="http://yoursite.com/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/GenericNLITrainingScheme.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/14/2017-09-14Supervised Learning of Universal Sentence Representations from Natural Language Inference Data/"/>





  <title>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data | EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/14/2017-09-14Supervised Learning of Universal Sentence Representations from Natural Language Inference Data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-14T15:52:30+10:00">
                2017-09-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="contents">Contents</h1>
<ul>
<li>Introduction</li>
<li>The Natural Language Inference Task</li>
<li>Encoder Models
<ul>
<li>LSTM and GRU</li>
<li>BiLSTM With Mean/Max Pooling</li>
<li>Self-attentive Network</li>
<li>Hierarchical ConvNet</li>
</ul></li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>This paper studies the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks.</p>
<p>Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks. The experiments of this paper show that an encoder based on a bi-directional LSTM architecture with max pooling, trained the Stanford Natural Language Inference dataset, yields state-of-the-art sentence embeddings compared to all existing alternative unsupervised approaches like SkipThought or FastSent, while being much faster to train.</p>
<p>The difficulty for this task is that neural networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these biases.</p>
<h1 id="the-natural-language-inference-task">The Natural Language Inference Task</h1>
<p>The SNLI dataset consists of 570k human-generated English sentence pairs, manually labeled with one of three categories: entailment, contradiction and neutral. It captures natural language inference, also known in previous incarnations as Recognizing Textual Entailment (RTE), and constitutes one of the largest high-quality labeled resources explicitly constructed in order to require understanding sentence semantics.</p>
<center>
<img src="/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/GenericNLITrainingScheme.png">
</center>
<h1 id="encoder-models">Encoder Models</h1>
<h3 id="lstm-and-gru">LSTM and GRU</h3>
<p>Encoders apply recurrent neural networks using either LSTM or GRU modules, as in sequence to sequence encoders. For a sequence of T words <span class="math inline">\((w_1,...,w_T)\)</span>, the network computes a set of <span class="math inline">\(T\)</span> hidden representations <span class="math inline">\(h_1,h_2,...,h_T\)</span>, with <span class="math inline">\(h_t = \overrightarrow{LSTM}(w_1,...,w_T)\)</span> (or GRU units instead). A sentence is represented by the last hidden vector, <span class="math inline">\(h_T\)</span>.</p>
<h3 id="bilstm-with-meanmax-pooling">BiLSTM With Mean/Max Pooling</h3>
<center>
<img src="/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/BiLSTMMaxPoolingNetwork.png">
</center>
<span class="math display">\[\begin{equation}\begin{split}
\overrightarrow{h_t} &amp;= \overrightarrow{LSTM}_t(w_1,...,w_T)\\\\
\overleftarrow{h_t} &amp;= \overleftarrow{LSTM}_t(w_1,...,w_T)\\\\
h_t &amp;= [\overrightarrow{h_t}, \overleftarrow{h_t}]
\end{split}\end{equation}\]</span>
<h3 id="self-attentive-network">Self-attentive Network</h3>
<center>
<img src="/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/InnerAttentionNetworkArchitecture.png">
</center>
<span class="math display">\[\begin{equation}\begin{split}
\overline{h}_i &amp;= tanh(Wh_i + b_w)\\\\
\alpha_i &amp;= \frac{e^{\overline{h}^T_i u_w}}{\Sigma_i e^{\overline{h}^T_i u_w}}\\\\
u &amp;= \Sigma_i \alpha_i h_i
\end{split}\end{equation}\]</span>
<p>Where {<span class="math inline">\(h_1,...,h_T\)</span>} are the output vectors of a BiLSTM. These are fed to an affine transformation (<span class="math inline">\(W, b_w\)</span>) whcih outputs a set of keys (<span class="math inline">\(\overline{h}_1,...,\overline{h}_T\)</span>). The {<span class="math inline">\(\alpha_i\)</span>} represent the score of similarity between the keys and a learned context query vector <span class="math inline">\(u_w\)</span>. These weights are used to produce the final representation <span class="math inline">\(u\)</span>, which is a weighted linear combination of the hidden vectors.</p>
<h3 id="hierarchical-convnet">Hierarchical ConvNet</h3>
<center>
<img src="/images/2017-09-14Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data/HierarchicalConvNetArchitecture.png">
</center>
<p>One of the currently best performing models on classification tasks is a convolutional architecture. It concatenates different representations of the sentences at different level of abstractions. The paper introduces a faster version consisting of 4 convolutional layers. At every layer, a representation <span class="math inline">\(u_i\)</span> is computed by a max-pooling operation over the feature maps. The final representation <span class="math inline">\(u = [u_1,u_2,u_3,u_4]\)</span> concatenates representations at different levels of the input sentence. The model thus captures hierarchical abstractions of an input sentence in a fixed-size representation.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Sentence-Embedding/" rel="tag"># Sentence Embedding</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/13/2017-09-13Show attend and tell/" rel="next" title="Show Attend and Tell">
                <i class="fa fa-chevron-left"></i> Show Attend and Tell
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/14/2017-09-14Language Modeling with Gated Convolutional Networks/" rel="prev" title="Language Modeling with Gated Convolutional Networks">
                Language Modeling with Gated Convolutional Networks <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#contents"><span class="nav-number">1.</span> <span class="nav-text">Contents</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#the-natural-language-inference-task"><span class="nav-number">3.</span> <span class="nav-text">The Natural Language Inference Task</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#encoder-models"><span class="nav-number">4.</span> <span class="nav-text">Encoder Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lstm-and-gru"><span class="nav-number">4.0.1.</span> <span class="nav-text">LSTM and GRU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bilstm-with-meanmax-pooling"><span class="nav-number">4.0.2.</span> <span class="nav-text">BiLSTM With Mean/Max Pooling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attentive-network"><span class="nav-number">4.0.3.</span> <span class="nav-text">Self-attentive Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hierarchical-convnet"><span class="nav-number">4.0.4.</span> <span class="nav-text">Hierarchical ConvNet</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  








  








  





  

  

  

  
  


  

  

</body>
</html>

<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="CNN,DL," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="1. Introduction The dominant approach of sequence to sequence learning to date encodes the input sequence with a series of bi-directional RNN and generates a variable length output with another set of">
<meta name="keywords" content="CNN,DL">
<meta property="og:type" content="article">
<meta property="og:title" content="Convolutional Sequence to Sequence Learning">
<meta property="og:url" content="http://yoursite.com/2017/09/18/2017-09-18Convolutional Sequence to Sequence Learning/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:description" content="1. Introduction The dominant approach of sequence to sequence learning to date encodes the input sequence with a series of bi-directional RNN and generates a variable length output with another set of">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/2017-09-18Convolutional%20Sequence%20to%20Sequence%20Learning/ModelArchitecture.png">
<meta property="og:image" content="http://yoursite.com/images/2017-09-18Convolutional%20Sequence%20to%20Sequence%20Learning/Attention.gif">
<meta property="og:updated_time" content="2017-09-18T09:37:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Convolutional Sequence to Sequence Learning">
<meta name="twitter:description" content="1. Introduction The dominant approach of sequence to sequence learning to date encodes the input sequence with a series of bi-directional RNN and generates a variable length output with another set of">
<meta name="twitter:image" content="http://yoursite.com/images/2017-09-18Convolutional%20Sequence%20to%20Sequence%20Learning/ModelArchitecture.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/18/2017-09-18Convolutional Sequence to Sequence Learning/"/>





  <title>Convolutional Sequence to Sequence Learning | EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/18/2017-09-18Convolutional Sequence to Sequence Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Convolutional Sequence to Sequence Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-18T16:49:36+10:00">
                2017-09-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="introduction">1. Introduction</h1>
<p>The dominant approach of sequence to sequence learning to date encodes the input sequence with a series of bi-directional RNN and generates a variable length output with another set of decoder RNNs both of which interface via a <a href="https://embernlp.github.io/2017/09/15/2017-09-15Seq2seq/" target="_blank" rel="external">soft-attention mechanism</a>.</p>
<p>Compared to recurrent layers, convolutions create representations for <em>fixed size</em> contexts, however, the effective context size of the network can easily be made larger by stacking several layers on top of each other. This allows to precisely control the maximum length of dependencies to be modeled. Multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher levels.</p>
<p>This papr proposes an architecture for sequence to sequence modeling that is entirely convolutional. The model is equipped with <a href="https://embernlp.github.io/2017/09/14/2017-09-14Language%20Modeling%20with%20Gated%20Convolutional%20Networks/" target="_blank" rel="external">gated linear unit</a> and <a href="https://embernlp.github.io/2017/09/15/2017-09-15Highway&amp;Residual&amp;Training%20RNNs%20as%20Fast%20as%20CNNs/" target="_blank" rel="external">residual connections</a>. It also uses attention in every decoders layer and demonstrate that each attention layer only adds a negligible amount of overhead. And the combination of these choices enables us to tackle large scale problems.</p>
<h1 id="model-architecture">2. Model architecture</h1>
<p>The overall model architecture is shown below:</p>
<center>
<img src="/images/2017-09-18Convolutional%20Sequence%20to%20Sequence%20Learning/ModelArchitecture.png">
</center>
<h3 id="position-embedding">2.1 Position Embedding</h3>
<p>Input elements <span class="math inline">\(\mathbf{x} = (x_1,...x_m)\)</span> embedded in distributional space as <span class="math inline">\(\mathbf{w} = (w_1,...w_m)\)</span>, where <span class="math display">\[w_j \in \mathbb{R}^f\]</span> is a column in an embedding matrix <span class="math inline">\(\mathcal{D} \in \mathbb{R}^{V \times f}\)</span>.</p>
<p>The model is also equipped with a sense of order by embedding the absolute position of input elements <span class="math inline">\(\mathbf{p} = (p_1,...p_m)\)</span>, where <span class="math display">\[p_j \in \mathbb{R}^f\]</span> Just like a one-hot coding.</p>
Both are combined to obtain input element representations: <span class="math display">\[\mathbf{e} = (w_1+p_1,...,w_m+p_m)\]</span> And matrix <span class="math inline">\(\mathbf{e}\)</span> will result as below (column per word):
<span class="math display">\[\begin{equation}
\mathbf{e} = 
\left(\begin{array}{cc} 
w_{11} + 1 &amp; w_{21} &amp; \cdots &amp; w_{m1} \\ 
w_{12} &amp; w_{22} + 1 &amp; \cdots &amp; w_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{1f} &amp; w_{2f} &amp; \cdots &amp; w_{mf} + 1 \\
\end{array}\right)
\end{equation}\]</span>
<p>The model proceeds similarly for ouput elements that were <em>already generated by the decoder network</em> to yield output element representations that are being fed back into the decoder network, <span class="math inline">\(\mathbf{g} = (g_1,...,g_n)\)</span>.</p>
<p>Position embeddings are useful in model architecture since they give the model a sense of which portion of the sequence in the input or output it is currently dealing with.</p>
<h3 id="convolutional-block-structure">2.2 Convolutional Block Structure</h3>
<p>Both encoder and decoder networks share a simple block structure that computes intermediate states based on a fixed number of input elements. Each block contains <strong>one dimensional convolution</strong> followed by <strong>a non-linearity</strong>.</p>
<p>Denote the output of the <span class="math inline">\(l^{th}\)</span> block as <span class="math inline">\(\mathbf{h} = (h^l_1,...,h^l_n)\)</span> for the decoder network and <span class="math inline">\(\mathbf{z} = (z^l_1,...,z^l_m)\)</span> for the encoder network.</p>
<h4 id="gated-linear-convolution">2.2.1 Gated Linear Convolution</h4>
<p>For a decoder network with a single block and kernel width <span class="math inline">\(k\)</span>, each resulting state <span class="math inline">\(h^1_i\)</span> contains information over <span class="math inline">\(k\)</span> input elements. Stacking several blocks on top of each other increases the number of input elements represented in a state. For instance, stacking 6 blocks with <span class="math inline">\(k=5\)</span>, results in an input field of <span class="math inline">\(25 (4 \times 5 + 5)\)</span> elements.</p>
<p>Each convolution kernel is parameterized as <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{2d \times kd}\)</span>, <span class="math inline">\(b_w \in \mathbf{R}^{2d}\)</span> and takes as input vector <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{kd}\)</span> which is a concatenation (hstack) of <span class="math inline">\(k\)</span> input elements embedded in <span class="math inline">\(d\)</span> dimensions and maps them to a single output element <span class="math inline">\(\mathbf{Y} \in \mathbb{R}^{2d}\)</span>.</p>
<h4 id="non-linearities">2.2.2 Non-linearities</h4>
<p>Non-linearities allow the networks to exploit the full input field, or to focus on fewer elements if needed. This paper chooses gated linear units (<a href="https://embernlp.github.io/2017/09/14/2017-09-14Language%20Modeling%20with%20Gated%20Convolutional%20Networks/" target="_blank" rel="external">GLU</a>):</p>
<span class="math display">\[\begin{equation}\begin{split}
Y &amp;= [A \space B] \in \mathbb{R}^{2d}\\\\
v([A \space B]) &amp;= A \otimes \sigma(B)\\\\
v([A \space B]) &amp;\in \mathbb{R}^d
\end{split}\end{equation}\]</span>
<h4 id="residual-connection">2.2.3 Residual Connection</h4>
<p>To enable deep convolutional networks, residual connections are added from the input of each convolution to the output of the block:</p>
<span class="math display">\[\begin{equation}
h^l_i= v(\mathbf{W}^l[h^{l-1}_{i-k/2},...,h^{l-1}_{i+k/2}] + b^l_w) + h^{l-1}_i
\end{equation}\]</span>
<h4 id="block-output">2.2.4 Block Output</h4>
<p>For encoder networks, ensure that the output of the convolutional layers matches the input length by padding the input at each layer. However, for decoder networks, we need to take care that no future information is available to the decoder. To achieve this, pad the input by <span class="math inline">\(k-1\)</span> elements on both the left and right side by zero vectors, and then remove <span class="math inline">\(k\)</span> elements from the end of the convolution output.</p>
<h4 id="linear-mapping">2.2.5 Linear Mapping</h4>
<p>Linear mapping is applied to <span class="math inline">\(\mathbf{w}\)</span> when feeding embeddings to the encoder network; to the encoder output <span class="math inline">\(z^u_j\)</span>; to the finnal layer of the decoder just before the softmax <span class="math inline">\(\mathbf{h}^L\)</span>; and to all decoder layers <span class="math inline">\(\mathbf{h}^l\)</span> before computing attention scores.</p>
<h4 id="prediction-layer">2.2.6 Prediction Layer</h4>
<p>To compute a distribution over the T possible next target elements <span class="math inline">\(y_{i+1}\)</span> by transforming the top decoder output <span class="math inline">\(h^L_i\)</span> via a linear layer with weights <span class="math inline">\(W_0\)</span> and bias <span class="math inline">\(b_0\)</span>.</p>
<span class="math display">\[\begin{equation}
p(y_{i+1} | y_1,...,y_i, \mathbf{x}) = softmax(\mathbf{W}_0h^L_i + b_0) \in \mathbb{R}^T
\end{equation}\]</span>
<h3 id="multi-step-attention">2.3 Multi-step Attention</h3>
<p>The paper introduces a seperate attention mechanism for each decoder layer, shown below:</p>
<center>
<img src="/images/2017-09-18Convolutional%20Sequence%20to%20Sequence%20Learning/Attention.gif">
</center>
<p>To compute the attention, we combine the current docoder state <span class="math inline">\(h^l_i\)</span> with an embedding of the previous target element <span class="math inline">\(g_i\)</span>:</p>
<span class="math display">\[\begin{equation}
d^l_i = W^l_dh^l_i + b^l_i + g_i
\end{equation}\]</span>
<p>where <span class="math inline">\(h^l_i \in \mathbb{R}^d\)</span> and <span class="math inline">\(W^l_i \in \mathbb{R}^{e \times d}\)</span>ï¼Œ so <span class="math inline">\(d^l_i \in \mathbb{R}^d\)</span>.</p>
<p>For decoder layer <span class="math inline">\(l\)</span> the attention <span class="math inline">\(a^l_{ij}\)</span> of state <span class="math inline">\(i\)</span> and source element <span class="math inline">\(j\)</span> is computed as a dot_product between the deocder state summary <span class="math inline">\(d^l_i\)</span> and each output <span class="math inline">\(z^u_j\)</span> of the last encoder block <span class="math inline">\(u\)</span>:</p>
<span class="math display">\[\begin{equation}
a^l_{ij} = \frac{exp(d^l_i \cdot z^u_j)}{\Sigma^m_{t=1}exp(d^l_i \cdot z^u_t)}
\end{equation}\]</span>
<p>The conditional input <span class="math inline">\(c^l_i\)</span> to the current decoder layer is a weighted sum of the encoder outputs as well as the input element embeddings <span class="math inline">\(e_j\)</span>:</p>
<span class="math display">\[\begin{equation}
c^l_i = \Sigma^m_{j=1}a^l_{ij}(z^u_j+e_j)
\end{equation}\]</span>
<p>where <span class="math inline">\(m\)</span> is the length of encoder output (roughly equal to length of input to the model).</p>
<p>This is slightly different to recurrent approaches which compute both the attention and the weighted sum over <span class="math inline">\(z^u_j\)</span> only. Adding <span class="math inline">\(e_j\)</span> to be beneficial and it resembles key-value memory networks where the keys are the <span class="math inline">\(z^u_j\)</span> and the values are the <span class="math inline">\(z^u_j + e_j\)</span>. Encoder outputs <span class="math inline">\(z^u_j\)</span> represent <strong>potentially large input contexts</strong> and <span class="math inline">\(e_j\)</span> provides <strong>point information</strong> about a specific input element that is useful when making a prediction. Once <span class="math inline">\(c^l_i\)</span> has been computed, it is simply added to the output of the correspoding decoder layer <span class="math inline">\(h^l_i\)</span>.</p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="external">Convolutional Sequence to Sequence Learning</a></li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/CNN/" rel="tag"># CNN</a>
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/15/2017-09-15Attention Is All You Need/" rel="next" title="Attention Is All You Need">
                <i class="fa fa-chevron-left"></i> Attention Is All You Need
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#model-architecture"><span class="nav-number">2.</span> <span class="nav-text">2. Model architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#position-embedding"><span class="nav-number">2.0.1.</span> <span class="nav-text">2.1 Position Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convolutional-block-structure"><span class="nav-number">2.0.2.</span> <span class="nav-text">2.2 Convolutional Block Structure</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gated-linear-convolution"><span class="nav-number">2.0.2.1.</span> <span class="nav-text">2.2.1 Gated Linear Convolution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#non-linearities"><span class="nav-number">2.0.2.2.</span> <span class="nav-text">2.2.2 Non-linearities</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#residual-connection"><span class="nav-number">2.0.2.3.</span> <span class="nav-text">2.2.3 Residual Connection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#block-output"><span class="nav-number">2.0.2.4.</span> <span class="nav-text">2.2.4 Block Output</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#linear-mapping"><span class="nav-number">2.0.2.5.</span> <span class="nav-text">2.2.5 Linear Mapping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#prediction-layer"><span class="nav-number">2.0.2.6.</span> <span class="nav-text">2.2.6 Prediction Layer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-step-attention"><span class="nav-number">2.0.3.</span> <span class="nav-text">2.3 Multi-step Attention</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">3.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  








  








  





  

  

  

  
  


  

  

</body>
</html>

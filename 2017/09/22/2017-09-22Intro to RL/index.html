<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DL,RL," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Contents  Introduction Policy Gradient  Step 1: Define the Actor Step 2: How to Evaluate the Actor Step 3: Step 3: How to Improve the Actor Policy Gradient in Practice  Value Based Approach—Learning a">
<meta name="keywords" content="DL,RL">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction to Reinforcement Learning">
<meta property="og:url" content="http://yoursite.com/2017/09/22/2017-09-22Intro to RL/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:description" content="Contents  Introduction Policy Gradient  Step 1: Define the Actor Step 2: How to Evaluate the Actor Step 3: Step 3: How to Improve the Actor Policy Gradient in Practice  Value Based Approach—Learning a">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/2017-09-22Intro%20to%20RL/GlobalPicture.png">
<meta property="og:image" content="http://yoursite.com/images/2017-09-22Intro%20to%20RL/PolicyGradient.png">
<meta property="og:image" content="http://yoursite.com/images/2017-09-22Intro%20to%20RL/Baseline.png">
<meta property="og:updated_time" content="2017-09-24T00:25:53.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Introduction to Reinforcement Learning">
<meta name="twitter:description" content="Contents  Introduction Policy Gradient  Step 1: Define the Actor Step 2: How to Evaluate the Actor Step 3: Step 3: How to Improve the Actor Policy Gradient in Practice  Value Based Approach—Learning a">
<meta name="twitter:image" content="http://yoursite.com/images/2017-09-22Intro%20to%20RL/GlobalPicture.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/22/2017-09-22Intro to RL/"/>





  <title>Introduction to Reinforcement Learning | EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/22/2017-09-22Intro to RL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Introduction to Reinforcement Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-22T10:46:01+10:00">
                2017-09-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="contents">Contents</h1>
<ul>
<li>Introduction</li>
<li>Policy Gradient
<ul>
<li>Step 1: Define the Actor</li>
<li>Step 2: How to Evaluate the Actor</li>
<li>Step 3: Step 3: How to Improve the Actor</li>
<li>Policy Gradient in Practice</li>
</ul></li>
<li>Value Based Approach—Learning a Critic
<ul>
<li>Monte-Carlo based approach</li>
<li>Temporal-difference approach</li>
<li>Q-learning</li>
</ul></li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>Deep Reinforcement learning is the combination of <em>deep learning</em> and <em>reinforcement learning</em>.</p>
<p>In the scenario of reinforcement leanring, we have three components: agent, environment and state. Agent interacts with environment and “observe” something called state. <strong>State</strong> is not only about observed images, it includes all the information the agent senses. After receiving the observation, the agent takes some actions and these actions will change the environment. Then, environment will give agent a reward.</p>
<center>
<img src="/images/2017-09-22Intro%20to%20RL/GlobalPicture.png">
</center>
<p>Like the picture above, the agent observes a cup of water, takes the action to knock it over and spill the water, the environment gives it a negative reward like -1. The agent keeps interacting with the environment, and adjust the parameters according to the rewards to take actions to maximize the expected reward. (Expected reward will be explained later)</p>
<p><em><strong>Reinforcement leanring is trying to find a best function <span class="math inline">\(\pi(a|s)\)</span> (called actor/policy), which can maximize the expected reward. Compared to supervised leanring approach learning from a teacher, reinforcement leanring learns from experience.</strong></em></p>
<p>RL has two properties.</p>
<ul>
<li>Reward Delay.</li>
<li>Agent’s actions affect subsequent data it receives.</li>
</ul>
<h1 id="policy-gradient">Policy Gradient</h1>
<h4 id="step-1-define-the-actor">Step 1: Define the Actor</h4>
<p>In policy gradient, neural network plays the role as actor. The input to the neural network is the observation of the agent represented as a vector or a matrix; each neuron in the output layer corresponds to one action. Neural network is good at generalization than past methods in RL like a lookup table. Generalization means the agent can still take proper actions if the state never appears.</p>
<center>
<img src="/images/2017-09-22Intro%20to%20RL/PolicyGradient.png">
</center>
<p>The picture above shows an example. Input is a screenshot of the game, the nerual network in the middle is trying to learn a good function <span class="math inline">\(\pi (a|s)\)</span> and finally outputs the probability distribution over all possible actions.</p>
<h4 id="step-2-how-to-evaluate-the-actor">Step 2: How to Evaluate the Actor</h4>
<p>But how to evaluate the actor? What <span class="math inline">\(\pi_{\theta}(a|s)\)</span> is a good function?</p>
<p>Suppose we are given an arbitraty actor <span class="math inline">\(\pi_{\theta}(a|s)\)</span>, <span class="math inline">\(\theta\)</span> represent all the parameters in the neural network. We use this actor to generate a sequence of actions:</p>
<ul>
<li>Start with state <span class="math inline">\(s_1\)</span></li>
<li>NN decides to take action <span class="math inline">\(a_1\)</span></li>
<li>NN obtains reward <span class="math inline">\(r_1\)</span></li>
<li>NN sees observation <span class="math inline">\(s_2\)</span></li>
<li>NN decides to take action <span class="math inline">\(a_2\)</span></li>
<li>NN obtains reward <span class="math inline">\(r_2\)</span></li>
<li>……</li>
<li>NN decides to take action <span class="math inline">\(a_T\)</span></li>
<li>NN obtains reward <span class="math inline">\(r_T\)</span> <strong>[End]</strong></li>
</ul>
<p>And total reward is: <span class="math inline">\(R_{\theta} = \Sigma^T_{t=1} r_t\)</span></p>
<p>We should notice that, even with the same actor, <span class="math inline">\(R_{\theta}\)</span> is different each time because of the <strong>randomness of the environment and actor</strong> (output of the actions is a distribution, action is sampled from the distribution). So what we really interested in is the <strong>expected value of <span class="math inline">\(R\)</span> defined as <span class="math inline">\(\bar{R}_{\theta}\)</span></strong>. It’s the expected reward of many sequences which are generated from actor <span class="math inline">\(\pi_{\theta}(a|s)\)</span>. Of course, <span class="math inline">\(\bar{R}_{\theta}\)</span> is the proper way to evaluate the goodness of the actor <span class="math inline">\(\pi_{\theta}(a|s)\)</span>.</p>
<p>To calculate <span class="math inline">\(\bar{R}_{\theta}\)</span>, first we define a trajectory as <span class="math inline">\(\tau = \{ s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T \}\)</span>:</p>
<span class="math display">\[\begin{equation}\begin{split}
p(\tau | \theta) &amp;= p(s_1)p(a_1|s_1,\theta)p(r_1,s_2 | s_1, a_1)p(a_2 | s_2, \theta) p(r_2,s_3 | s_2,a_2) \cdots\\\\
&amp;= p(s_1)\prod\limits^T_{t=1}p(a_t | s_t, \theta) p(r_t,s_{t+1} | s_t,a_t)
\end{split}\end{equation}\]</span>
<p>The first term and third term do not relate to actor, but the second term is controled by actor <span class="math inline">\(\pi_{\theta}\)</span>.</p>
<p>Then we can calculate <span class="math inline">\(\bar{R}_{\theta}\)</span> as:</p>
<span class="math display">\[\begin{equation}
\bar{R}_{\theta} = \Sigma_{\tau} R(\tau) p(\tau | \theta)
\end{equation}\]</span>
<p>However, it’s not possible to compute above equation in real application, so we approximate the value by MCMC method. For example, when using <span class="math inline">\(\pi_{\theta}\)</span> to play a game, we generate <span class="math inline">\(N\)</span> trajectories and obtain {<span class="math inline">\(\tau^1,\tau^2,...,\tau^N\)</span>}, and use the <span class="math inline">\(N\)</span> trajectories to approximate <span class="math inline">\(\bar{R}_{\theta} \approx \frac{1}{N} \Sigma^N_{n=1} R(\tau^n)\)</span>.</p>
<h4 id="step-3-how-to-improve-the-actor">Step 3: How to Improve the Actor</h4>
<p>We first define our problem as:</p>
<span class="math display">\[\begin{equation}
\theta^* = argmax_{\theta} \space \bar{R}_{\theta}
\end{equation}\]</span>
<p>In general neural network, we usually use gradient descent/ascent based methods to obtain the optimal value. In RL situation, if we can compute the derivative of <span class="math inline">\(\bar{R}_{\theta}\)</span> with respect to <span class="math inline">\(\theta\)</span>, then problem solved.</p>
<p>To describle it in math, we have <span class="math inline">\(\theta = \{w_1,w_2,...,b_1,b_2,...\}\)</span>, then:</p>
<span class="math display">\[\begin{equation}
\mathbf{\nabla} \bar{R}_{\theta} = 
\left(\begin{array}{cc} 
\partial \bar{R}_{\theta} / \partial w_1 \\ 
\partial \bar{R}_{\theta} / \partial w_2 \\ 
\vdots \\
\partial \bar{R}_{\theta} / \partial b_1 \\
\partial \bar{R}_{\theta} / \partial b_2 \\
\vdots \\
\end{array}\right)
\end{equation}\]</span>
<p>Start with <span class="math inline">\(\theta^0\)</span> (gradietn ascent)</p>
<ul>
<li><span class="math inline">\(\theta^1 \leftarrow \theta^0 + \eta \mathbf{\nabla} \bar{R}_{\theta^0}\)</span></li>
<li><span class="math inline">\(\theta^2 \leftarrow \theta^1 + \eta \mathbf{\nabla} \bar{R}_{\theta^1}\)</span></li>
<li><span class="math inline">\(\cdots \cdots\)</span></li>
</ul>
<p>Recall that: <span class="math inline">\(\bar{R}_{\theta} = \Sigma_{\tau} R(\tau) p(\tau | \theta)\)</span>, then <span class="math inline">\(\nabla \bar{R}_{\theta} = \Sigma_{\tau} R(\tau) \nabla p(\tau | \theta)\)</span>. Now, we can not use MCMC methods to approximate the gradient because we do not have a distribution. However, we can do a trick to get the distribution <span class="math inline">\(p(\tau | \theta)\)</span> back.</p>
<span class="math display">\[\begin{equation}\begin{split}
\nabla \bar{R}_{\theta} &amp;= \Sigma_{\tau} R(\tau) \nabla p(\tau | \theta)\\\\
&amp;= \Sigma_{\tau} R(\tau) p(\tau | \theta) \frac{\nabla p(\tau | \theta)}{p(\tau | \theta)}\\\\
&amp;= \Sigma_{\tau} R(\tau) p(\tau | \theta) \nabla log p(\tau | \theta)\\\\
&amp;\approx \frac{1}{N} \Sigma^N_{n=1} R(\tau^n) \nabla log p(\tau^n | \theta) \space (Because \space we \space \space have \space summation \space operation \space and \space a \space distribution)
\end{split}\end{equation}\]</span>
<p>Recall again: <span class="math inline">\(p(\tau | \theta) = p(s_1)\prod\limits^T_{t=1}p(a_t | s_t, \theta) p(r_t,s_{t+1} | s_t,a_t)\)</span>, then:</p>
<span class="math display">\[\begin{equation}\begin{split}
\nabla log p(\tau | \theta) &amp;= \nabla [log p(s_1) + \Sigma^T_{t=1} log p(a_t | s_t, \theta) + log p(r_t,s_{t+1} | s_t,a_t)]\\\\
&amp;= \Sigma^T_{t=1} \nabla log p(a_t | s_t, \theta) \space (Ignore \space the \space terms \space not \space related \space to \space  \theta)
\end{split}\end{equation}\]</span>
<p>And finally, we have the <strong>Policy Gradient Algorithm</strong></p>
<span class="math display">\[\begin{equation}\begin{split}
\theta^{new} &amp;\leftarrow \theta^{old} + \eta \nabla \bar{R}_{\theta^{old}}\\\\
\nabla \bar{R}_{\theta} &amp;\approx \frac{1}{N} \Sigma^N_{n=1} R(\tau^n) \nabla logp(\tau^n | \theta)\\\\
&amp;= \frac{1}{N} \Sigma^N_{n=1} R(\tau^n) \Sigma^{T_n}_{t=1} \nabla logp(a^n_t | s^n_t, \theta)\\\\
&amp;= \frac{1}{N} \Sigma^N_{n=1} \Sigma^{T_n}_{t=1} R(\tau^n) \nabla logp(a^n_t | s^n_t, \theta)
\end{split}\end{equation}\]</span>
<p>To explain the meaning of the above formula, suppose in <span class="math inline">\(\tau^n\)</span>, the actor takes <span class="math inline">\(a^n_t\)</span> when seeing <span class="math inline">\(s^n_t\)</span>:</p>
<ul>
<li><strong>Tuning <span class="math inline">\(\theta\)</span> to increase <span class="math inline">\(p(a^n_t | s^n_t)\)</span>, if <span class="math inline">\(R(\tau^n)\)</span> is positive</strong></li>
<li><strong>Tuning <span class="math inline">\(\theta\)</span> to decrease <span class="math inline">\(p(a^n_t | s^n_t)\)</span>, if <span class="math inline">\(R(\tau^n)\)</span> is negative</strong></li>
</ul>
<p>Why? If <span class="math inline">\(R(\tau^n)\)</span> is positive, <span class="math inline">\(\bar{R}_{\theta}\)</span> and <span class="math inline">\(logp(a^n_t | s^n_t, \theta)\)</span> will increase or decrease simultaneously. Our goal is to maximize <span class="math inline">\(\bar{R}_{\theta}\)</span>, so <span class="math inline">\(\nabla \bar{R}_{\theta}\)</span> is always positive. Then <span class="math inline">\(\nabla logp(a^n_t | s^n_t, \theta)\)</span> should be positive, which means <span class="math inline">\(logp(a^n_t | s^n_t, \theta)\)</span> will be increased. In another hand, if <span class="math inline">\(R(\tau^n)\)</span> is negative, then <span class="math inline">\(\nabla \bar{R}_{\theta} \times \nabla logp(a^n_t | s^n_t, \theta) &lt;= 0\)</span>, so <span class="math inline">\(logp(a^n_t | s^n_t, \theta)\)</span> will be decreased.</p>
<p><strong>It is very important to consider the cumulative reward <span class="math inline">\(R(\tau^n)\)</span> of the whole trajectory <span class="math inline">\(\tau^n\)</span> instead of immediate reward <span class="math inline">\(r^n_t\)</span>.</strong></p>
<h4 id="policy-gradient-in-practice">Policy Gradient in Practice</h4>
<p>We have described all the mathematical details and physcial meanings of policy gradient. The problem now is how to put this powerful tool into practice, how we can use it to build our model.</p>
<p>The big picutre is, we randomly initialize the neural network parameters <span class="math inline">\(\theta\)</span> and use this actor to genereate sequences. Now we can have our trajectories and corresponding rewards. Our data should look like this:</p>
<span class="math display">\[\begin{equation}\begin{split}
\tau^1: \space &amp;(s^1_1, a^1_1) \space &amp;R(\tau^1)\\\\
&amp;(s^1_2, a^1_2) \space &amp;R(\tau^1)\\\\
&amp;(s^1_3, a^1_3) \space &amp;R(\tau^1)\\\\
&amp; \space \vdots &amp; \space \vdots\\\\

\tau^2: \space &amp;(s^2_1, a^2_1) \space &amp;R(\tau^2)\\\\
&amp;(s^2_2, a^2_2) \space &amp;R(\tau^2)\\\\
&amp;(s^2_3, a^2_3) \space &amp;R(\tau^2)\\\\
&amp; \space \vdots &amp; \space \vdots
\end{split}\end{equation}\]</span>
<p>The next step is to use this dataset to update the parameters <span class="math inline">\(\theta\)</span>.</p>
<span class="math display">\[\begin{equation}\begin{split}
\theta^{new} &amp;\leftarrow \theta^{old} + \eta \nabla \bar{R}_{\theta^{old}}\\\\
\nabla \bar{R}_{\theta} &amp;\approx \frac{1}{N} \Sigma^N_{n=1} \Sigma^{T_n}_{t=1} R(\tau^n) \nabla logp(a^n_t | s^n_t, \theta)
\end{split}\end{equation}\]</span>
<p>I have a new actor now! Use this new actor we can collect a new dataset. Repeat the two steps we can finally obtain our optimal actor!</p>
<p>Here, i give a more intuitive example.</p>
<p>Suppose the action space is: {left, right, fire}. If <span class="math inline">\(a^1_1 = left\)</span>, when we put <span class="math inline">\(s^1_1\)</span> into our model, we hope the output will be <span class="math inline">\([1,0,0]\)</span>. And if <span class="math inline">\(a^2_1 = fire\)</span>, when we put <span class="math inline">\(s^2_1\)</span> into our model, we hope the output will be <span class="math inline">\([0,0,1]\)</span>. Translate it into mathematical form, we have:</p>
<span class="math display">\[\begin{equation}\begin{split}
s^1_1 \rightarrow &amp;NN \rightarrow [1,0,0] \space (left)\\\\
&amp;\space \vdots\\\\
s^2_1 \rightarrow &amp;NN \rightarrow [0,0,1] \space (fire)\\\\
&amp;\space \vdots\\\\
\end{split}\end{equation}\]</span>
<p>Look, we transform the RL into the normal classification problem. But wait, we omit the reward <span class="math inline">\(R\)</span>. If <span class="math inline">\(R(\tau^1) = 3\)</span> and <span class="math inline">\(R(\tau^1) = 2\)</span>, we need to copy <span class="math inline">\((s^1_1, a^1_1)\)</span> twice and <span class="math inline">\((s^2_1, a^2_1)\)</span> once.</p>
<span class="math display">\[\begin{equation}\begin{split}
s^1_1 \rightarrow &amp;NN \rightarrow [1,0,0] \space (left)\\\\
s^1_1 \rightarrow &amp;NN \rightarrow [1,0,0] \space (left)\\\\
s^1_1 \rightarrow &amp;NN \rightarrow [1,0,0] \space (left)\\\\
&amp;\space \vdots\\\\
s^2_1 \rightarrow &amp;NN \rightarrow [0,0,1] \space (fire)\\\\
s^2_1 \rightarrow &amp;NN \rightarrow [0,0,1] \space (fire)\\\\
&amp;\space \vdots\\\\
\end{split}\end{equation}\]</span>
<h5 id="in-another-word-each-training-data-is-weighted-by-rtaun.">In another word, each training data is weighted by <span class="math inline">\(R(\tau^n)\)</span>.</h5>
<p>This is what policy gradient doing! It do cost much more time to train the actor than normal classification problem because we have to train our actor many times!</p>
<p>The last thing about policy gradient is the <strong>baseline</strong>. In practice, it is possible that <span class="math inline">\(R(\tau^n)\)</span> is always positive.</p>
<center>
<img src="/images/2017-09-22Intro%20to%20RL/Baseline.png">
</center>
<p>The above graph is an illustration of the problem when all rewards are positive. In ideal case, for a specific state <span class="math inline">\(s\)</span>, <span class="math inline">\(\pi(a|s)\)</span>, <span class="math inline">\(\pi(b|s)\)</span> and <span class="math inline">\(\pi(c|s)\)</span> will increase simultaneously. After softmax function, we get a reasonable probability distribution of <span class="math inline">\(\pi(action | s)\)</span>. However, recall that how we collect our dataset to train our actor, for a specific state, we can not guarantee all the possible actions are sampled. Actually, the samples are very sparse. In the graph, action “a” is not sampled, so its corresponding output probability will decrease which is not reasonable. To solve this problem, we add a baseline <strong><span class="math inline">\(b\)</span></strong> to the reward to ensure that not every reward is positive.</p>
<span class="math display">\[\begin{equation}
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \Sigma^N_{n=1} \Sigma^{T_n}_{t=1} (R(\tau^n)-b) \nabla logp(a^n_t | s^n_t, \theta)
\end{equation}\]</span>
<p>The value of b depends on your application. A common choice is to use the averaged reward; or you can let the actor to decide its value (treat it as a variable).</p>
<h1 id="value-based-approachlearning-a-critic">Value Based Approach—Learning a Critic</h1>
<p>A critic does not determine the action. Given an actor <span class="math inline">\(\pi\)</span>, it evaluates how good the actor is. But we can find a actor from a critic.</p>
<p>A critic is a <strong>state value function <span class="math inline">\(V^{\pi}(s)\)</span></strong>. It represents the <strong>cumulated reward</strong> expects to be obtained after seeing observation (state) s using actor <span class="math inline">\(\pi\)</span>. Critic is also a neural network, the input is the state s, and the output is the cumulated expected reward <span class="math inline">\(V^{\pi}(s)\)</span>.</p>
<p>But how to estimate the value function <span class="math inline">\(V^{\pi}(s)\)</span>? There are two main methods.</p>
<h4 id="monte-carlo-based-approach">Monte-Carlo based approach</h4>
<p>Under MC based approach, the critic watches actor <span class="math inline">\(\pi\)</span> generating trajectories. After seeing <span class="math inline">\(s_a\)</span>, until the end of the episode, wo get the cumulated reward <span class="math inline">\(G_a\)</span>; After seeing <span class="math inline">\(s_b\)</span>, until the end of the episode, the cumulated reward is <span class="math inline">\(G_b\)</span>. We then formulate a dataset, inputs are different states (<span class="math inline">\(s_a\)</span>, <span class="math inline">\(s_b\)</span>), outputs are corresponding cumulated reward (<span class="math inline">\(G_a\)</span>, <span class="math inline">\(G_b\)</span>). We turn the problem to a regression task.</p>
<h4 id="temporal-difference-approach">Temporal-difference approach</h4>
<p>Some applications have very long episodes, sothat delaying all learning until an episode’s end is too slow. Suppose we can know only part of the trajectory (the trajectory never ends), {<span class="math inline">\(\cdots,s_t,a_t,r_t,s_{t+1},\cdots\)</span>}. Then we have:</p>
<span class="math display">\[\begin{equation}
V^{\pi}(s_t) + r_t = V^{\pi}(s_{t+1})
\end{equation}\]</span>
<p>Our goal is to optimize the actor to have <span class="math inline">\(V^{\pi}(s_t) - V^{\pi}(s_{t+1})\)</span> close to <span class="math inline">\(r_t\)</span>.</p>
<h4 id="difference-between-mc-and-td">Difference between MC and TD</h4>
<p>MC: Larger variance but unbiased (r is random) TD: Small variance but may be biased</p>
<h4 id="q-learning">Q-learning</h4>
<p>Suppose we have a state-action value function <span class="math inline">\(Q^{\pi}(s,a)\)</span>, the cumulated reward expects to be obtained after seeing observation s and taking action a when using actor <span class="math inline">\(\pi\)</span>. The input to the actor is state s, the output is the state-action value function, like <span class="math inline">\([Q^{\pi} (s, a = left), Q^{\pi} (s, a = right), Q^{\pi} (s, a = fire)]\)</span>. But it has a limitation, the action space must be discrete.</p>
<p>The big picutre of Q-learning is:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">initialize Q[num_states, num_actions] arbitrarily</div><div class="line">observe initial state s</div><div class="line">repeat </div><div class="line">    select and carry out an action a</div><div class="line">    observe reward r and new state s&apos;</div><div class="line">    Q[s,a] = Q[s,a] + alpha * (r + gamma * max&#123;a&apos;&#125; Q[s&apos;,a&apos;] - Q[s,a])</div><div class="line">    s = s&apos;</div><div class="line">until terminated</div></pre></td></tr></table></figure>
<p>Q-value can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.</p>
<p>\begin{eqaution} L = [r + max_{a’}Q(s’,a’) - Q(s,a)]^2 \end{equation}</p>
<p>By now we have an idea how to estimate the future reward in each state using Q-learning and approximate the Q-function using a convolutional neural network. But it turns out that approximation of Q-values using non-linear functions is not very stable.</p>
<p>The most important trick is experience replay. During gameplay all the experiences &lt; s, a, r, s’ &gt; are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.</p>
<p>Deep Q-learning Algorithm <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">initialize replay memory D</div><div class="line">initialize action-value <span class="keyword">function</span> Q with random weights</div><div class="line">observe initial state s</div><div class="line">Repeat</div><div class="line">    select an action a</div><div class="line">        with probability epsilon select a random action</div><div class="line">        otherwise select a = argmax (a<span class="string">') Q(s,a'</span>)</div><div class="line">    carry out action a</div><div class="line">    observe reward r and new state s<span class="string">'</span></div><div class="line"><span class="string">    store experience &lt; s,a,r,s'</span> &gt; <span class="keyword">in</span> replay memory D</div><div class="line"></div><div class="line">    sample random transitions &lt;ss,aa,rr,ss<span class="string">'&gt; from replay memory D</span></div><div class="line"><span class="string">    calculate target for each minibatch transition</span></div><div class="line"><span class="string">        if ss'</span> is terminal state <span class="keyword">then</span> tt = rr</div><div class="line">        otherwise tt = rr + gamma max(a<span class="string">') Q(ss'</span>,aa<span class="string">')</span></div><div class="line"><span class="string">    train the Q network using (tt-Q(ss,aa))**2 as loss</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    s = s'</span></div><div class="line">until terminated</div></pre></td></tr></table></figure></p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">Demystify deep reinforcement learning</a></li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
            <a href="/tags/RL/" rel="tag"># RL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/20/2017-09-20Mathematical Proof of Gradient Vanishing Problem/" rel="next" title="Mathematical Proof of Gradient Vanishing Problem">
                <i class="fa fa-chevron-left"></i> Mathematical Proof of Gradient Vanishing Problem
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/24/2017-09-24Why deep?/" rel="prev" title="Why deep?">
                Why deep? <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">22</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#contents"><span class="nav-number">1.</span> <span class="nav-text">Contents</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#policy-gradient"><span class="nav-number">3.</span> <span class="nav-text">Policy Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#step-1-define-the-actor"><span class="nav-number">3.0.0.1.</span> <span class="nav-text">Step 1: Define the Actor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#step-2-how-to-evaluate-the-actor"><span class="nav-number">3.0.0.2.</span> <span class="nav-text">Step 2: How to Evaluate the Actor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#step-3-how-to-improve-the-actor"><span class="nav-number">3.0.0.3.</span> <span class="nav-text">Step 3: How to Improve the Actor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#policy-gradient-in-practice"><span class="nav-number">3.0.0.4.</span> <span class="nav-text">Policy Gradient in Practice</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#in-another-word-each-training-data-is-weighted-by-rtaun."><span class="nav-number">3.0.0.4.1.</span> <span class="nav-text">In another word, each training data is weighted by \(R(\tau^n)\).</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#value-based-approachlearning-a-critic"><span class="nav-number">4.</span> <span class="nav-text">Value Based Approach—Learning a Critic</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#monte-carlo-based-approach"><span class="nav-number">4.0.0.1.</span> <span class="nav-text">Monte-Carlo based approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#temporal-difference-approach"><span class="nav-number">4.0.0.2.</span> <span class="nav-text">Temporal-difference approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#difference-between-mc-and-td"><span class="nav-number">4.0.0.3.</span> <span class="nav-text">Difference between MC and TD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#q-learning"><span class="nav-number">4.0.0.4.</span> <span class="nav-text">Q-learning</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  








  








  





  

  

  

  
  


  

  

</body>
</html>

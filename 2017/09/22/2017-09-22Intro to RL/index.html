<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DL,RL," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Introduction Deep Reinforcement learning is the combination of deep learning and reinforcement learning. In the scenario of reinforcement leanring, we have three components: agent, environment and sta">
<meta name="keywords" content="DL,RL">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction to Reinforcement Learning">
<meta property="og:url" content="http://yoursite.com/2017/09/22/2017-09-22Intro to RL/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:description" content="Introduction Deep Reinforcement learning is the combination of deep learning and reinforcement learning. In the scenario of reinforcement leanring, we have three components: agent, environment and sta">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/2017-09-22Intro%20to%20RL/GlobalPicture.png">
<meta property="og:image" content="http://yoursite.com/images/2017-09-22Intro%20to%20RL/PolicyGradient.png">
<meta property="og:updated_time" content="2017-09-22T08:49:09.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Introduction to Reinforcement Learning">
<meta name="twitter:description" content="Introduction Deep Reinforcement learning is the combination of deep learning and reinforcement learning. In the scenario of reinforcement leanring, we have three components: agent, environment and sta">
<meta name="twitter:image" content="http://yoursite.com/images/2017-09-22Intro%20to%20RL/GlobalPicture.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/22/2017-09-22Intro to RL/"/>





  <title>Introduction to Reinforcement Learning | EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/22/2017-09-22Intro to RL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Introduction to Reinforcement Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-22T10:46:01+10:00">
                2017-09-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="introduction">Introduction</h1>
<p>Deep Reinforcement learning is the combination of deep learning and reinforcement learning.</p>
<p>In the scenario of reinforcement leanring, we have three components: agent, environment and state. Agent interacts with environment and “observe” something called state. <strong>State</strong> is not only about observed images, it includes all the information the agent senses. After receiving the observation, the agent takes some actions and these actions will change the environment. Then, environment will give agent a reward.</p>
<center>
<img src="/images/2017-09-22Intro%20to%20RL/GlobalPicture.png">
</center>
<p>Like the picture above, the agent observes a cup of water, takes the action to knock it over and spill the water, the environment gives it a negative reward like -1. The agent keeps interacting with the environment, and adjust the parameters according to the rewards to take actions to maximize the expected reward.</p>
<p><em><strong>Reinforcement leanring is trying to find a best function <span class="math inline">\(\pi(a|s)\)</span> (called actor/policy), which can maximize the expected reward. Compared to supervised leanring approach learning from a teacher, reinforcement leanring learns from experience.</strong></em></p>
<p>RL has two properties.</p>
<ul>
<li>Reward Delay.</li>
<li>Agent’s actions affects subsequent data it receives.</li>
</ul>
<h1 id="policy-gradient">Policy Gradient</h1>
<h4 id="step-1-define-the-actor">Step 1: Define the Actor</h4>
<p>In policy gradient, neural network plays the role as actor. The input to the neural network is the observation of the agent represented as a vector or a matrix; each neuron in the output layer corresponds to one action. Neural network is good at generalization than past methods in RL like a lookup table, which means the agent can still take proper actions if the state never appears.</p>
<center>
<img src="/images/2017-09-22Intro%20to%20RL/PolicyGradient.png">
</center>
<p>The picture above shows an example. Input is a screenshot of the game, nerual network is trying to learn a good function <span class="math inline">\(\pi (a|s)\)</span> and finally outputs the probability distribution over all possible actions.</p>
<h4 id="step-2-how-to-evaluate-the-actor">Step 2: How to Evaluate the Actor</h4>
<p>But how to evaluate the actor? Suppose we are given an actor <span class="math inline">\(\pi_{\theta}(a|s)\)</span>, <span class="math inline">\(\theta\)</span> represent all the parameters in the neural network. We use this actor to generate a sequence of actions:</p>
<ul>
<li>Start with state <span class="math inline">\(s_1\)</span></li>
<li>NN decides to take action <span class="math inline">\(a_1\)</span></li>
<li>NN obtains reward <span class="math inline">\(r_1\)</span></li>
<li>NN sees observation <span class="math inline">\(s_2\)</span></li>
<li>NN decides to take action <span class="math inline">\(a_2\)</span></li>
<li>NN obtains reward <span class="math inline">\(r_2\)</span></li>
<li>……</li>
<li>NN decides to take action <span class="math inline">\(a_T\)</span></li>
<li>NN obtains reward <span class="math inline">\(r_T\)</span> <strong>[End]</strong></li>
</ul>
<p>And total reward is: <span class="math inline">\(R_{\theta} = \Sigma^T_{t=1} r_t\)</span></p>
<p>We should notice that, even with the same actor, <span class="math inline">\(R_{\theta}\)</span> is different each time because of the randomness of the environment and actor (output of the actions is a distribution, action is sampled from the distribution). So what we really interested in is the expected value of <span class="math inline">\(R\)</span> defined as <span class="math inline">\(\bar{R}_{\theta}\)</span>. It’s the expected reward of using the actor <span class="math inline">\(\pi_{\theta}(a|s)\)</span> to generate many sequences. Of course, we can evaluates the goodness of the actor <span class="math inline">\(\pi_{\theta}(a|s)\)</span> using <span class="math inline">\(\bar{R}_{\theta}\)</span>.</p>
<p>To calculate <span class="math inline">\(\bar{R}_{\theta}\)</span>, first we define a trajectory as <span class="math inline">\(\tau = \{ s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T \}\)</span>:</p>
<span class="math display">\[\begin{equation}\begin{split}
p(\tau | \theta) &amp;= p(s_1)p(a_1|s_1,\theta)p(r_1,s_2 | s_1, a_1)p(a_2 | s_2, \theta) p(r_2,s_3 | s_2,a_2) \cdots\\\\
&amp;= p(s_1)\prod\limits^T_{t=1}p(a_t | s_t, \theta) p(r_t,s_{t+1} | s_t,a_t)
\end{split}\end{equation}\]</span>
<p>The first term and third term do not relate to actor, but the second term is controled by actor <span class="math inline">\(\pi_{\theta}\)</span>.</p>
<p>Then we can calculate <span class="math inline">\(\bar{R}_{\theta}\)</span> as:</p>
<span class="math display">\[\begin{equation}
\bar{R}_{\theta} = \Sigma_{\tau} R(\tau) p(\tau | \theta)
\end{equation}\]</span>
<p>However, it’s not possible to compute above equation in real application, so we approximate the value by MCMC method. For example, when using <span class="math inline">\(\pi_{\theta}\)</span> to play a game, we generate <span class="math inline">\(N\)</span> trajectories and obtain {<span class="math inline">\(\tau^1,\tau^2,...,\tau^N\)</span>}, and use the <span class="math inline">\(N\)</span> trajectories to approximate <span class="math inline">\(\bar{R}_{\theta} \approx \frac{1}{N} \Sigma^N_{n=1} R(\tau^n)\)</span>.</p>
<h4 id="step-3-how-to-improve-the-actor">Step 3: How to Improve the Actor</h4>
<p>We first define our problem as:</p>
<span class="math display">\[\begin{equation}
\theta^* = argmax_{\theta} \space \bar{R}_{\theta}
\end{equation}\]</span>
<p>In general neural network, we usually use gradient descent/ascent based methods to obtain the optimal value. In RL situation, if we can compute the derivative of <span class="math inline">\(\bar{R}_{\theta}\)</span> with respect to <span class="math inline">\(\theta\)</span>, then problem solved.</p>
<p>To describle it in math, we have <span class="math inline">\(\theta = \{w_1,w_2,...,b_1,b_2,...\}\)</span>, then:</p>
<span class="math display">\[\begin{equation}
\mathbf{\nabla} \bar{R}_{\theta} = 
\left(\begin{array}{cc} 
\partial \bar{R}_{\theta} / \partial w_1 \\ 
\partial \bar{R}_{\theta} / \partial w_2 \\ 
\vdots \\
\partial \bar{R}_{\theta} / \partial b_1 \\
\partial \bar{R}_{\theta} / \partial b_2 \\
\vdots \\
\end{array}\right)
\end{equation}\]</span>
<p>Start with <span class="math inline">\(\theta^0\)</span> (gradietn ascent)</p>
<ul>
<li><span class="math inline">\(\theta^1 \leftarrow \theta^0 + \eta \mathbf{\nabla} \bar{R}_{\theta^0}\)</span></li>
<li><span class="math inline">\(\theta^2 \leftarrow \theta^1 + \eta \mathbf{\nabla} \bar{R}_{\theta^1}\)</span></li>
<li><span class="math inline">\(\cdots \cdots\)</span></li>
</ul>
<p>Recall that: <span class="math inline">\(\bar{R}_{\theta} = \Sigma_{\tau} R(\tau) p(\tau | \theta)\)</span>, then <span class="math inline">\(\nabla \bar{R}_{\theta} = \Sigma_{\tau} R(\tau) \nabla p(\tau | \theta)\)</span>. Now, we can not use MCMC methods to approximate the gradient because we do not have a distribution. However, we can do a trick to get the distribution <span class="math inline">\(p(\tau | \theta)\)</span> back.</p>
<span class="math display">\[\begin{equation}\begin{split}
\nabla \bar{R}_{\theta} &amp;= \Sigma_{\tau} R(\tau) \nabla p(\tau | \theta)\\\\
&amp;= \Sigma_{\tau} R(\tau) p(\tau | \theta) \frac{\nabla p(\tau | \theta)}{p(\tau | \theta)}\\\\
&amp;= \Sigma_{\tau} R(\tau) p(\tau | \theta) \nabla log p(\tau | \theta)\\\\
&amp;\approx \frac{1}{N} \Sigma^N_{n=1} R(\tau^n) \nabla log p(\tau^n | \theta) \space (Because \space we \space \space have \space summation \space operation \space and \space a \space distribution)
\end{split}\end{equation}\]</span>
<p>Recall again: <span class="math inline">\(p(\tau | \theta) = p(s_1)\prod\limits^T_{t=1}p(a_t | s_t, \theta) p(r_t,s_{t+1} | s_t,a_t)\)</span>, then:</p>
<span class="math display">\[\begin{equation}\begin{split}
\nabla log p(\tau | \theta) &amp;= \nabla [log p(s_1) + \Sigma^T_{t=1} log p(a_t | s_t, \theta) + log p(r_t,s_{t+1} | s_t,a_t)]\\\\
&amp;= \Sigma^T_{t=1} \nabla log p(a_t | s_t, \theta) \space (Ignore \space the \space terms \space not \space related \space to \space  \theta)
\end{split}\end{equation}\]</span>
<p>And finally, we have the <strong>Policy Gradient Algorithm</strong></p>
<span class="math display">\[\begin{equation}\begin{split}
\theta^{new} &amp;\leftarrow \theta^{old} + \eta \nabla \bar{R}_{\theta^{old}}\\\\
\nabla \bar{R}_{\theta} &amp;\approx \frac{1}{N} \Sigma^N_{n=1} R(\tau^n) \nabla logp(\tau^n | \theta)\\\\
&amp;= \frac{1}{N} \Sigma^N_{n=1} R(\tau^n) \Sigma^{T_n}_{t=1} \nabla logp(a^n_t | s^n_t, \theta)\\\\
&amp;= \frac{1}{N} \Sigma^N_{n=1} \Sigma^{T_n}_{t=1} R(\tau^n) \nabla logp(a^n_t | s^n_t, \theta)
\end{split}\end{equation}\]</span>
<p>To explain the meaning of the above formula, suppose in <span class="math inline">\(\tau^n\)</span>, the actor takes <span class="math inline">\(a^n_t\)</span> when seeing <span class="math inline">\(s^n_t\)</span>:</p>
<ul>
<li>Tuning <span class="math inline">\(\theta\)</span> to increase <span class="math inline">\(p(a^n_t) | s^n_t\)</span>, if <span class="math inline">\(R(\tau^n)\)</span> is positive</li>
<li>Tuning <span class="math inline">\(\theta\)</span> to decrease <span class="math inline">\(p(a^n_t) | s^n_t\)</span>, if <span class="math inline">\(R(\tau^n)\)</span> is negative</li>
</ul>
<p><strong>It is very important to consider the cumulative reward <span class="math inline">\(R(\tau^n)\)</span> of the whole trajectory <span class="math inline">\(\tau^n\)</span> instead of immediate reward <span class="math inline">\(r^n_t\)</span>.</strong></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
            <a href="/tags/RL/" rel="tag"># RL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/20/2017-09-20Mathematical Proof of Gradient Vanishing Problem/" rel="next" title="Mathematical Proof of Gradient Vanishing Problem">
                <i class="fa fa-chevron-left"></i> Mathematical Proof of Gradient Vanishing Problem
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#policy-gradient"><span class="nav-number">2.</span> <span class="nav-text">Policy Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#step-1-define-the-actor"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">Step 1: Define the Actor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#step-2-how-to-evaluate-the-actor"><span class="nav-number">2.0.0.2.</span> <span class="nav-text">Step 2: How to Evaluate the Actor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#step-3-how-to-improve-the-actor"><span class="nav-number">2.0.0.3.</span> <span class="nav-text">Step 3: How to Improve the Actor</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  








  








  





  

  

  

  
  


  

  

</body>
</html>

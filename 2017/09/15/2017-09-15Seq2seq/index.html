<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="RNN,DL," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Contents  Sequence to Sequence Learning with Neural Networks Neural Machine Translation by Jointly Learning to Align and Translate The Cost of Attention Practical Illustration  Sequence to Sequence Le">
<meta name="keywords" content="RNN,DL">
<meta property="og:type" content="article">
<meta property="og:title" content="Sequence To Sequence Models">
<meta property="og:url" content="http://yoursite.com/2017/09/15/2017-09-15Seq2seq/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:description" content="Contents  Sequence to Sequence Learning with Neural Networks Neural Machine Translation by Jointly Learning to Align and Translate The Cost of Attention Practical Illustration  Sequence to Sequence Le">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/2017-09-15Seq2seq/GeneralSeq2seq.png">
<meta property="og:image" content="http://yoursite.com/images/2017-09-15Seq2seq/SoftAttentionSeq2seq.png">
<meta property="og:updated_time" content="2017-09-22T00:31:28.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sequence To Sequence Models">
<meta name="twitter:description" content="Contents  Sequence to Sequence Learning with Neural Networks Neural Machine Translation by Jointly Learning to Align and Translate The Cost of Attention Practical Illustration  Sequence to Sequence Le">
<meta name="twitter:image" content="http://yoursite.com/images/2017-09-15Seq2seq/GeneralSeq2seq.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/15/2017-09-15Seq2seq/"/>





  <title>Sequence To Sequence Models | EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/15/2017-09-15Seq2seq/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Sequence To Sequence Models</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-15T10:21:38+10:00">
                2017-09-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="contents">Contents</h1>
<ul>
<li>Sequence to Sequence Learning with Neural Networks</li>
<li>Neural Machine Translation by Jointly Learning to Align and Translate</li>
<li>The Cost of Attention</li>
<li>Practical Illustration</li>
</ul>
<h2 id="sequence-to-sequence-learning-with-neural-networks">Sequence to Sequence Learning with Neural Networks</h2>
<center>
<img src="/images/2017-09-15Seq2seq/GeneralSeq2seq.png">
</center>
<p>The Recurrent Neural Network (RNN) is a natural generalization of feedforward neural networks to sequences. Given a sequence of inputs <span class="math inline">\((x_1,...,x_T)\)</span>, a standard RNN computes a sequence of outputs <span class="math inline">\((y_1,...,y_T)\)</span> by iterating the following equation:</p>
<span class="math display">\[\begin{equation}\begin{split}
h_t &amp;= sigm(W^{hx}x_t + W^{hh}h_{t-1})\\\\
y_t &amp;= W^{yh}h_t
\end{split}\end{equation}\]</span>
<p>The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships.</p>
<p>A simple strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one RNN, and then to map the vector to the target sequence with another RNN. While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies. However, the Long Short-Term Memory (LSTM) is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting.</p>
<p>The goal of the LSTM is to estimate the conditional probability <span class="math inline">\(p(y_1,...,y_{T&#39;} | x_1,...,x_T)\)</span> where <span class="math inline">\((x_1,...,x_T)\)</span> is an input sequence and <span class="math inline">\((y_1,...,y_{T&#39;})\)</span> is its corresponding output sequence whose length <span class="math inline">\(T&#39;\)</span> may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-dimensional representation <span class="math inline">\(v\)</span> of the input sequence <span class="math inline">\((x_1,...,x_T)\)</span> given by the last hidden state of the LSTM, and then computing the probability of <span class="math inline">\(y_1,...,y_{T&#39;}\)</span> with a standard LSTM-LM formulation whose initial hidden state is set to the representation <span class="math inline">\(v\)</span> of <span class="math inline">\(x_1,...,x_T\)</span>:</p>
<span class="math display">\[\begin{equation}\begin{split}
p(y_1,...,y_{T&#39;} | x_1,...,x_T) = \prod\limits^{T&#39;}_{t=1}p(y_t|v,y_1,...,y_{t-1})
\end{split}\end{equation}\]</span>
<p>In this equation, each <span class="math inline">\(p(y_t|v,y_1,...,y_{t-1})\)</span> distribution is represented with a softmax over all the words in the vocabulary. Note that each sentence ends with a special end-of-sentence symbol “<span class="math inline">\(&lt;EOS&gt;\)</span>”, which enables the model to define a distribution over sequences of all possible lengths. The overall scheme is outlined in figure 1, where the shown LSTM computes the representation of “A”, “B”, “C”, “<span class="math inline">\(&lt;EOS&gt;\)</span>” and then uses this representation to compute the probability of “W”, “X”, “Y”, “Z”, “<span class="math inline">\(&lt;EOS&gt;\)</span>”.</p>
<p>Three things to note:</p>
<ol style="list-style-type: decimal">
<li>Two LSTM are used, one for the encoder and one for the decoder.</li>
<li>Deep LSTMs significantly outperformed shallow LSTMs.</li>
<li>It extremely valuable to reverse the order of the words of the input sentence.</li>
<li>Different sentences have different lengths. Most sentences are short, but some sentences are long, so a minibatch of 128 randomly chosen training sentences will have many short sentences and few long sentences, and as a result, much of the computation in the minibatch is wasted. The strategy is that make sure all sentences in a minibatch are roughly of the same length.</li>
</ol>
<h2 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation by Jointly Learning to Align and Translate</h2>
<center>
<img src="/images/2017-09-15Seq2seq/SoftAttentionSeq2seq.png">
</center>
<p>Each conditional probability is defined as:</p>
<span class="math display">\[\begin{equation}\begin{split}
p(y_i|y_1,...,y_{i-1},\mathbf{x}) = g(y_{i-1},s_i,c_i)
\end{split}\end{equation}\]</span>
where <span class="math inline">\(s_i\)</span> is an RNN hidden state for time <span class="math inline">\(i\)</span>, computed by:
<span class="math display">\[\begin{equation}\begin{split}
s_i = f(s_{i-1},y_{i-1},c_i)
\end{split}\end{equation}\]</span>
<p>It should be noted that unlike the above general encoder-decoder approach, here the probability is conditioned on a distinct context vector <span class="math inline">\(c_i\)</span> for each target <span class="math inline">\(y_i\)</span>.</p>
<p>The context vector <span class="math inline">\(c_i\)</span> depends on a sequence of annotations <span class="math inline">\((h_1,...,h_{T_x})\)</span> to which an encoder maps the input sentence. Each annotation <span class="math inline">\(h_i\)</span> contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence. The context vector <span class="math inline">\(c_i\)</span> is, then, computed as a weighted sum of these annotations <span class="math inline">\(h_i\)</span>:</p>
<span class="math display">\[\begin{equation}\begin{split}
c_i = \prod\limits^{T_x}_{j=1}\alpha_{ij}h_j
\end{split}\end{equation}\]</span>
<p>The weight <span class="math inline">\(\alpha_{ij}\)</span> of each annotation <span class="math inline">\(h_j\)</span> is computed by:</p>
<span class="math display">\[\begin{equation}\begin{split}
\alpha_{ij} = \frac{exp(e_{ij})}{\Sigma^{T_x}_{k=1} exp(e_{ik})}
\end{split}\end{equation}\]</span>
<p>where</p>
<span class="math display">\[\begin{equation}\begin{split}
e_{ij} = a(s_{i-1}, h_j)
\end{split}\end{equation}\]</span>
<p>is an alignment model which scores how well the inputs around position <span class="math inline">\(j\)</span> and the output at position <span class="math inline">\(i\)</span> match. The score is based on the RNN hidden state <span class="math inline">\(s_{i−1}\)</span> and the j-th annotation <span class="math inline">\(h_j\)</span> of the input sentence.</p>
<h2 id="the-cost-of-attention">The Cost of Attention</h2>
<p>If we look a bit more look closely at the equation for attention we can see that attention comes at a cost. We need to calculate an attention value for each combination of input and output word. If you have a 50-word input sequence and generate a 50-word output sequence that would be 2500 attention values. That’s not too bad, but if you do character-level computations and deal with sequences consisting of hundreds of tokens the above attention mechanisms can become prohibitively expensive.</p>
<p>Actually, that’s quite counterintuitive. Human attention is something that’s supposed to save computational resources. By focusing on one thing, we can neglect many other things. But that’s not really what we’re doing in the above model. We’re essentially looking at everything in detail before deciding what to focus on. Intuitively that’s equivalent outputting a translated word, and then going back through all of your internal memory of the text in order to decide which word to produce next. That seems like a waste, and not at all what humans are doing. In fact, it’s more akin to memory access, not attention, which in my opinion is somewhat of a misnomer (more on that below). Still, that hasn’t stopped attention mechanisms from becoming quite popular and performing well on many tasks.</p>
<h2 id="practical-illustration">Practical Illustration</h2>
<h4 id="padding">Padding</h4>
<p>Before training, we work on the dataset to convert the variable length sequences into fixed length sequences, by padding. We use a few special symbols to fill in the sequence, and each symbol is also represented by a vector.</p>
<ol style="list-style-type: decimal">
<li><em>EOS</em>: End of sentence</li>
<li><em>PAD</em>: Filler</li>
<li><em>GO</em>: Start decoding</li>
<li><em>UNK</em>: Unknown; word not in vocabulary</li>
</ol>
<p>Consider the following query-response pair.</p>
<p>Q: How are you? A: I am fine. Assuming that we would like our sentences (queries and responses) to be of fixed length, 10, this pair will be converted to:</p>
<p>Q: [ PAD, PAD, PAD, PAD, PAD, PAD, “?”, “you”, “are”, “How” ] and [ “How”, “are”, “you”, “?” PAD, PAD, PAD, PAD, PAD, PAD ] A: [ GO, “I”, “am”, “fine”, “.”, EOS, PAD, PAD, PAD, PAD ]</p>
<h4 id="bucketing">Bucketing</h4>
<p>Introduction of padding did solve the problem of variable length sequences, but consider the case of large sentences.If the largest sentence in our dataset is of length 100, we need to encode all our sentences to be of length 100, in order to not lose any words. Now, what happens to “How are you?” ? There will be 97 PAD symbols in the encoded version of the sentence. This will overshadow the actual information in the sentence.</p>
<p>Bucketing kind of solves this problem, by putting sentences into buckets of different sizes. Consider this list of buckets : <span class="math inline">\([ (5,10), (10,15), (20,25), (40,50) ]\)</span>. If the length of a query is 4 and the length of its response is 4 (as in our previous example), we put this sentence in the bucket (5,10). The query will be padded to length 5 and the response will be padded to length 10. While running the model (training or predicting), we use a different model for each bucket, compatible with the lengths of query and response. All these models, share the same parameters and hence function exactly the same way.</p>
<p>If we are using the bucket (5,10), our sentences will be encoded to :</p>
<p>Q : [ PAD, “?”, “you”, “are”, “How” ] and [ “How”, “are”, “you”, “?” PAD ] A : [ GO, “I”, “am”, “fine”, “.”, EOS, PAD, PAD, PAD, PAD ]</p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="external">Attention and memory in deep learning and nlp</a></li>
<li><a href="http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/" target="_blank" rel="external">Practical seq2seq</a></li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/RNN/" rel="tag"># RNN</a>
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/14/2017-09-14Language Modeling with Gated Convolutional Networks/" rel="next" title="Language Modeling with Gated Convolutional Networks">
                <i class="fa fa-chevron-left"></i> Language Modeling with Gated Convolutional Networks
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/15/2017-09-15Highway&Residual&Training RNNs as Fast as CNNs/" rel="prev" title="Highway Network & Residual Network & Training RNNs as Fast as CNNs">
                Highway Network & Residual Network & Training RNNs as Fast as CNNs <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#contents"><span class="nav-number">1.</span> <span class="nav-text">Contents</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sequence-to-sequence-learning-with-neural-networks"><span class="nav-number">1.1.</span> <span class="nav-text">Sequence to Sequence Learning with Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-machine-translation-by-jointly-learning-to-align-and-translate"><span class="nav-number">1.2.</span> <span class="nav-text">Neural Machine Translation by Jointly Learning to Align and Translate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-cost-of-attention"><span class="nav-number">1.3.</span> <span class="nav-text">The Cost of Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#practical-illustration"><span class="nav-number">1.4.</span> <span class="nav-text">Practical Illustration</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#padding"><span class="nav-number">1.4.0.1.</span> <span class="nav-text">Padding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bucketing"><span class="nav-number">1.4.0.2.</span> <span class="nav-text">Bucketing</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">2.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  








  








  





  

  

  

  
  


  

  

</body>
</html>

<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta property="og:type" content="website">
<meta property="og:title" content="EmberNLP">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EmberNLP">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title>EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/06/2017-09-06Siamese Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/06/2017-09-06Siamese Network/" itemprop="url">Siamese Network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-06T16:56:51+10:00">
                2017-09-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>The main idea of Siamese Network is to map the inputs to a new space by a function which can be learned by neural network. Then it is able to use similarity measurement like (Euclidean Metric or Cosine Similarity) to compare the extent of similarity. During training, the goal is to minimize the value of cost functon of the pairs of samples from same category, in the meantime, maximize the value of cost function of the pairs from different categories. For a given neural network (CNN, RNN), the goal is to optimize its parameters <span class="math inline">\(\mathbf{W}\)</span> to make:</p>
<p><span class="math inline">\(E_w = \left \| G_w(X_1) - G_w(X_2) \right \|\)</span>, small if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are in the same category</p>
<p><span class="math inline">\(E_w = \left \| G_w(X_1) - G_w(X_2) \right \|\)</span>, large if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are in the different categories</p>
<p>The only assumption about W is differentiable.</p>
<p>And the cost function is defined as below: <span class="math display">\[\mathcal{L}(W) = \Sigma^n_{i=1} (1-Y)L_G(E_w(x_1,x_2)^i) + YL_I(E_w(X_1,X_2)^i)\]</span></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/05/2017-09-05PixelCNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/05/2017-09-05PixelCNN/" itemprop="url">PixelCNN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-05T14:49:49+10:00">
                2017-09-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="content">Content</h1>
<ul>
<li>PixelCNN</li>
<li>Gated Convolutional Layer</li>
<li>BlindSpot And Solution</li>
<li>Conditional PixelCNN</li>
<li>fast PixelCNN ++</li>
<li>PixelRNN</li>
</ul>
<h2 id="pixelcnn">PixelCNN</h2>
<p>PixelCNNs (and PixelRNNs) model the joint distribution of pixels over an image <span class="math inline">\(x\)</span> as the following product of conditional distributions, where <span class="math inline">\(x_i\)</span> is a single pixel: <span class="math display">\[p(x) = \prod\limits^{n^2}_{i=1}p(x_i|x_1,...,x_{i-1})\]</span> The ordering of the pixel dependencies is in raster scan order: row by row and pixel by pixel within every row. Every pixel therefore depends on all the pixels above and to the left of it, and not on any of other pixels.</p>
<p>In PixelCNN every conditional distribution is modelled by a convolutional neural network. To make sure the CNN can only use information about pixels above and to the left of the current pixel, the convolution are <em><strong>masked</strong></em> as the figure below.</p>
<center>
<img src="/images/2017-09-05PixelCNN/PixelCNN.png">
</center>
<p>For image with only one channel (like mnist dataset): <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">filter_mid_h = shape[0]//2</div><div class="line">filter_mid_w = shape[1]//2</div><div class="line">mask_filter =np.ones(shape, dtype= np.float32)</div><div class="line">mask_filter[filter_mid_h,filter_mid_w+1:, :, :] = 0.</div><div class="line">mask_filter[filter_mid_h+1:, :, :, :] = 0.</div><div class="line"><span class="comment"># W *= mask_filter</span></div></pre></td></tr></table></figure></p>
<p>For a <span class="math inline">\(7 * 7\)</span> filter, the mask tensor should look like this: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.],</div><div class="line">       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],</div><div class="line">       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],</div><div class="line">       [ 1.,  1.,  1.,  0.,  0.,  0.,  0.],</div><div class="line">       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</div><div class="line">       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</div><div class="line">       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]])</div></pre></td></tr></table></figure></p>
<p>For colourful images with three channels (R,G,B), in the first layer, each of the RGB channels is connected to previous channels and to the context, but is not connected to itself. In subsequent layers, the channels are also connected to themselves (Shown below).</p>
<center>
<img src="/images/2017-09-05PixelCNN/RGB.png">
</center>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def bmask(i_in, i_out):</div><div class="line">	cin_idx = np.expand_dims(np.arange(Cin) % 3 == i_in, 1)</div><div class="line">    cout_idx = np.expand_dims(np.arange(Cout) % 3 == i_out, 0)</div><div class="line">    a1, a2 = np.broadcast_arrays(cin_idx, cout_idx)</div><div class="line">    <span class="built_in">return</span> a1 * a2</div><div class="line"></div><div class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(3):</div><div class="line">    mask_filter[filter_mid_h,filter_mid_w][bmask(j,j)] = 0. <span class="keyword">if</span> mask ==<span class="string">"A"</span> <span class="keyword">else</span> 1.</div><div class="line"></div><div class="line">mask_filter[filter_mid_h,filter_mid_w][bmask(1, 0)] = 0.</div><div class="line">mask_filter[filter_mid_h,filter_mid_w][bmask(2, 0)] = 0.</div><div class="line">mask_filter[filter_mid_h,filter_mid_w][bmask(2, 1)] = 0.</div></pre></td></tr></table></figure>
<p>For example, <span class="math inline">\(i_{in} = 3\)</span>, <span class="math inline">\(i_{out} = 6\)</span>, the last two dimensions should look like this: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Mask A</span></div><div class="line">array([[ 0.,  1.,  1.,  0.,  1.,  1.],</div><div class="line">       [ 0.,  0.,  1.,  0.,  0.,  1.],</div><div class="line">       [ 0.,  0.,  0.,  0.,  0.,  0.]])</div><div class="line"><span class="comment"># Mask B</span></div><div class="line">array([[ 1.,  1.,  1.,  1.,  1.,  1.],</div><div class="line">       [ 0.,  1.,  1.,  0.,  1.,  1.],</div><div class="line">       [ 0.,  0.,  1.,  0.,  0.,  1.]])</div></pre></td></tr></table></figure></p>
<center>
<img src="/images/2017-09-05PixelCNN/PixelCNNPred.png">
</center>
<p>As shown above, the 256 possible values for each colour channel are then modelled using a softmax.</p>
<p>PixelCNN typically consists of a stack of masked convolutional layers that takes an <span class="math inline">\((N,N,3)\)</span> image as input and produces <span class="math inline">\((N,N,3,256)\)</span> predictions as output. The use of convolutions allows the predictions for all the pixels to be made in parallel during training.</p>
<p>During sampling the predictions are sequential: every time a pixel is predicted, it is fed back into the network to predict the next pixel. This sequentiality is essential to generating high quality images, as it allows every pixel to depend in a highly non-linear and multimodal way on the previous pixels.</p>
<h2 id="gated-convolutional-layer">Gated Convolutional Layer</h2>
<p>For a gated convolutional layer, relu activation block with more complex combination of sigmoid (as a forget gate) and tanh (as real activation). It suggests that: <span class="math display">\[\mathbf{y} = tanh(W_{k,f} \ast \mathbf{x}) \odot \sigma(W_{k,g} \ast \mathbf{x})\]</span></p>
<p>Here is the code to illustrate the formula: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">W_f = get_weights(W_shape, <span class="string">"v_W"</span>, mask=mask)</div><div class="line">W_g = get_weights(W_shape, <span class="string">"h_W"</span>, mask=mask)</div><div class="line"></div><div class="line">b_f = tf.get_variable(<span class="string">"v_b"</span> , b_shape, tf.float32, tf.zeros_initializer)</div><div class="line">b_g = tf.get_variable(<span class="string">"h_b"</span> , b_shape, tf.float32, tf.zeros_initializer)</div><div class="line"></div><div class="line">conv_f = tf.nn.conv2d(input_layer, W_f, strides=[1,1,1,1], padding=<span class="string">'SAME'</span>)</div><div class="line">conv_g = tf.nn.conv2d(input_layer, W_g, strides=[1,1,1,1], padding=<span class="string">'SAME'</span>)</div><div class="line">output_layer = tf.mul(tf.tanh(conv_f + b_f), tf.sigmoid(conv_g+ b_g))</div></pre></td></tr></table></figure></p>
<h2 id="blindspot-and-solution">BlindSpot And Solution</h2>
<p>Note that a significant portion of the input image is ignored by the masked convolutional architecture. This ‘blind spot’ can cover as much as a quarter of the potential receptive field (e.g., when using 3x3 filters), meaning that none of the content to the right of the current pixel would be taken into account.</p>
<center>
<img src="/images/2017-09-05PixelCNN/BlindSpot.png">
</center>
<p>To gain some intuition about blind spot, following code may help: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># create 5x5 input</span></div><div class="line"><span class="comment"># chainer requires input to have shape [BATCH, CHANNELS, HEIGHT, WIDTH]</span></div><div class="line">input = np.arange(25).reshape([1,1,5,5]).astype(<span class="string">'f'</span>)</div><div class="line"><span class="comment"># array([[[[    0.,     1.,     2.,     3.,     4.],</span></div><div class="line"><span class="comment">#          [    5.,     6.,     7.,     8.,     9.],</span></div><div class="line"><span class="comment">#          [   10.,    11.,    12.,    13.,    14.],</span></div><div class="line"><span class="comment">#          [   15.,    16.,    17.,    18.,    19.],</span></div><div class="line"><span class="comment">#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)</span></div><div class="line"></div><div class="line"><span class="comment"># create kernel of ones so it just sums all values within</span></div><div class="line"><span class="comment"># use one for simplicity: easy to check</span></div><div class="line">kernel = np.ones([3, 3])</div><div class="line"><span class="comment"># turn to proper type 'A' mask</span></div><div class="line">kernel[2:, :] = 0.0</div><div class="line">kernel[1, 1:] = 0.0</div><div class="line"><span class="comment"># array([[ 1.,  1.,  1.],</span></div><div class="line"><span class="comment">#        [ 1.,  0.,  0.],</span></div><div class="line"><span class="comment">#        [ 0.,  0.,  0.]])</span></div><div class="line"></div><div class="line"><span class="comment"># create two convolution layers with total receptive field size 5x5</span></div><div class="line"><span class="comment"># so out input is exact fit</span></div><div class="line">import chainer.links as L</div><div class="line"></div><div class="line">l1 = L.Convolution2D(1, 1, ksize=3, initialW=kernel)</div><div class="line">l2 = L.Convolution2D(1, 1, ksize=3, initialW=kernel)</div><div class="line"></div><div class="line"><span class="comment"># here is the trick: pixel at [1, 4] position will be inside blind spot</span></div><div class="line"><span class="comment"># if we perform convolution its value won't be included in final sum</span></div><div class="line"><span class="comment"># so let's increase its value so it would be easy to check</span></div><div class="line">input[:, :, 1, 4] = 1000</div><div class="line"><span class="comment"># array([[[[    0.,     1.,     2.,     3.,     4.],</span></div><div class="line"><span class="comment">#          [    5.,     6.,     7.,     8.,  1000.],</span></div><div class="line"><span class="comment">#          [   10.,    11.,    12.,    13.,    14.],</span></div><div class="line"><span class="comment">#          [   15.,    16.,    17.,    18.,    19.],</span></div><div class="line"><span class="comment">#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)</span></div><div class="line"></div><div class="line">output = l2(l1(input)).data</div><div class="line"><span class="comment"># array([[[[ 64.]]]], dtype=float32)</span></div><div class="line"><span class="comment"># Viola! Sum is lesser that 1000 which means pixel at [1, 4] wasn't seen!</span></div><div class="line"></div><div class="line"><span class="comment"># Otherwise, let's return it value back</span></div><div class="line">input[:, :, 1, 4] = 9</div><div class="line"><span class="comment"># array([[[[    0.,     1.,     2.,     3.,     4.],</span></div><div class="line"><span class="comment">#          [    5.,     6.,     7.,     8.,     9.],</span></div><div class="line"><span class="comment">#          [   10.,    11.,    12.,    13.,    14.],</span></div><div class="line"><span class="comment">#          [   15.,    16.,    17.,    18.,    19.],</span></div><div class="line"><span class="comment">#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)</span></div><div class="line"></div><div class="line"><span class="comment"># perform computation again..</span></div><div class="line">output = l2(l1(input)).data</div><div class="line"><span class="comment"># array([[[[ 64.]]]], dtype=float32)</span></div><div class="line"><span class="comment"># Another evidence: no matter what value we assign to it final sum doesn't change</span></div><div class="line"><span class="comment"># That proves it's within blind spot and we can't access information at it.</span></div></pre></td></tr></table></figure></p>
<p>To remove the blind spot, authors introduce neat idea: they split convolution into two different operations: two separate stacks - vertical and horizontal.</p>
<center>
<img src="/images/2017-09-05PixelCNN/PixelCNNStack.png">
</center>
<p>Here we have horizontal stack (in purple): convolution operation that conditions on only current row, so it has access to left pixels. Vertical stack (blue) has access to all top pixels. Implementation details would follow. Note that horizontal and vertical stacks are sort of independent: vertical stack should not access any information horizontal stack has: otherwise it will have access to pixels it shouldn’t see. But vertical stack can be connected to vertical as it predicts pixel following those in vertical stack.</p>
<h2 id="conditional-pixelcnn">Conditional PixelCNN</h2>
<p>Given a high-level image description represented as a latent vector <span class="math inline">\(\mathbf{h}\)</span>, we seek to model the conditional distribution <em><span class="math inline">\(p(\mathbf{x}|\mathbf{h})\)</span></em> of images suiting this description. Formally the conditional PixelCNN models the following distribution: <span class="math display">\[p(\mathbf{x}|\mathbf{h}) = \prod\limits^{n^2}_{i=1}p(x_i|x_1,x_2,...,x_{i-1},\mathbf{h})\]</span></p>
<p>To model the conditional distribution by adding terms that depend on <span class="math inline">\(\mathbf{h}\)</span> to the activations before the nonlinearities, which now becomes: <span class="math display">\[\mathbf{y} = tanh(W_{k,f} \ast \mathbf{x} + V^T_{k,f} \mathbf{h}) \odot \sigma(W_{k,g} \ast \mathbf{x} + V^T_{k,g} \mathbf{h})\]</span></p>
<p>where k is the layer number. If <span class="math inline">\(\mathbf{h}\)</span> is a one-hot encoding that specifies a class this is equivalent to adding a class dependent bias at every layer. Notice that the conditioning does not depend on the location of the pixel in the image; this is appropriate as long as <span class="math inline">\(\mathbf{h}\)</span> only contains information about what should be in the image and not where.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">def conditional_gated_conv(self):</div><div class="line">    W_f = get_weights(self.W_shape, <span class="string">"v_W"</span>, mask=self.mask)</div><div class="line">    W_g = get_weights(self.W_shape, <span class="string">"h_W"</span>, mask=self.mask)</div><div class="line">    <span class="keyword">if</span> self.conditional is not None:</div><div class="line">        h_shape = int(self.conditional.get_shape()[1])</div><div class="line">        V_f = get_weights([h_shape, self.W_shape[3]], <span class="string">"v_V"</span>)</div><div class="line">        b_f = tf.matmul(self.conditional, V_f)</div><div class="line">        V_g = get_weights([h_shape, self.W_shape[3]], <span class="string">"h_V"</span>)</div><div class="line">        b_g = tf.matmul(self.conditional, V_g)</div><div class="line"></div><div class="line">        b_f_shape = tf.shape(b_f)</div><div class="line">        b_f = tf.reshape(b_f, (b_f_shape[0], 1, 1, b_f_shape[1]))</div><div class="line">	    b_g_shape = tf.shape(b_g)</div><div class="line">        b_g = tf.reshape(b_g, (b_g_shape[0], 1, 1, b_g_shape[1]))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        b_f = get_bias(self.b_shape, <span class="string">"v_b"</span>)</div><div class="line">        b_g = get_bias(self.b_shape, <span class="string">"h_b"</span>)</div><div class="line"></div><div class="line">    conv_f = conv_op(self.fan_in, W_f)</div><div class="line">    conv_g = conv_op(self.fan_in, W_g)</div><div class="line">       </div><div class="line">    self.fan_out = tf.multiply(tf.tanh(conv_f + b_f), tf.sigmoid(conv_g + b_g))</div></pre></td></tr></table></figure>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/1601.06759.pdf" target="_blank" rel="external">Pixel Recurrent Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/1606.05328.pdf" target="_blank" rel="external">Conditional Image Generation with PixelCNN Decoders</a></li>
<li><a href="http://sergeiturukin.com/2017/02/24/gated-pixelcnn.html" target="_blank" rel="external">sergeiturukin’s Post</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/04/2017-09-04The Intuition Behind LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/04/2017-09-04The Intuition Behind LSTM/" itemprop="url">Intuition Behind LSTM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-04T10:55:06+10:00">
                2017-09-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="content">Content</h1>
<ul>
<li>A mathematically Sufficient Condition For Vanishing Sensitivity</li>
<li>A minimum Weight Initialization to Avoid Vanishing Gradients</li>
<li>Written memories: the intuition behind LSTMs</li>
<li>Different Between Tanh and Sigmoid Function</li>
</ul>
<h2 id="a-mathematically-sufficient-condition-for-vanishing-sensitivity">A mathematically Sufficient Condition For Vanishing Sensitivity</h2>
<p>This is a mathematical proof of a sufficient condition for vanishing sensitivity in vanilla RNNs. The proof here also takes advantage of the mean value theorem to go one step further than Pascanu et al. and reach a slightly stronger result, effectively showing vanishing causation rather than vanishing sensitivity. Note that mathematical analyses of vanishing and exploding gradients date back to the early 1990s, in Bengio et al. (1994) and Hochreiter (1991) (original in German, relevant portions summarized in Hochreiter and Schmidhuber (1997)).</p>
To begin, from the definition of a vanilla RNN cell, we have: <span class="math display">\[s_{t+1} = \phi (z_t), where \space z_t = Ws_t + Ux_{t+1} + b\]</span> Recall the <em><strong>Mean value theorem</strong></em>: If a function <span class="math inline">\(f\)</span> is continuous on the closed interval <span class="math inline">\([a,b]\)</span>, and differentiable on the open interval <span class="math inline">\((a,b)\)</span> , then there exists a point <span class="math inline">\(c\)</span> in <span class="math inline">\((a,b)\)</span> such that: <span class="math display">\[f&#39;(c) = \frac{f(b)-f(a)}{b-a}\]</span> Applying the mean value theorem, we get that there exists <span class="math inline">\(c \in [z_t, z_t + \Delta z_t]\)</span> such that:
<span class="math display">\[\begin{equation}\begin{split}
\Delta S_{t+1} &amp;= [\phi&#39;(c)] \Delta z_t\\\\
&amp;= [\phi&#39;(c)] \Delta (Ws_t)\\\\
&amp;= [\phi&#39;(c)] W\Delta(s_t)
\end{split}\end{equation}\]</span>
<p>Now let <span class="math inline">\(\left \| A \right \|\)</span> represent the matrix 2-norm, <span class="math inline">\(\left | v \right |\)</span>the Euclidean vector norm, and define: <span class="math display">\[\gamma = sup_{c \in \{z_t, z_t + \Delta z_t\}} \left \| [\phi&#39; (c)] \right \|\]</span> Note that for the logistic sigmoid, <span class="math inline">\(\gamma \leq \frac{1}{4}\)</span>, and for tanh, <span class="math inline">\(\gamma \leq 1\)</span>.</p>
Taking the vector norm of each side, we obtain, where the first inequality comes from the definition of the 2-norm (applied twice), and second from the definition of supremum:
<span class="math display">\[\begin{equation}\begin{split}
\left | \Delta s_{t+1} \right | &amp;= \left | [\phi&#39;(c)] W \Delta s_t \right |\\\\
&amp;\leq \left \| [\phi&#39;(c)] \right \| \left \| W \right \| \left | \Delta s_t \right |\\\\
&amp;\leq \gamma \left \| W \right \| \left | \Delta s_t \right |\\\\
&amp;= \left \| \gamma W \right \| \left | \Delta s_t \right |
\end{split}\end{equation}\]</span>
<p>By expanding this formula over <span class="math inline">\(k\)</span> time steps we get: <span class="math display">\[\left | \Delta s_{t+k} \right | \leq \left \| \gamma W \right \|^k \left | \Delta s_t \right |\]</span> So that: <span class="math display">\[\frac{\left | \Delta s_{t+k} \right |}{\left | \Delta s_t \right |} = \left \| \gamma W \right \|^k\]</span></p>
<p>Therefore, if <span class="math inline">\(\left \| \gamma W \right \| &lt; 1\)</span> , we have that <span class="math inline">\(\frac{\left | \Delta s_{t+k} \right |}{\left | \Delta s_t \right |}\)</span> decreases exponentially in time.</p>
<h2 id="a-minimum-weight-initialization-to-avoid-vanishing-gradients">A minimum Weight Initialization to Avoid Vanishing Gradients</h2>
<p>It is beneficial to find a weight initialization that will not immediately suffer from this problem. Extending the above analysis to find the initialization of <span class="math inline">\(W\)</span> that gets us as close to equality as possible leads to a nice result. First, let us assume that <span class="math inline">\(\phi = tanh\)</span> and take <span class="math inline">\(\gamma = 1\)</span>. Our goal is to find an initialization of W for which:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\left \| \gamma W \right \| = 1\)</span></li>
<li>We get as close to equality as possible in above equation.</li>
</ol>
<p>From point 1, since we took <span class="math inline">\(\gamma\)</span> to be 1, we have <span class="math inline">\(\left \| W \right \| = 1\)</span>. From point 2, we get that we should try to set all singular values of <span class="math inline">\(W\)</span> to 1, not just the largest. Then, if all singular values of <span class="math inline">\(W\)</span> equal 1, that means that the norm of each column of <span class="math inline">\(W\)</span> is 1 (since each column is <span class="math inline">\(We_i\)</span> for some elementary basis vector <span class="math inline">\(e_i\)</span> and we have <span class="math inline">\(\left |We_i \right | = \left | e_i \right | = 1\)</span>). That means that for column <span class="math inline">\(j\)</span> we have: <span class="math display">\[\Sigma_i w_{ij}^2 = 1\]</span></p>
<p>There are <span class="math inline">\(n\)</span> entries in column <span class="math inline">\(j\)</span>, and we are choosing each from the same random distribution, so let us find a distribution for a random weight <span class="math inline">\(w\)</span> for which: <span class="math display">\[n\mathbb{E}(w^2) = 1\]</span></p>
<p>Now let’s suppose we want to initialize <span class="math inline">\(w\)</span> uniformly in the interval <span class="math inline">\([-R,R]\)</span>. Then the mean of <span class="math inline">\(w\)</span> is 0, so that, by definition, <span class="math inline">\(\mathbb{E}(w^2)\)</span> is its variance, <span class="math inline">\(\mathbb{V}(w)\)</span>. The variance of a uniform distribution over the interval <span class="math inline">\([a,b]\)</span> is given by <span class="math inline">\(\frac{(b-a)^2}{12}\)</span>, from which we get <span class="math inline">\(\mathbb{V}(w) = \frac{R^2}{3}\)</span>. Substituting this into our equation we get: <span class="math display">\[n\frac{R^2}{3} = 1\]</span></p>
<p>So that: <span class="math display">\[R = \frac{\sqrt{3}}{\sqrt{n}}\]</span></p>
<p>This is a nice result because it is the Xavier-Glorot initialization for a square weight matrix, yet was motivated by a different idea. The Xavier-Glorot initialization, introduced by Glorot and Bengio (2010), has proven to be an effective weight initialization prescription in practice. More generally, the Xavier-Glorot prescription applies to m-by-n weight matrices used in a layer that has an activation function whose derivative is near one at the origin (like tanh), and says that we should initialize our weights according to a uniform distribution of the interval: <span class="math display">\[[-\frac{\sqrt{6}}{\sqrt{m+n}},+\frac{\sqrt{6}}{\sqrt{m+n}}]\]</span></p>
<p>We saw above that good weight initializations are crucial, but this only impacts the start of training.</p>
<h2 id="written-memories-the-intuition-behind-lstms">Written memories: the intuition behind LSTMs</h2>
<p>Very much like the messages passed by children playing a game of broken telephone, information is morphed by RNN cells and the original message is lost. A small change in the original message may not have made any difference in the final message, or it may have resulted in something completely different.</p>
<p>How can we protect the integrity of messages? This is the fundamental principle of LSTMs: to ensure the integrity of our messages in the real world, we write them down. Writing is <em><strong>a delta to the current state</strong></em>: it is an act of creation (pen on paper) or destruction (carving in stone); the subject itself does not morph when you write on it and the error gradient on the backward-pass is constant.</p>
<p>This is precisely what was proposed by the landmark paper of Hocreiter and Schmidhuber (1997), which introduced the LSTM. They asked: “how can we achieve constant error flow through a single unit with a single connection to itself [i.e., a single piece of isolated information]?”</p>
<p>The answer, quite simply, is to avoid information morphing: changes to the state of an LSTM are explicitly written in, by an explicit addition or subtraction, so that each element of the state stays constant without outside interference: “the unit’s activation has to remain constant … this will be ensured by using the identity function”.</p>
<span class="math display">\[\begin{equation}\begin{split}
f_t &amp;= \sigma (W_f \cdot [h_{t-1}, x_t] + b_f)\\\\
i_t &amp;= \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\\\\
\tilde{C}_t &amp;= tanh(W_c \cdot [h_{t-1}, x_t] +b_c)\\\\
C_t &amp;= f_t \ast C_{t-1} + i_t \ast \tilde{C}_t \\\\
o_t &amp;= \sigma (W_o \cdot [h_{t-1}, x_t] + b_o)\\\\
h_t &amp;= o_t \ast tanh(C_t)
\end{split}\end{equation}\]</span>
<p>Suppose we have calculated the value of <span class="math inline">\(\frac{\partial L}{C_{t+k}}\)</span>, then: <span class="math display">\[\frac{\partial L}{C_t} = \frac{\partial L}{C_{t+k}} \odot f_{t+k-1} \odot f_{t+k-2} ... \odot f_{t}\]</span> Except for the very first term, the vanishing problem still exist.</p>
<h2 id="different-between-tanh-and-sigmoid-function">Different Between Tanh and Sigmoid Function</h2>
<ul>
<li>Sigmoid function have domain of all real numbers, ranging from 0 to 1, campared with Tanh function whose domain ranging from -1 to 1. The problem of Sigmoid is that the activation value is always positive, which give rise to the phenomenon that the derivative of weights are always all positive or all negative.</li>
<li>When a sigmoidal activation function must be used, the hyperbolic tangent activation function typically performs better than the logistic sigmoid. It <em><strong>resembles the identity function more closely</strong></em>, in the sense that <span class="math inline">\(tanh(0) = 0\)</span> while <span class="math inline">\(\sigma(0) = \frac{1}{2}\)</span>. Because tanh is similar to the identity function near 0, training a deep neural network <span class="math inline">\(\hat{y} = W^T tanh(U^T tanh(V^TX))\)</span> resembles training a linear model <span class="math inline">\(\hat{y} = W^TU^TV^TX\)</span> so long as the activations of the network can be kept small. This makes training the tanh network easier.</li>
</ul>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#written-memories-the-intuition-behind-lstms" target="_blank" rel="external">Written memories the intuition behind lstms</a></li>
<li><a href="http://www.deeplearningbook.org/contents/mlp.html" target="_blank" rel="external">Deep Learning Chap6</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/02/2017-09-02RNN for NLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/02/2017-09-02RNN for NLP/" itemprop="url">RNN for NLP</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-02T17:49:17+10:00">
                2017-09-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DL/" itemprop="url" rel="index">
                    <span itemprop="name">DL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>When training LSTM networks, Jozefowicz et al. [2015] strongly recommend to always initialize the bias term of the forget gate to be close to one.</p>
<p>Applying dropout to RNNs can be a bit tricky, that is, the dropout masks are sampled once per sequence, and not once per time step.</p>
<p>For longer sentences classification, Li et al.[2015] found it useful to use a hierarchical architecture, in which the sentence is split into smaller spans based on punctuation. Then, each span is fed into a forward and a backward RNN. Sequence of resulting vectors (one for each span) are then fed into an RNN acceptor.</p>
<p>To represent a word, except for general word embedding method like word2vec, we can use two character-level RNNs. For a word <span class="math inline">\(w\)</span>, made of characters <span class="math inline">\(c_1,c_2,...,c_l\)</span>, we will map each character into a corresponding embedding vector <span class="math inline">\(\mathbf{c}_i\)</span>. The word will then be encoded using a forward RNN and a reverse RNN over the characters. These RNNs can then either replace the word embedding vector, or, better yet, be concatenated.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/01/2017-09-01CNN for NLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/01/2017-09-01CNN for NLP/" itemprop="url">CNN for NLP</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-01T14:45:30+10:00">
                2017-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DL/" itemprop="url" rel="index">
                    <span itemprop="name">DL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>A convolutional neural network is designed to identify indicative local predictors in a large structure, and to combine them to produce a fixed size vector representation of the structure, capturing the local aspects that are most informative for the prediction task at hand. For instance, the convolutional architecture will identy n-grams that are predictive for the task at hand, without the need to pre-specify an embedding vector for each possible n-gram.<br>
The convolutional architecture also allows to share predictive behavior between n-grams that share similar components, even if the exact n-gram was never seen at test time.<br>
It could be expanded into a hierarchy of convolution layers, each one effectively looking at longer range of n-grams in the sentence.</p>
<center>
<img src="/images/2017-09-01CNN%20for%20NLP/single%20layer%20cnn.png">
</center>
The graph above is an example of only one convolution and max-pooling for sentence classification whose filters have different sizes. Without the pooling layer and scarifice the diverse filter sizes, we can generalize CNN model to hierarchical form.
<center>
<img src="/images/2017-09-01CNN%20for%20NLP/hierarchical%20cnn.png">
</center>
<h3 id="about-channels">About Channels</h3>
<p>When applying a convolution to an image in computer vision, it is common to apply a different set of filters to each channel, and then combine the three resulting vectors into a single vector. For text processing, we can still have multiple channels. For example, one channel will be the <em><strong>sequence of words</strong></em>, one channel will be the sequence of the corresponding <em><strong>POS</strong></em> tags and one will be the sequence of the corresponding <em><strong>positions</strong></em>. These three views can then be combined either by summation or by concatenation.</p>
<h3 id="feature-hashing">Feature Hashing</h3>
<p>There is no need to pre-compute vocabulary-to-index mapping, instead, allocate an embedding matrix <em><span class="math inline">\(E\)</span></em> with N rows. N should be sufficiently large, but no prohibitive. When a k-gram is seen in training, we assign it to a row in <em><span class="math inline">\(E\)</span></em> by applying hash function that will deterministically map it into a number in the range <span class="math inline">\([1,N]\)</span>. Then use the corresponding row as the embedding vector.</p>
<h3 id="dilated-convolution-architecture">Dilated Convolution Architecture</h3>
<p>A recent development (e.g. see paper by Fisher Yu and Vladlen Koltun) is to introduce one more hyperparameter to the CONV layer called the dilation. The normal CONV filters are contiguous. However, it’s possible to have filters that have spaces between each cell, called dilation.</p>
<p>It’s perhaps useful to first note why vanilla convolutions struggle to integrate global context. Consider a purely convolutional network composed of layers of <span class="math inline">\(k \times k\)</span> convolutions, without pooling. It is easy to see that size of the receptive field of each unit — the block of pixels which can influence its activation — is <span class="math inline">\(l \ast (k-1) + k\)</span>, where l is the layer index. So the effective receptive field of units can only grow linearly with layers. This is very limiting, especially for high-resolution input images.</p>
<p>As an example, in one dimension a filter w of size 3 would compute over input <span class="math inline">\(x\)</span> the following: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">w[0]*x[0] + w[1]*x[1] + w[2]*x[2]</div></pre></td></tr></table></figure></p>
<p>This is dilation of 0. For dilation 1 the filter would instead compute: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">w[0]*x[0] + w[1]*x[2] + w[2]*x[4]</div></pre></td></tr></table></figure></p>
<p>In other words there is a gap of 1 between the applications. This can be very useful in some settings to use in conjunction with 0-dilated filters because it allows you to merge spatial information across the inputs much more agressively with fewer layers. For example, if you stack two 3x3 CONV layers on top of each other then you can convince yourself that the neurons on the 2nd layer are a function of a 5x5 patch of the input (we would say that the effective receptive field of these neurons is 5x5). If we use dilated convolutions then this effective receptive field would grow much quicker.</p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li>http://cs231n.github.io/convolutional-networks/</li>
<li>http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</li>
<li>Neural Network Methods in Natural Language Processing Chapter 13</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/31/2017-08-31Batch Normalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/31/2017-08-31Batch Normalization/" itemprop="url">Batch Normalization & Layer Normalization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-31T22:00:34+10:00">
                2017-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="batch-normalization">Batch Normalization</h1>
<p>Batch normalization makes your hyperparameter search problem much easier, makes the neural network much more robust to the choice of hyperparameters, there’s a much bigger range of hyperparameters that work well, and will also enable you to much more easily train even very deep networks. Normalizing the input feature values can turn the contours of the learning problem from something that might be very elongated to something that is more round and easier for an algorithm like gradient descend to optimize. So, can we apply normalization to the intermediate layers? This is where batch normalization inspired from!</p>
<strong>Batch Normalization Algorithm </strong><br>
Given some intermediate values in a neural network for a specific neuron in a mini-batch, <span class="math inline">\({z^{(1)},z^{(2)},...,z^{(m)}}\)</span>
<span class="math display">\[\begin{equation}\begin{split}
\mu &amp;= \frac{1}{m}\Sigma_i z^{(i)}\\\\
\sigma^2 &amp;= \frac{1}{m}\Sigma_i (z^{(i)} - \mu)^2\\\\
z^{(i)}_{norm} &amp;= \frac{(z^{(i)} - \mu)}{\sqrt{\sigma^2 + \epsilon}}\\\\
\widehat{z}^{(i)} &amp;= \gamma z^{(i)}_{norm} + \beta
\end{split}\end{equation}\]</span>
<p>With batch normalization, the entire computational process should be: <span class="math display">\[X \stackrel{W^{[1]},b^{[1]}}{\longrightarrow}Z^{[1]}\stackrel{\beta^{[1]},\gamma^{[1]}}{\longrightarrow}\widehat{Z}^{[1]}\rightarrow A^{[1]} \stackrel{W^{[2]},b^{[2]},BN}{\longrightarrow} \widehat{Z}^{[2]} \rightarrow \cdot\cdot\cdot\]</span> The training parameters includes: <span class="math inline">\(w^{[1]}, b^{[1]}, ... ,w^{[l]}, b^{[l]}\)</span> and <span class="math inline">\(\beta^{[1]}, \gamma^{[1]}, ... ,\beta^{[l]}, \gamma^{[l]}\)</span> We should notice that, b can be eliminated due to that BN zeros out the mean.</p>
<p>The intuition behind is that:<br>
- Batch Normalization reduces the problem of the input values changing, it really causes these values to become stable, so that the later layers of the neural network has more firm ground to stand on.<br>
- Due to mini-batch, it adds some noise to each hidden layers’ activations, and produces some light regularization impact.</p>
<p>At test time, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are estimated by using exponentially weighted average across mini-batch. For a specific layer <span class="math inline">\(l\)</span>, we use EWA to keep track of the values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> of layer <span class="math inline">\(l\)</span> during training which will be used at test time.</p>
<p>For CNN model, for a mini-batch of size <span class="math inline">\(m\)</span> and feature maps of size <span class="math inline">\(p \times q\)</span>, we use the effecive mini-batch of size <span class="math inline">\(m&#39; = m \cdot pq\)</span>. We learn a pair of parameters <span class="math inline">\(\gamma^{(k)}\)</span> and <span class="math inline">\(\beta^{(k)}\)</span> per feature map, rather than per activation.</p>
<h1 id="layer-normalization">Layer Normalization</h1>
<p>In feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps.</p>
<p>Notice that changes in the output of one layer will tend to cause highly correlated changes in the summed inputs to the next layer, especially with ReLU units whose outputs can change by a lot. This suggests the “covariate shift” problem can be reduced by fixing the mean and the variance of the summed inputs within each layer. Thus, this paper proposes to compute the layer normalization statistics over all the hidden units in the same layer as follows:</p>
<span class="math display">\[\begin{equation}\begin{split}
\mu^L &amp;= \frac{1}{H} \Sigma^H_{i=1} a^L_i\\\\
\sigma &amp;= \sqrt{ \frac{1}{H} \Sigma^H_{i=1} (a^l_i - \mu^l)^2 }
\end{split}\end{equation}\]</span>
<p>where <span class="math inline">\(H\)</span> denotes the number of hidden units in a layer. The difference between Eq. (1) and Eq. (2) is that under layer normalization, all the hidden units in a layer share the same normalization terms <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, but different training cases have different normalization terms. Unlike batch normalization, layer normaliztion does not impose any constraint on the size of a mini-batch and it can be used in the pure online regime with batch size 1.</p>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li>Andrew Ng’s Deep Learning Course</li>
<li><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="external">Layer Normalization</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/31/2017-08-31Optimization Methods in Deep Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/31/2017-08-31Optimization Methods in Deep Learning/" itemprop="url">Optimization Methods in Deep Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-31T15:56:48+10:00">
                2017-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="why-gradient-descend-works">Why Gradient Descend Works?</h1>
Gradient Descend is the fundamental optimization method to learn parameters. To illustrate why it works, let’s take the simple example $f(x) = (x - 1)^2 $ to gain some intuition.
<center>
<img src="/images/2017-08-31Optimization%20Methods%20in%20Deep%20Learning/GD_intuition.png">
</center>
<ul>
<li>For point <span class="math inline">\(x=3\)</span>, <span class="math inline">\(f&#39;(x)=4&gt;0\)</span></li>
<li>For point <span class="math inline">\(x=-1\)</span>, <span class="math inline">\(f&#39;(x)=-4&lt;0\)</span></li>
</ul>
<p>If we want to decreace the value of <span class="math inline">\(f(x)\)</span>:<br>
- For point <span class="math inline">\(x=3\)</span>, we need to move a small step towards the opposite direction of <span class="math inline">\(f&#39;(x=3)\)</span><br>
- For point <span class="math inline">\(x=-1\)</span>, we need to move a samll step towards the opposite direction of <span class="math inline">\(f&#39;(x=-1)\)</span></p>
<p>So, if we want to descreace the value of a given function, we just move a small step towards the opposite direction of the function’s gradient.</p>
<h1 id="the-difference-among-gd-sgd-batch-sgd">The difference among GD, SGD, Batch-SGD</h1>
For <em><strong>Gradient Descend</strong></em>, take gradient steps with respect to all training examples on each step:
<center>
<img src="/images/2017-08-31Optimization%20Methods%20in%20Deep%20Learning/SGD_GD.png">
</center>
For <em><strong>Stochastic Gradient Descend</strong></em>, take gradient steps with respect to just one random training example on each step:
<center>
<img src="/images/2017-08-31Optimization%20Methods%20in%20Deep%20Learning/SGD_BGD.png">
</center>
<p>For <em><strong>Batch Gradient Descend</strong></em>, take gradient steps with respect to <span class="math inline">\(m\)</span> training examples on each step.</p>
<h1 id="momentum">Momentum</h1>
<p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will <em><strong>oscillate</strong></em> toward convergence. Using momentum can reduce these oscillations.</p>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the <em><strong>direction</strong></em> of the previous gradients in the variable <span class="math inline">\(v\)</span>. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of <span class="math inline">\(v\)</span> as the <em><strong>velocity</strong></em> of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.</p>
<center>
<img src="/images/2017-08-31Optimization%20Methods%20in%20Deep%20Learning/Momentum.png">
</center>
<p>The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence <span class="math inline">\(v\)</span> and then take a step in the direction of <span class="math inline">\(v\)</span>.</p>
<p>First, let’s gain some intuition from <em><strong>Exponentially Weighted Average</strong></em>.<br>
Let <span class="math inline">\(v\)</span> denote the variable for later use, and <span class="math inline">\(\theta\)</span> is the current observation <span class="math display">\[v_0 = 0\]</span> <span class="math display">\[v_1 = \beta v_0 + (1-\beta)\theta_1\]</span> <span class="math display">\[v_2 = \beta v_1 + (1-\beta)\theta_2\]</span> <span class="math display">\[.\]</span> <span class="math display">\[.\]</span> <span class="math display">\[.\]</span></p>
<p><span class="math display">\[v_t = \beta v_{t-1} + (1-\beta)\theta_t\]</span></p>
The intuition behind the above equation is that:
<span class="math display">\[\begin{equation}\begin{split}
v_t &amp;= \beta v_{t-1} + (1-\beta)\theta_t\\\\
&amp;= (1 - \beta)\theta_t + \beta(\beta v_{t-2} + (1-\beta)\theta_{t-1})\\\\
&amp;= (1-\beta)\theta_t + \beta(1-\beta)\theta_{t-1} + \beta^2v_{t-2}\\\\
&amp;= (1-\beta)\theta_t + \beta(1-\beta)\theta_{t-1} + \beta^2(1-\beta)\theta_{t-2} + ...
\end{split}\end{equation}\]</span>
<p>It’s just the exponentially weighted average of all the observations.</p>
<p>Inspired by the above equations, we define the gradient descent with momentum as follows: <span class="math display">\[v_{d_w} = \beta v_{d_w} + (1 - \beta)d_w\]</span> <span class="math display">\[v_{d_b} = \beta v_{d_b} + (1 - \beta)d_b\]</span> <span class="math display">\[w := w - \alpha v_{d_w}\]</span> <span class="math display">\[b := w - \alpha v_{d_b}\]</span></p>
<p>How do you choose <span class="math inline">\(\beta\)</span>?<br>
- The larger the momentum <span class="math inline">\(\beta\)</span> is, the smoother the update because the more we take the past gradients into account. But if <span class="math inline">\(\beta\)</span> is too big, it could also smooth out the updates too much.<br>
- Common values for <span class="math inline">\(\beta\)</span> range from 0.8 to 0.999. If you don’t feel inclined to tune this, <span class="math inline">\(\beta = 0.9\)</span> is often a reasonable default.<br>
- Tuning the optimal <span class="math inline">\(\beta\)</span> for your model might need trying several values to see what works best in term of reducing the value of the cost function.</p>
<p>Example code: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using Momentum</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></div><div class="line"><span class="string">                    v['dW' + str(l)] = ...</span></div><div class="line"><span class="string">                    v['db' + str(l)] = ...</span></div><div class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line"></div><div class="line">    L = len(parameters) // 2 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    </div><div class="line">    <span class="comment"># Momentum update for each parameter</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        </div><div class="line">        <span class="comment"># compute velocities</span></div><div class="line">        v[<span class="string">"dW"</span> + str(l+1)] = beta * v[<span class="string">"dW"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        v[<span class="string">"db"</span> + str(l+1)] = beta * v[<span class="string">"db"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"db"</span> + str(l+1)]</div><div class="line">        <span class="comment"># update parameters</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * v[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * v[<span class="string">"db"</span> + str(l+1)]</div><div class="line">        </div><div class="line">    <span class="built_in">return</span> parameters, v</div></pre></td></tr></table></figure></p>
<h1 id="rmsprop">RMSprop</h1>
<p>RMSprop, which stands for root mean square prop, is also inspired by the <em>Exponentially Weighted Average</em>. The algorithm is:<br>
<span class="math display">\[s_{d_w} = \beta s_{d_w} + (1 - \beta)d_w^2\]</span> <span class="math display">\[s_{d_b} = \beta s_{d_b} + (1 - \beta)d_b^2\]</span> <span class="math display">\[w := w - \alpha \frac{d_w}{\sqrt{s_{d_w}}}\]</span> <span class="math display">\[b := b - \alpha \frac{d_b}{\sqrt{s_{d_b}}}\]</span></p>
<p>The intuition behind is also to damp out the huge oscillations. Let’s say <span class="math inline">\(d_w\)</span> is larger than <span class="math inline">\(d_b\)</span>, then <span class="math inline">\(s_{d_w}\)</span> is also larger than <span class="math inline">\(s_{d_b}\)</span>. When we update parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>, <span class="math inline">\(d_w\)</span> is divided by a relatively larger number than <span class="math inline">\(d_b\)</span> whcih helps to damp out oscillations.</p>
<p>The common choice of <span class="math inline">\(\beta\)</span> is 0.999.</p>
<p>Example code: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_RMSprop(parameters, grads, s, beta, learning_rate):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using RMSprop</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    s -- python dictionary containing the current squared gradient:</span></div><div class="line"><span class="string">                    s['dW' + str(l)] = ...</span></div><div class="line"><span class="string">                    s['db' + str(l)] = ...</span></div><div class="line"><span class="string">    beta -- the RMSprop hyperparameter, scalar</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    s -- python dictionary containing your updated squared gradient</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line"></div><div class="line">    L = len(parameters) // 2 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    </div><div class="line">    <span class="comment"># RMSprop update for each parameter</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        </div><div class="line">        <span class="comment"># compute current squared gradient</span></div><div class="line">        s[<span class="string">"dW"</span> + str(l+1)] = beta * s[<span class="string">"dW"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"dW"</span> + str(l+1)] ** 2</div><div class="line">        s[<span class="string">"db"</span> + str(l+1)] = beta * s[<span class="string">"db"</span> + str(l+1)] + (1-beta) * grads[<span class="string">"db"</span> + str(l+1)] ** 2</div><div class="line">        <span class="comment"># update parameters</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * s[<span class="string">"dW"</span> + str(l+1)] / np.sqrt(s[<span class="string">"dW"</span> + str(l+1)])</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * s[<span class="string">"db"</span> + str(l+1)] / np.sqrt(s[<span class="string">"db"</span> + str(l+1)])</div><div class="line">        </div><div class="line">    <span class="built_in">return</span> parameters, s</div></pre></td></tr></table></figure></p>
<h1 id="adam">Adam</h1>
<p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. The update rule is, for <span class="math inline">\(l = 1,...L\)</span>: <span class="math display">\[v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial Cost}{\partial W^{[l]}}\]</span> <span class="math display">\[v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t}\]</span> <span class="math display">\[s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial Cost}{\partial W^{[l]}})^2\]</span> <span class="math display">\[s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_2)^t}\]</span> <span class="math display">\[W^{[L]} = W^{[L]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}} + \epsilon}}\]</span></p>
<p>where:<br>
- t counts the number of steps taken of Adam<br>
- L is the number of layers<br>
- <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters that control the two exponentially weighted averages.<br>
- <span class="math inline">\(\alpha\)</span> is the learning rate<br>
- <span class="math inline">\(\epsilon\)</span> is a very small number to avoid dividing by zero.</p>
<p>The common choice of hyperparameters s <span class="math inline">\(\beta_1 = 0.9. \beta_2 = 0.999, \epsilon = 1e-8\)</span></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,</div><div class="line">                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line"><span class="string">    Update parameters using Adam</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></div><div class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></div><div class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></div><div class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></div><div class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></div><div class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></div><div class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></div><div class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></div><div class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></div><div class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></div><div class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></div><div class="line"><span class="string">    "</span><span class="string">""</span></div><div class="line">    </div><div class="line">    L = len(parameters) // 2                 <span class="comment"># number of layers in the neural networks</span></div><div class="line">    v_corrected = &#123;&#125;                         <span class="comment"># Initializing first moment estimate, python dictionary</span></div><div class="line">    s_corrected = &#123;&#125;                         <span class="comment"># Initializing second moment estimate, python dictionary</span></div><div class="line">    </div><div class="line">    <span class="comment"># Perform Adam update on all parameters</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        <span class="comment"># Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".</span></div><div class="line">        v[<span class="string">"dW"</span> + str(l+1)] = beta1 * v[<span class="string">"dW"</span> + str(l+1)] + (1 - beta1) * grads[<span class="string">"dW"</span> + str(l+1)]</div><div class="line">        v[<span class="string">"db"</span> + str(l+1)] = beta1 * v[<span class="string">"db"</span> + str(l+1)] + (1 - beta1) * grads[<span class="string">"db"</span> + str(l+1)]</div><div class="line"></div><div class="line">        <span class="comment"># Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".</span></div><div class="line">        v_corrected[<span class="string">"dW"</span> + str(l+1)] = v[<span class="string">"dW"</span> + str(l+1)] / (1 - beta1 ** t)</div><div class="line">        v_corrected[<span class="string">"db"</span> + str(l+1)] = v[<span class="string">"db"</span> + str(l+1)] / (1 - beta1 ** t)</div><div class="line"></div><div class="line">        <span class="comment"># Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".</span></div><div class="line">        s[<span class="string">"dW"</span> + str(l+1)] = beta2 * s[<span class="string">"dW"</span> + str(l+1)] + (1 - beta2) * grads[<span class="string">"dW"</span> + str(l+1)] ** 2</div><div class="line">        s[<span class="string">"db"</span> + str(l+1)] = beta2 * s[<span class="string">"db"</span> + str(l+1)] + (1 - beta2) * grads[<span class="string">"db"</span> + str(l+1)] ** 2</div><div class="line"></div><div class="line">        <span class="comment"># Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".</span></div><div class="line">        s_corrected[<span class="string">"dW"</span> + str(l+1)] = s[<span class="string">"dW"</span> + str(l+1)] / (1 - beta2 ** t)</div><div class="line">        s_corrected[<span class="string">"db"</span> + str(l+1)] = s[<span class="string">"db"</span> + str(l+1)] / (1 - beta2 ** t)</div><div class="line"></div><div class="line">        <span class="comment"># Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".</span></div><div class="line">        parameters[<span class="string">"W"</span> + str(l+1)] = parameters[<span class="string">"W"</span> + str(l+1)] - learning_rate * v_corrected[<span class="string">"dW"</span> + str(l+1)] / np.sqrt(s_corrected[<span class="string">"dW"</span> + str(l+1)] + epsilon)</div><div class="line">        parameters[<span class="string">"b"</span> + str(l+1)] = parameters[<span class="string">"b"</span> + str(l+1)] - learning_rate * v_corrected[<span class="string">"db"</span> + str(l+1)] / np.sqrt(s_corrected[<span class="string">"db"</span> + str(l+1)] + epsilon)</div><div class="line"></div><div class="line">    <span class="built_in">return</span> parameters, v, s</div></pre></td></tr></table></figure>
<h1 id="reference">Reference</h1>
<p>Andrew Ng’s deep learning course</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/31/2017-08-31Vanishing and Exploding Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/31/2017-08-31Vanishing and Exploding Gradients/" itemprop="url">Vanishing and Exploding Gradients</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-31T15:08:39+10:00">
                2017-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Intuitively, extra hidden layers ought to make the network able to learn more complex classification functions, and thus do a better job classifying. Certainly, things shouldn’t get worse, since the extra layers can, in the worst case, simply do nothing. But that’s not what’s going on.</p>
<p>Here is an example to illustrate why the vanishing gradient problem occurs. Let’s explicitly write out the entire expression for the gradient: <span class="math display">\[\frac{\partial Cost}{\partial w1} = \frac{\partial Cost}{\partial a_4}f&#39;(z_4)w_4f&#39;(z_3)w_3f&#39;(z_2)w_2f&#39;(z_1)x_{input}\]</span></p>
<p>Excepting the very first term and very last term, this expression is a product of terms of the form: <span class="math display">\[f&#39;(z_j)w_j\]</span></p>
<p>Recall that the standard approach to initialize the weights in the network is to choose the weights using a Gaussian with mean 0 and small standard deviation. So the weights will usually satisfy <span class="math inline">\(|w_j| &lt; 1\)</span>.</p>
<p>If the non-linear function is the widely used relu function, then the derivative of <span class="math inline">\(f&#39;(z_j)\)</span> can only take the values of 1 or 0. So the terms <span class="math inline">\(f&#39;(z_j)w_j\)</span> will usually much less than 1. And when we take a product of many such terms, the product will tend to exponentially decrease: the more terms, the smaller the product will be. This is starting to smell like a possible explanation for the vanishing gradient problem. In particular, we might wonder whether the weights <span class="math inline">\(w_j\)</span> could grow during training. If they do, it’s possible the terms <span class="math inline">\(f&#39;(z_j)w_j\)</span> in the product will no longer satisfy <span class="math inline">\(|f&#39;(z_j)w_j| &lt; 1\)</span>. Indeed, if the terms get large enough - greater than 1 - then we will no longer have a vanishing gradient problem. Instead, the gradient will actually grow exponentially as we move backward through the layers. Instead of a vanishing gradient problem, we’ll have an exploding gradient problem.</p>
<p>It is the fact that the gradient in early layers is the product of terms from all the later layers. When there are many layers, that’s an intrinsically unstable situation. The only way all layers can learn at close to the same speed is if all those products of terms come close to balancing out. Without some mechanism or underlying reason for that balancing to occur, it’s highly unlikely to happen simply by chance. In short, the real problem here is that neural networks suffer from an unstable gradient problem. As a result, if we use standard gradient-based learning techniques, different layers in the network will tend to learn at wildly different speeds.</p>
<p>To reduce the effect of vanishing or exploding gradient problem, espeically for relu function, we can initilize the weights by: <span class="math display">\[w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}})\]</span> The intuition behind is that: If we need the value of <span class="math inline">\(z = w_1x_1 + ... + w_nx_n + b\)</span> to be near 1, we should set <span class="math inline">\(Var(w) = \frac{1}{n}\)</span>, because the larger is n, the smaller the term <span class="math inline">\(w_jx_j\)</span> should be. In practice, using <span class="math inline">\(\frac{2}{n^{[l-1]}}\)</span> instead works better.</p>
<p>And for tanh activation function, an effective scheme due to <strong>Glorot and Bengio [2010]</strong>, called <strong>xavier initialization</strong>, suggests initializing a weight matrix <em><span class="math inline">\(W\)</span></em> <span class="math inline">\(\in \mathbb{R}^{d_{in} \times d_{out}}\)</span> as: <span class="math display">\[W \sim U[-\frac{\sqrt{6}}{\sqrt{d_{in}+d_{out}}},+\frac{\sqrt{6}}{\sqrt{d_{in}+d_{out}}}]\]</span></p>
<p>Another way is to perform <em><strong>Batch Normalization</strong></em>, the simple intuition behind is: for every minibatch, normalizing the inputs to each of the network layers to have zeros mean and unit variance. I’ll talk about this method in another post in detail.</p>
<p>Besides, dealing with the exploding gradients has a simple but very effective solution: clipping the gradients if their norm exceeds a given threshold. Let <em><span class="math inline">\(g\)</span></em> be the gradients of all parameters in the network, and <em><span class="math inline">\(\|g\|\)</span></em> be their <span class="math inline">\(L_2\)</span> norm. Pascanu et al.[2012] suggest to set: <span class="math inline">\(g \leftarrow \frac{threshlod}{\|g\|}g\)</span> # Reference</p>
<ol style="list-style-type: decimal">
<li><a href="http://neuralnetworksanddeeplearning.com/chap5.html" target="_blank" rel="external">Neural Network and Deep Leanring Chpater 5</a><br>
</li>
<li>Andrew Ng’s Deep Learning Course</li>
<li>Neural network methods in natural language processing</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/29/2017-08-29fastText/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/29/2017-08-29fastText/" itemprop="url">Some understanding about how fastText works</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-29T16:31:06+10:00">
                2017-08-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="table-of-contents">Table of contents</h1>
<ul>
<li>Introduction</li>
<li>Word Embedding
<ul>
<li>The Skip-gram Model</li>
<li>CBOW Model</li>
<li>fastText Model</li>
</ul></li>
<li>Hierarchical Softmax
<ul>
<li>Huffman Tree</li>
<li>Huffman Coding</li>
<li>Derivatives of Hierarchical Softmax</li>
</ul></li>
<li>Conclusion</li>
</ul>
<h1 id="introduction">1. Introduction</h1>
<p>FastText is a library created by the Facebook Research Team for efficient learning of <strong>word representations</strong> and <strong>sentence classification</strong>. There are two main refernece papers:</p>
<ul>
<li>P. Bojanowski<em>, E. Grave</em>, A. Joulin, T. Mikolov, <a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a></li>
<li>A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="external">Bag of Tricks for Efficient Text Classification</a></li>
</ul>
<h1 id="word-embedding">2. Word Embedding</h1>
<p>Representing words in vectors (espicially word2vec) is widely used in Natural Language Processing applications because it’s fast and easy to use. However, there are several drawbacks of word2vec.</p>
<ol style="list-style-type: decimal">
<li>You can not build sentence representations easily. People take like average vector of a sentence and use it, but actually it not works well.</li>
<li>The other thing is that it doesn’t exploit morphology. Which means words with same radicals do not share parameters. For example: <strong>disastrous</strong> is different from <strong>disaster</strong>.</li>
</ol>
<h2 id="the-skip-gram-model">2.1 The Skip-gram Model</h2>
<p>Given a word in the middle, we can sucessively predict all the words in its context.</p>
<center>
<img src="/images/2017-08-29fastText/skipgram.png">
</center>
<p>For instance: <em>The mighty <strong>knight</strong> Lancelot fought bravely</em>, we use <strong>knight</strong> to predict <em>the</em>, <em>mighty</em>, <em>lancelot</em>, <em>fought</em> and <em>bravely</em>.</p>
<p>Our goal is to model probability of a <strong>context word</strong> given a word:</p>
<ul>
<li>feature for word w: <span class="math inline">\(\space x_{w}\)</span></li>
<li>classifier for context word c: <span class="math inline">\(\space v_{c}\)</span></li>
</ul>
<p><span class="math display">\[p(w|c) = \frac{exp(x^T_wv_c)}{\Sigma_{k=1}^Kexp(x_w^Tv_k)}\]</span></p>
<p>And Word Vectors <span class="math inline">\(x_w\in\mathbb{R}^d\)</span>, are used for some downstream tasks.</p>
<p>To train this model, we need to minimize a <em><strong>negative log likelihood</strong></em>:</p>
<p><span class="math display">\[min_{x,v} \space-\Sigma^T_{t=1}\Sigma_{c{\in}C_t}log\frac{e^{x^T_{w_t}v_c}}{\Sigma^K_{k=1}e^{x^T_{w_t}v_k}}\]</span></p>
<p>However, the denominator of the softmax is very computationally intensive. There are two main approximations.</p>
<ul>
<li>Replace the multiclass loss by a set of binary logistic losses</li>
<li><em><strong>Negative sampling</strong></em> (you have the positive word in the context and then just sample some random ones in the dictionary)</li>
</ul>
<p><span class="math display">\[log(1+e^{-x^T_{w_t}v_c}) + \Sigma_{n{\in}\mathcal{N}_c}log(1+e^{x^T_{w_t}v_n})\]</span></p>
<ul>
<li><em><strong>Hierarchical softmax</strong></em>: represent every word as a set of codes <span class="math inline">\(y_{ck}\)</span>. These codes are obtained by computing the Huffman tree. In the end, very frequent words are going to have short codes and very unfrequent words are going to have longer codes, so it’s very fast to evaluate and just sum over the codes for every example. I’ll explain this method in detail in the next section.</li>
</ul>
<p><span class="math display">\[\Sigma_{k{\in}\mathcal{K}_c}log(1 + e^{y_{ck}x^T_{w_t}v_k})\]</span></p>
<h2 id="cbow-model">2.2 CBOW Model</h2>
<p>Given a context and sum the vectors to predict the word in the middle.</p>
<center>
<img src="/images/2017-08-29fastText/CBOW.png">
</center>
<p>For instance: <em>The mighty <strong>knight</strong> Lancelot fought bravely</em>, we use <em>the</em>, <em>mighty</em>, <em>lancelot</em>, <em>fought</em> and <em>bravely</em> to predict <em>knight</em>.</p>
<p>Our goal is to model the probability of a word given a context:</p>
<ul>
<li>feature for context <span class="math inline">\(\mathcal{C}: h_{\mathcal{C}}\)</span></li>
<li>classifier for word <span class="math inline">\(w: v_w\)</span></li>
</ul>
<p><span class="math display">\[p(w|\mathcal{C}) = \frac{e^{h^T_{\mathcal{C}}v_w}}{\Sigma^K_{k=1}e^{h^T_{\mathcal{C}}v_k}}\]</span></p>
<p>And <span class="math inline">\(h_{\mathcal{C}}\)</span> is just the sum of the words in the context and that’s why it is called the <em><strong>continuous bag of words</strong></em>:</p>
<p><span class="math display">\[h_{\mathcal{C}} = \Sigma_{c{\in}\mathcal{C}}x_c\]</span></p>
<h2 id="fasttext-model">2.3 fastText Model</h2>
<p>The goal is to model the probability of a <em>label</em> given a <em>paragraph</em> so you have:</p>
<ul>
<li>feature for paragraph <span class="math inline">\(\mathcal{P}: h_{\mathcal{p}}\)</span></li>
<li>classifier for label <span class="math inline">\(l: v_l\)</span></li>
</ul>
<p><span class="math display">\[p(l|\mathcal{P}) = \frac{e^{h^T_{\mathcal{P}}v_l}}{\Sigma^K_{k=1}e^{h^T_{\mathcal{P}}v_k}}\]</span></p>
<ul>
<li>Paragraph feather:</li>
</ul>
<p><span class="math display">\[h_{\mathcal{p}} = \Sigma_{w{\in}\mathcal{p}}x_w\]</span></p>
<h4 id="exploiting-sub-word-information">Exploiting sub-word information</h4>
<ul>
<li>Represent words as the sum of its <em><strong>character n-grams</strong></em>.
<ul>
<li>Add special positional characters: ^knight$</li>
<li>All ending n-grams have special meaning</li>
</ul></li>
<li>Grammatical variations have roughtly the same mixture of n-grams</li>
<li>As in skip-gram: model probability of a <em><em>context word</em></em> given a word
<ul>
<li>classifer for word <span class="math inline">\(c: v_c\)</span></li>
<li>feature for word <span class="math inline">\(w: h_w\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[p(c|w) = \frac{e^{h^T_w}v_c}{\Sigma^K_{k=1}e^{h^T_wv_k}}\]</span></p>
<ul>
<li>Feature of a word is computed using n-grams, it takes all the character n-grams and the word itself, using hashing to store them. This makes it possible to build vectors for unseen words.</li>
</ul>
<p><span class="math display">\[h_w = \Sigma_{g{\in}w}x_g\]</span></p>
<h1 id="hierarchical-softmax">3. Hierarchical Softmax</h1>
<h2 id="hoffman-tree">3.1 Hoffman Tree</h2>
<p><em><strong>Algorithm of constucting a Huffman Tree</strong></em></p>
<ol style="list-style-type: decimal">
<li>Create a leaf node for each symbol and add it to the priority queue.</li>
<li>While there is more than one node in the queue:
<ul>
<li>Remove the two nodes of highest priority (lowest probability) from the queue</li>
<li>Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes’ probabilities.</li>
<li>Add the new node to the queue.</li>
</ul></li>
<li>The remaining node is the root node and the tree is complete.</li>
</ol>
<p>The following graph is an example:</p>
<center>
<img src="/images/2017-08-29fastText/HuffmanTree.png">
</center>
<p>To use Hoffman tree in our language models, symbols here can represents words (for both Skip-gram model and CBOW model) or paragraph labels (fastText Model). And also, from the algorithm, we can easily know that symbols with higher frequency are closer to the root of the Huffman tree.</p>
<h2 id="hoffman-coding">3.2 Hoffman Coding</h2>
To code a Huffman tree, just simply code all left nodes as 1 and right nodes as 0. Then we can represent each leaf based on the path from root. Also, nodes closer to the root will have shorter path.
<center>
<img src="/images/2017-08-29fastText/HuffmanCoding.png">
</center>
<p>For exampl, the Huffman coding of node (3) is <em>0111</em>.</p>
<h2 id="derivatives-of-hierarchical-softmax">3.3 Derivatives of Hierarchical Softmax</h2>
Let us use the following graph as an example to illustrate how to calculate the derivatives of hierarchical softmax.
<center>
<img src="/images/2017-08-29fastText/Derivative_of_Softmax.png">
</center>
<p>First, let’s define some notations here:</p>
<ul>
<li><span class="math inline">\(x_w\)</span> denote the input feature to the hierarchical softmax;</li>
<li><span class="math inline">\(p^w\)</span>: path from root to leaf <span class="math inline">\(w\)</span>;</li>
<li><span class="math inline">\(l^w\)</span>: number of nodes in the path <span class="math inline">\(p^w\)</span>;</li>
<li><span class="math inline">\(p^w_1,p^w_2,...,p^w_{l^w}\)</span>: denote the nodes in the path <span class="math inline">\(p^w\)</span>;</li>
<li><span class="math inline">\(d^w_2,d^w_3,...,d^w_{l^w}\)</span>: the Huffman code of word w, it consists <span class="math inline">\(l^w-1\)</span> digits (0 or 1);</li>
<li><span class="math inline">\(\theta^w_1,\theta^w_2,...,\theta^w_{l^w-1}\)</span>: the weights of nodes in the path <span class="math inline">\(p^w\)</span> except for the leaf;</li>
<li>And just follow Google’s usage, 1 stands for negative samples and 0 stands for positive samples.</li>
</ul>
<p>Now, take the node “Sport” as an example. It takes four binary classification to travel from root to node “Sport”:</p>
<ul>
<li>First: <span class="math inline">\(\space p(d^w_2|x_w,\theta^w_1) = \sigma(x^T_w\theta^w_1)\)</span></li>
<li>Second: <span class="math inline">\(\space p(d^w_3|x_w,\theta^w_2) = 1 - \sigma(x^T_w\theta^w_2)\)</span></li>
<li>Third: <span class="math inline">\(\space p(d^w_4|x_w,\theta^w_3) = 1 - \sigma(x^T_w\theta^w_3)\)</span></li>
<li>Fourth: <span class="math inline">\(\space p(d^w_5|x_w,\theta^w_4) = 1 - \sigma(x^T_w\theta^w_4)\)</span></li>
</ul>
<p>And we have: <span class="math display">\[p(Sport|input) = \prod^5_{j=2}p(d^w_j|x_w,\theta^w_1)\]</span></p>
In general, we have: <span class="math display">\[p(w|input) = \prod^{l^w}_{j=2}p(d^w_j|x_w,\theta^w_{j-1})\]</span> <span class="math display">\[p(d^w_j|x_w,\theta^w_{j-1}) = [\sigma(x^T_w\theta^w_{j-1})]^{1-d^w_j}[1 - \sigma(x^T_w\theta^w_{j-1})]^{d^w_j}\]</span> The log-likelihood function is:
<span class="math display">\[\begin{equation}\begin{split}
\mathcal{L} &amp;= log \prod^{l^w}_{j=2}{[\sigma(x^T_w\theta^w_{j-1})]^{1-d^w_j}[1 - \sigma(x^T_w\theta^w_{j-1})]^{d^w_j}}\\\\
&amp;= \Sigma^{l^w}_{j=2}{(1-d^w_j)log[\sigma(x^T_w\theta^w_{j-1})]+d^w_jlog[1-\sigma(x^T_w\theta^w_{j-1})]}
\end{split}\end{equation}\]</span>
The calculation of derivatives are as follows:
<span class="math display">\[\begin{equation}\begin{split}
\frac{\partial \mathcal{L}(w,j)}{\partial \theta^w_{j-1}} &amp;= \frac{\partial}{\partial\theta^w_{j-1}}\{(1-d^w_j)log[\sigma(x^T_w\theta^w_{j-1})] + d^w_jlog[1-\sigma(x^T_w\theta^w_{j-1})]\}\\\\
&amp;= (1-d^w_j)[1-\sigma(x^T_w\theta^w_{j-1})]x_w-d^w_j\sigma(x^T_w\theta^w_{j-1})x_w\\\\
&amp;=\{(1-d^w_j)[1-\sigma(x^T_w\theta^w_{j-1})]-d^w_j\sigma(x^T_w\theta^w_{j-1})\}x_w\\\\
&amp;= [1-d^w_j-\sigma(x^T_w\theta^w_{j-1})]x_w
\end{split}\end{equation}\]</span>
<p>And similarly, for <span class="math inline">\(x_w\)</span>: <span class="math display">\[\frac{\partial\mathcal{L}(w,j)}{\partial x_w} = [1-d^w_j-\sigma(x^T_w\theta^w_{j-1})]\theta^w_{j-1}\]</span></p>
<h1 id="conclusion">Conclusion</h1>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/" target="_blank" rel="external">Text Classification &amp; Word Representations using FastText (An NLP library by Facebook)</a></li>
<li><a href="http://www.cnblogs.com/peghoty/p/3857839.html" class="uri" target="_blank" rel="external">http://www.cnblogs.com/peghoty/p/3857839.html</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/27/2017-08-26Start Your Blog Using Hexo and Githubpage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/27/2017-08-26Start Your Blog Using Hexo and Githubpage/" itemprop="url">Start Your Blog Using Hexo and Githubpage</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-27T23:24:53+10:00">
                2017-08-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This post will decrible how to setup your Blog using GithubPage and Hexo briefly. # Installation ### Requirement 1. <a href="https://nodejs.org/en/" target="_blank" rel="external">Node.js</a> 2. <a href="https://git-scm.com" target="_blank" rel="external">Git</a></p>
<p>If your computer already has these, just install Hexo with npm: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install -g hexo-cli</div></pre></td></tr></table></figure></p>
<h1 id="setup">Setup</h1>
<p>Once Hexo is installed, run the following commands to initialise Hexo in the target <folder>. <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo init &lt;folder&gt;</div><div class="line">$ <span class="built_in">cd</span> &lt;folder&gt;</div><div class="line">$ npm install</div></pre></td></tr></table></figure></folder></p>
<h1 id="deployment">Deployment</h1>
<p>Before your first deployment, you will have to modify some settings in _config.yml. A valid deployment setting must have a type field. For example: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  <span class="built_in">type</span>: git</div><div class="line">  repo: http://github.com/&lt;username&gt;/&lt;username&gt;.github.io.git</div><div class="line">  branch: master</div></pre></td></tr></table></figure></p>
<p>Before your first deployment, install <a href="https://github.com/hexojs/hexo-deployer-git" target="_blank" rel="external">hexo-deployer-git</a>: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-deployer-git --save</div></pre></td></tr></table></figure></p>
<p>Then need to set your git information as follow: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git config --global user.name <span class="string">"yourname"</span></div><div class="line">git config --global user.email <span class="string">"youremail"</span></div></pre></td></tr></table></figure></p>
<h1 id="write">Write</h1>
<p>Then you can write your posts and save them in the folder _post under source.</p>
<h1 id="publish">Publish</h1>
<p>Just type in the following command to publish your posts: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</div></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  








  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>

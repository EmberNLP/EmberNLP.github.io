<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta property="og:type" content="website">
<meta property="og:title" content="EmberNLP">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="EmberNLP">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EmberNLP">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title>EmberNLP</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">EmberNLP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/31/2017-08-31Vanishing and Exploding Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/31/2017-08-31Vanishing and Exploding Gradients/" itemprop="url">Vanishing and Exploding Gradients</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-31T15:08:39+10:00">
                2017-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Intuition/" itemprop="url" rel="index">
                    <span itemprop="name">Intuition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Intuitively, extra hidden layers ought to make the network able to learn more complex classification functions, and thus do a better job classifying. Certainly, things shouldn’t get worse, since the extra layers can, in the worst case, simply do nothing. But that’s not what’s going on.</p>
<p>Here is an example to illustrate why the vanishing gradient problem occurs. Let’s explicitly write out the entire expression for the gradient: <span class="math display">\[\frac{\partial Cost}{\partial w1} = \frac{\partial Cost}{\partial a_4}f&#39;(z_4)w_4f&#39;(z_3)w_3f&#39;(z_2)w_2f&#39;(z_1)x_{input}\]</span></p>
<p>Excepting the very first term and very last term, this expression is a product of terms of the form: <span class="math display">\[f&#39;(z_j)w_j\]</span></p>
<p>Recall that the standard approach to initialize the weights in the network is to choose the weights using a Gaussian with mean 0 and small standard deviation. So the weights will usually satisfy <span class="math inline">\(|w_j| &lt; 1\)</span>.</p>
<p>If the non-linear function is the widely used relu function, then the derivative of <span class="math inline">\(f&#39;(z_j)\)</span> can only take the values of 1 or 0. So the terms <span class="math inline">\(f&#39;(z_j)w_j\)</span> will usually much less than 1. And when we take a product of many such terms, the product will tend to exponentially decrease: the more terms, the smaller the product will be. This is starting to smell like a possible explanation for the vanishing gradient problem. In particular, we might wonder whether the weights <span class="math inline">\(w_j\)</span> could grow during training. If they do, it’s possible the terms <span class="math inline">\(f&#39;(z_j)w_j\)</span> in the product will no longer satisfy <span class="math inline">\(|f&#39;(z_j)w_j| &lt; 1\)</span>. Indeed, if the terms get large enough - greater than 1 - then we will no longer have a vanishing gradient problem. Instead, the gradient will actually grow exponentially as we move backward through the layers. Instead of a vanishing gradient problem, we’ll have an exploding gradient problem.</p>
<p>It is the fact that the gradient in early layers is the product of terms from all the later layers. When there are many layers, that’s an intrinsically unstable situation. The only way all layers can learn at close to the same speed is if all those products of terms come close to balancing out. Without some mechanism or underlying reason for that balancing to occur, it’s highly unlikely to happen simply by chance. In short, the real problem here is that neural networks suffer from an unstable gradient problem. As a result, if we use standard gradient-based learning techniques, different layers in the network will tend to learn at wildly different speeds.</p>
<p>To reduce the effect of vanishing or exploding gradient problem, espeically for relu function, we can initilize the weights by: <span class="math display">\[w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}})\]</span> The intuition behind is that: If we need the value of <span class="math inline">\(z = w_1x_1 + ... + w_nx_n + b\)</span> to be near 1, we should set <span class="math inline">\(Var(w) = \frac{1}{n}\)</span>, because the larger is n, the smaller the term <span class="math inline">\(w_jx_j\)</span> should be. In practice, using <span class="math inline">\(\frac{2}{n^{[l-1]}}\)</span> instead works better.</p>
<p>And for tanh activation function, an effective scheme due to <strong>Glorot and Bengio [2010]</strong>, called <strong>xavier initialization</strong>, suggests initializing a weight matrix <em><span class="math inline">\(W\)</span></em> <span class="math inline">\(\in \mathbb{R}^{d_{in} \times d_{out}}\)</span> as: <span class="math display">\[W \sim U[-\frac{\sqrt{6}}{\sqrt{d_{in}+d_{out}}},+\frac{\sqrt{6}}{\sqrt{d_{in}+d_{out}}}]\]</span></p>
<p>Another way is to perform <em><strong>Batch Normalization</strong></em>, the simple intuition behind is: for every minibatch, normalizing the inputs to each of the network layers to have zeros mean and unit variance. I’ll talk about this method in another post in detail.</p>
<p>Besides, dealing with the exploding gradients has a simple but very effective solution: clipping the gradients if their norm exceeds a given threshold. Let <em><span class="math inline">\(g\)</span></em> be the gradients of all parameters in the network, and <em><span class="math inline">\(\|g\|\)</span></em> be their <span class="math inline">\(L_2\)</span> norm. Pascanu et al.[2012] suggest to set: <span class="math inline">\(g \leftarrow \frac{threshlod}{\|g\|}g\)</span> # Reference</p>
<p>1.<a href="http://neuralnetworksanddeeplearning.com/chap5.html" target="_blank" rel="external">Neural Network and Deep Leanring Chpater 5</a><br>
2.Andrew Ng’s Deep Learning Course 3.Neural network methods in natural language processing</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/29/2017-08-29fastText/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/29/2017-08-29fastText/" itemprop="url">Some understanding about how fastText works</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-29T16:31:06+10:00">
                2017-08-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Paper Notes</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="table-of-contents">Table of contents</h1>
<ul>
<li>Introduction</li>
<li>Word Embedding
<ul>
<li>The Skip-gram Model</li>
<li>CBOW Model</li>
<li>fastText Model</li>
</ul></li>
<li>Hierarchical Softmax
<ul>
<li>Huffman Tree</li>
<li>Huffman Coding</li>
<li>Derivatives of Hierarchical Softmax</li>
</ul></li>
<li>Conclusion</li>
</ul>
<h1 id="introduction">1. Introduction</h1>
<p>FastText is a library created by the Facebook Research Team for efficient learning of <strong>word representations</strong> and <strong>sentence classification</strong>. There are two main refernece papers:</p>
<ul>
<li>P. Bojanowski<em>, E. Grave</em>, A. Joulin, T. Mikolov, <a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a></li>
<li>A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="external">Bag of Tricks for Efficient Text Classification</a></li>
</ul>
<h1 id="word-embedding">2. Word Embedding</h1>
<p>Representing words in vectors (espicially word2vec) is widely used in Natural Language Processing applications because it’s fast and easy to use. However, there are several drawbacks of word2vec.</p>
<ol style="list-style-type: decimal">
<li>You can not build sentence representations easily. People take like average vector of a sentence and use it, but actually it not works well.</li>
<li>The other thing is that it doesn’t exploit morphology. Which means words with same radicals do not share parameters. For example: <strong>disastrous</strong> is different from <strong>disaster</strong>.</li>
</ol>
<h2 id="the-skip-gram-model">2.1 The Skip-gram Model</h2>
<p>Given a word in the middle, we can sucessively predict all the words in its context.</p>
<center>
<img src="/images/2017-08-29fastText/skipgram.png">
</center>
<p>For instance: <em>The mighty <strong>knight</strong> Lancelot fought bravely</em>, we use <strong>knight</strong> to predict <em>the</em>, <em>mighty</em>, <em>lancelot</em>, <em>fought</em> and <em>bravely</em>.</p>
<p>Our goal is to model probability of a <strong>context word</strong> given a word:</p>
<ul>
<li>feature for word w: <span class="math inline">\(\space x_{w}\)</span></li>
<li>classifier for context word c: <span class="math inline">\(\space v_{c}\)</span></li>
</ul>
<p><span class="math display">\[p(w|c) = \frac{exp(x^T_wv_c)}{\Sigma_{k=1}^Kexp(x_w^Tv_k)}\]</span></p>
<p>And Word Vectors <span class="math inline">\(x_w\in\mathbb{R}^d\)</span>, are used for some downstream tasks.</p>
<p>To train this model, we need to minimize a <em><strong>negative log likelihood</strong></em>:</p>
<p><span class="math display">\[min_{x,v} \space-\Sigma^T_{t=1}\Sigma_{c{\in}C_t}log\frac{e^{x^T_{w_t}v_c}}{\Sigma^K_{k=1}e^{x^T_{w_t}v_k}}\]</span></p>
<p>However, the denominator of the softmax is very computationally intensive. There are two main approximations.</p>
<ul>
<li>Replace the multiclass loss by a set of binary logistic losses</li>
<li><em><strong>Negative sampling</strong></em> (you have the positive word in the context and then just sample some random ones in the dictionary)</li>
</ul>
<p><span class="math display">\[log(1+e^{-x^T_{w_t}v_c}) + \Sigma_{n{\in}\mathcal{N}_c}log(1+e^{x^T_{w_t}v_n})\]</span></p>
<ul>
<li><em><strong>Hierarchical softmax</strong></em>: represent every word as a set of codes <span class="math inline">\(y_{ck}\)</span>. These codes are obtained by computing the Huffman tree. In the end, very frequent words are going to have short codes and very unfrequent words are going to have longer codes, so it’s very fast to evaluate and just sum over the codes for every example. I’ll explain this method in detail in the next section.</li>
</ul>
<p><span class="math display">\[\Sigma_{k{\in}\mathcal{K}_c}log(1 + e^{y_{ck}x^T_{w_t}v_k})\]</span></p>
<h2 id="cbow-model">2.2 CBOW Model</h2>
<p>Given a context and sum the vectors to predict the word in the middle.</p>
<center>
<img src="/images/2017-08-29fastText/CBOW.png">
</center>
<p>For instance: <em>The mighty <strong>knight</strong> Lancelot fought bravely</em>, we use <em>the</em>, <em>mighty</em>, <em>lancelot</em>, <em>fought</em> and <em>bravely</em> to predict <em>knight</em>.</p>
<p>Our goal is to model the probability of a word given a context:</p>
<ul>
<li>feature for context <span class="math inline">\(\mathcal{C}: h_{\mathcal{C}}\)</span></li>
<li>classifier for word <span class="math inline">\(w: v_w\)</span></li>
</ul>
<p><span class="math display">\[p(w|\mathcal{C}) = \frac{e^{h^T_{\mathcal{C}}v_w}}{\Sigma^K_{k=1}e^{h^T_{\mathcal{C}}v_k}}\]</span></p>
<p>And <span class="math inline">\(h_{\mathcal{C}}\)</span> is just the sum of the words in the context and that’s why it is called the <em><strong>continuous bag of words</strong></em>:</p>
<p><span class="math display">\[h_{\mathcal{C}} = \Sigma_{c{\in}\mathcal{C}}x_c\]</span></p>
<h2 id="fasttext-model">2.3 fastText Model</h2>
<p>The goal is to model the probability of a <em>label</em> given a <em>paragraph</em> so you have:</p>
<ul>
<li>feature for paragraph <span class="math inline">\(\mathcal{P}: h_{\mathcal{p}}\)</span></li>
<li>classifier for label <span class="math inline">\(l: v_l\)</span></li>
</ul>
<p><span class="math display">\[p(l|\mathcal{P}) = \frac{e^{h^T_{\mathcal{P}}v_l}}{\Sigma^K_{k=1}e^{h^T_{\mathcal{P}}v_k}}\]</span></p>
<ul>
<li>Paragraph feather:</li>
</ul>
<p><span class="math display">\[h_{\mathcal{p}} = \Sigma_{w{\in}\mathcal{p}}x_w\]</span></p>
<h4 id="exploiting-sub-word-information">Exploiting sub-word information</h4>
<ul>
<li>Represent words as the sum of its <em><strong>character n-grams</strong></em>.
<ul>
<li>Add special positional characters: ^knight$</li>
<li>All ending n-grams have special meaning</li>
</ul></li>
<li>Grammatical variations have roughtly the same mixture of n-grams</li>
<li>As in skip-gram: model probability of a <em><em>context word</em></em> given a word
<ul>
<li>classifer for word <span class="math inline">\(c: v_c\)</span></li>
<li>feature for word <span class="math inline">\(w: h_w\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[p(c|w) = \frac{e^{h^T_w}v_c}{\Sigma^K_{k=1}e^{h^T_wv_k}}\]</span></p>
<ul>
<li>Feature of a word is computed using n-grams, it takes all the character n-grams and the word itself, using hashing to store them. This makes it possible to build vectors for unseen words.</li>
</ul>
<p><span class="math display">\[h_w = \Sigma_{g{\in}w}x_g\]</span></p>
<h1 id="hierarchical-softmax">3. Hierarchical Softmax</h1>
<h2 id="hoffman-tree">3.1 Hoffman Tree</h2>
<p><em><strong>Algorithm of constucting a Huffman Tree</strong></em></p>
<ol style="list-style-type: decimal">
<li>Create a leaf node for each symbol and add it to the priority queue.</li>
<li>While there is more than one node in the queue:
<ul>
<li>Remove the two nodes of highest priority (lowest probability) from the queue</li>
<li>Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes’ probabilities.</li>
<li>Add the new node to the queue.</li>
</ul></li>
<li>The remaining node is the root node and the tree is complete.</li>
</ol>
<p>The following graph is an example:</p>
<center>
<img src="/images/2017-08-29fastText/HuffmanTree.png">
</center>
<p>To use Hoffman tree in our language models, symbols here can represents words (for both Skip-gram model and CBOW model) or paragraph labels (fastText Model). And also, from the algorithm, we can easily know that symbols with higher frequency are closer to the root of the Huffman tree.</p>
<h2 id="hoffman-coding">3.2 Hoffman Coding</h2>
To code a Huffman tree, just simply code all left nodes as 1 and right nodes as 0. Then we can represent each leaf based on the path from root. Also, nodes closer to the root will have shorter path.
<center>
<img src="/images/2017-08-29fastText/HuffmanCoding.png">
</center>
<p>For exampl, the Huffman coding of node (3) is <em>0111</em>.</p>
<h2 id="derivatives-of-hierarchical-softmax">3.3 Derivatives of Hierarchical Softmax</h2>
Let us use the following graph as an example to illustrate how to calculate the derivatives of hierarchical softmax.
<center>
<img src="/images/2017-08-29fastText/Derivative_of_Softmax.png">
</center>
<p>First, let’s define some notations here:</p>
<ul>
<li><span class="math inline">\(x_w\)</span> denote the input feature to the hierarchical softmax;</li>
<li><span class="math inline">\(p^w\)</span>: path from root to leaf <span class="math inline">\(w\)</span>;</li>
<li><span class="math inline">\(l^w\)</span>: number of nodes in the path <span class="math inline">\(p^w\)</span>;</li>
<li><span class="math inline">\(p^w_1,p^w_2,...,p^w_{l^w}\)</span>: denote the nodes in the path <span class="math inline">\(p^w\)</span>;</li>
<li><span class="math inline">\(d^w_2,d^w_3,...,d^w_{l^w}\)</span>: the Huffman code of word w, it consists <span class="math inline">\(l^w-1\)</span> digits (0 or 1);</li>
<li><span class="math inline">\(\theta^w_1,\theta^w_2,...,\theta^w_{l^w-1}\)</span>: the weights of nodes in the path <span class="math inline">\(p^w\)</span> except for the leaf;</li>
<li>And just follow Google’s usage, 1 stands for negative samples and 0 stands for positive samples.</li>
</ul>
<p>Now, take the node “Sport” as an example. It takes four binary classification to travel from root to node “Sport”:</p>
<ul>
<li>First: <span class="math inline">\(\space p(d^w_2|x_w,\theta^w_1) = \sigma(x^T_w\theta^w_1)\)</span></li>
<li>Second: <span class="math inline">\(\space p(d^w_3|x_w,\theta^w_2) = 1 - \sigma(x^T_w\theta^w_2)\)</span></li>
<li>Third: <span class="math inline">\(\space p(d^w_4|x_w,\theta^w_3) = 1 - \sigma(x^T_w\theta^w_3)\)</span></li>
<li>Fourth: <span class="math inline">\(\space p(d^w_5|x_w,\theta^w_4) = 1 - \sigma(x^T_w\theta^w_4)\)</span></li>
</ul>
<p>And we have: <span class="math display">\[p(Sport|input) = \prod^5_{j=2}p(d^w_j|x_w,\theta^w_1)\]</span></p>
In general, we have: <span class="math display">\[p(w|input) = \prod^{l^w}_{j=2}p(d^w_j|x_w,\theta^w_{j-1})\]</span> <span class="math display">\[p(d^w_j|x_w,\theta^w_{j-1}) = [\sigma(x^T_w\theta^w_{j-1})]^{1-d^w_j}[1 - \sigma(x^T_w\theta^w_{j-1})]^{d^w_j}\]</span> The log-likelihood function is:
<span class="math display">\[\begin{equation}\begin{split}
\mathcal{L} &amp;= log \prod^{l^w}_{j=2}{[\sigma(x^T_w\theta^w_{j-1})]^{1-d^w_j}[1 - \sigma(x^T_w\theta^w_{j-1})]^{d^w_j}}\\\\
&amp;= \Sigma^{l^w}_{j=2}{(1-d^w_j)log[\sigma(x^T_w\theta^w_{j-1})]+d^w_jlog[1-\sigma(x^T_w\theta^w_{j-1})]}
\end{split}\end{equation}\]</span>
The calculation of derivatives are as follows:
<span class="math display">\[\begin{equation}\begin{split}
\frac{\partial \mathcal{L}(w,j)}{\partial \theta^w_{j-1}} &amp;= \frac{\partial}{\partial\theta^w_{j-1}}\{(1-d^w_j)log[\sigma(x^T_w\theta^w_{j-1})] + d^w_jlog[1-\sigma(x^T_w\theta^w_{j-1})]\}\\\\
&amp;= (1-d^w_j)[1-\sigma(x^T_w\theta^w_{j-1})]x_w-d^w_j\sigma(x^T_w\theta^w_{j-1})x_w\\\\
&amp;=\{(1-d^w_j)[1-\sigma(x^T_w\theta^w_{j-1})]-d^w_j\sigma(x^T_w\theta^w_{j-1})\}x_w\\\\
&amp;= [1-d^w_j-\sigma(x^T_w\theta^w_{j-1})]x_w
\end{split}\end{equation}\]</span>
<p>And similarly, for <span class="math inline">\(x_w\)</span>: <span class="math display">\[\frac{\partial\mathcal{L}(w,j)}{\partial x_w} = [1-d^w_j-\sigma(x^T_w\theta^w_{j-1})]\theta^w_{j-1}\]</span></p>
<h1 id="conclusion">Conclusion</h1>
<h1 id="reference">Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/" target="_blank" rel="external">Text Classification &amp; Word Representations using FastText (An NLP library by Facebook)</a></li>
<li><a href="http://www.cnblogs.com/peghoty/p/3857839.html" class="uri" target="_blank" rel="external">http://www.cnblogs.com/peghoty/p/3857839.html</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/27/2017-08-26Start Your Blog Using Hexo and Githubpage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ember">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EmberNLP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/27/2017-08-26Start Your Blog Using Hexo and Githubpage/" itemprop="url">Start Your Blog Using Hexo and Githubpage</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-27T23:24:53+10:00">
                2017-08-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This post will decrible how to setup your Blog using GithubPage and Hexo briefly. # Installation ### Requirement 1. <a href="https://nodejs.org/en/" target="_blank" rel="external">Node.js</a> 2. <a href="https://git-scm.com" target="_blank" rel="external">Git</a></p>
<p>If your computer already has these, just install Hexo with npm: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install -g hexo-cli</div></pre></td></tr></table></figure></p>
<h1 id="setup">Setup</h1>
<p>Once Hexo is installed, run the following commands to initialise Hexo in the target <folder>. <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo init &lt;folder&gt;</div><div class="line">$ <span class="built_in">cd</span> &lt;folder&gt;</div><div class="line">$ npm install</div></pre></td></tr></table></figure></folder></p>
<h1 id="deployment">Deployment</h1>
<p>Before your first deployment, you will have to modify some settings in _config.yml. A valid deployment setting must have a type field. For example: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  <span class="built_in">type</span>: git</div><div class="line">  repo: http://github.com/&lt;username&gt;/&lt;username&gt;.github.io.git</div><div class="line">  branch: master</div></pre></td></tr></table></figure></p>
<p>Before your first deployment, install <a href="https://github.com/hexojs/hexo-deployer-git" target="_blank" rel="external">hexo-deployer-git</a>: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-deployer-git --save</div></pre></td></tr></table></figure></p>
<p>Then need to set your git information as follow: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git config --global user.name <span class="string">"yourname"</span></div><div class="line">git config --global user.email <span class="string">"youremail"</span></div></pre></td></tr></table></figure></p>
<h1 id="write">Write</h1>
<p>Then you can write your posts and save them in the folder _post under source.</p>
<h1 id="publish">Publish</h1>
<p>Just type in the following command to publish your posts: <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</div></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Ember" />
          <p class="site-author-name" itemprop="name">Ember</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">13</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ember</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  








  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
